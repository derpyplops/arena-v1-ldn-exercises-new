{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyYNTtji_xXP"
      },
      "source": [
        "# Transformer Lens Main Demo Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "erAnuvJJ_xXQ"
      },
      "source": [
        "**Tips for reading this Colab:**\n",
        "* You can run all this code for yourself! \n",
        "* The graphs are interactive!\n",
        "* Use the table of contents pane in the sidebar to navigate\n",
        "* Collapse irrelevant sections with the dropdown arrows\n",
        "* Search the page using the search in the sidebar, not CTRL+F"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEbfYAP4_xXR"
      },
      "source": [
        "# Setup\n",
        "(No need to read)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "32SWMgsx_xXR",
        "outputId": "0a778dd3-5a84-4e7f-de92-2b10df0995ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running as a Jupyter notebook - intended for development only!\n",
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/c4/4fjxf_xd34l0pg999qb04d_80000gn/T/ipykernel_73625/830796270.py:21: DeprecationWarning:\n",
            "\n",
            "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "\n",
            "/var/folders/c4/4fjxf_xd34l0pg999qb04d_80000gn/T/ipykernel_73625/830796270.py:22: DeprecationWarning:\n",
            "\n",
            "`magic(...)` is deprecated since IPython 0.13 (warning added in 8.1), use run_line_magic(magic_name, parameter_s).\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Janky code to do different setup when run in a Colab notebook vs VSCode\n",
        "DEVELOPMENT_MODE = False\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"Running as a Colab notebook\")\n",
        "    %pip install git+https://github.com/neelnanda-io/TransformerLens.git@new-demo\n",
        "    %pip install circuitsvis\n",
        "    \n",
        "    # PySvelte is an unmaintained visualization library, use it as a backup if circuitsvis isn't working\n",
        "    # # Install another version of node that makes PySvelte work way faster\n",
        "    # !curl -fsSL https://deb.nodesource.com/setup_16.x | sudo -E bash -; sudo apt-get install -y nodejs\n",
        "    # %pip install git+https://github.com/neelnanda-io/PySvelte.git\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"Running as a Jupyter notebook - intended for development only!\")\n",
        "    from IPython import get_ipython\n",
        "\n",
        "    ipython = get_ipython()\n",
        "    # Code to automatically update the HookedTransformer code as its edited without restarting the kernel\n",
        "    ipython.magic(\"load_ext autoreload\")\n",
        "    ipython.magic(\"autoreload 2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "0wfCJJ3T_xXS",
        "outputId": "b965b1bf-c12f-4545-9c9f-aa5a09937d5a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using renderer: colab\n"
          ]
        }
      ],
      "source": [
        "# Plotly needs a different renderer for VSCode/Notebooks vs Colab argh\n",
        "import plotly.io as pio\n",
        "if IN_COLAB or not DEVELOPMENT_MODE:\n",
        "    pio.renderers.default = \"colab\"\n",
        "else:\n",
        "    pio.renderers.default = \"notebook_connected\"\n",
        "print(f\"Using renderer: {pio.renderers.default}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "xenspa5x_xXS",
        "outputId": "8d3a9641-4c92-41d1-f3d8-84140cc033dd"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-ce902738-5eae\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, Hello } from \"https://unpkg.com/circuitsvis@1.34.0/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-ce902738-5eae\",\n",
              "      Hello,\n",
              "      {\"name\": \"Neel\"}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x16dcb55b0>"
            ]
          },
          "execution_count": 64,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import circuitsvis as cv\n",
        "# Testing that the library works\n",
        "cv.examples.hello(\"Neel\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"e698efe7-f1dc-44ab-9059-e24f9297c0e2\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e698efe7-f1dc-44ab-9059-e24f9297c0e2\")) {                    Plotly.newPlot(                        \"e698efe7-f1dc-44ab-9059-e24f9297c0e2\",                        [{\"x\":[1,2,3],\"y\":[4,1,2],\"type\":\"scatter\"},{\"x\":[1,2,3],\"y\":[2,4,5],\"yaxis\":\"y2\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"yaxis2\":{\"title\":{\"text\":\"y2\",\"font\":{\"color\":\"red\"}},\"tickfont\":{\"color\":\"red\"},\"overlaying\":\"y\",\"side\":\"right\"},\"title\":{\"text\":\"Double Y Axis Example\"},\"xaxis\":{\"title\":{\"text\":\"x1\"}},\"yaxis\":{\"title\":{\"text\":\"y1\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e698efe7-f1dc-44ab-9059-e24f9297c0e2');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# create test plotly figure\n",
        "import plotly.graph_objects as go\n",
        "fig = go.Figure()\n",
        "fig.add_trace(go.Scatter(x=[1, 2, 3], y=[4, 1, 2]))\n",
        "fig.add_trace(go.Scatter(x=[1, 2, 3], y=[2, 4, 5], yaxis=\"y2\"))\n",
        "fig.update_layout(\n",
        "    title=\"Double Y Axis Example\",\n",
        "    xaxis_title=\"x1\",\n",
        "    yaxis_title=\"y1\",\n",
        "    yaxis2=dict(\n",
        "        title=\"y2\",\n",
        "        titlefont=dict(color=\"red\"),\n",
        "        tickfont=dict(color=\"red\"),\n",
        "        overlaying=\"y\",\n",
        "        side=\"right\",\n",
        "    )\n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "3V3cZxAh_xXS"
      },
      "outputs": [],
      "source": [
        "# Import stuff\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import einops\n",
        "from fancy_einsum import einsum\n",
        "import tqdm.auto as tqdm\n",
        "import random\n",
        "from pathlib import Path\n",
        "import plotly.express as px\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "from torchtyping import TensorType as TT\n",
        "from typing import List, Union, Optional\n",
        "from functools import partial\n",
        "import copy\n",
        "\n",
        "import itertools\n",
        "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer\n",
        "import dataclasses\n",
        "import datasets\n",
        "from IPython.display import HTML"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "w1ZUKWhP_xXS"
      },
      "outputs": [],
      "source": [
        "import transformer_lens\n",
        "import transformer_lens.utils as utils\n",
        "from transformer_lens.hook_points import (\n",
        "    HookedRootModule,\n",
        "    HookPoint,\n",
        ")  # Hooking utilities\n",
        "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRP1t06J_xXT"
      },
      "source": [
        "We turn automatic differentiation off, to save GPU memory, as this notebook focuses on model inference not model training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "qZfup1Wi_xXT",
        "outputId": "0857534c-6eb5-49d5-e41b-71dbf77f7751"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<torch.autograd.grad_mode.set_grad_enabled at 0x16ddc2520>"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.set_grad_enabled(False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2N7LXIF_xXT"
      },
      "source": [
        "Plotting helper functions:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "x8mfJffS_xXU"
      },
      "outputs": [],
      "source": [
        "def imshow(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.imshow(utils.to_numpy(tensor), color_continuous_midpoint=0.0, color_continuous_scale=\"RdBu\", labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def line(tensor, renderer=None, xaxis=\"\", yaxis=\"\", **kwargs):\n",
        "    px.line(utils.to_numpy(tensor), labels={\"x\":xaxis, \"y\":yaxis}, **kwargs).show(renderer)\n",
        "\n",
        "def scatter(x, y, xaxis=\"\", yaxis=\"\", caxis=\"\", renderer=None, **kwargs):\n",
        "    x = utils.to_numpy(x)\n",
        "    y = utils.to_numpy(y)\n",
        "    px.scatter(y=y, x=x, labels={\"x\":xaxis, \"y\":yaxis, \"color\":caxis}, **kwargs).show(renderer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TgdYOeq_xXU"
      },
      "source": [
        "# Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDrew49y_xXU"
      },
      "source": [
        "This is a demo notebook for [TransformerLens](https://github.com/neelnanda-io/TransformerLens), **a library I ([Neel Nanda](neelnanda.io)) wrote for doing [mechanistic interpretability](https://distill.pub/2020/circuits/zoom-in/) of GPT-2 Style language models.** The goal of mechanistic interpretability is to take a trained model and reverse engineer the algorithms the model learned during training from its weights. It is a fact about the world today that we have computer programs that can essentially speak English at a human level (GPT-3, PaLM, etc), yet we have no idea how they work nor how to write one ourselves. This offends me greatly, and I would like to solve this! Mechanistic interpretability is a very young and small field, and there are a *lot* of open problems - if you would like to help, please try working on one! **Check out my [list of concrete open problems](TODO: link) to figure out where to start.**\n",
        "\n",
        "I wrote this library because after I left the Anthropic interpretability team and started doing independent research, I got extremely frustrated by the state of open source tooling. There's a lot of excellent infrastructure like HuggingFace and DeepSpeed to *use* or *train* models, but very little to dig into their internals and reverse engineer how they work. **This library tries to solve that**, and to make it easy to get into the field even if you don't work at an industry org with real infrastructure! The core features were heavily inspired by [Anthropic's excellent Garcon tool](https://transformer-circuits.pub/2021/garcon/index.html). Credit to Nelson Elhage and Chris Olah for building Garcon and showing me the value of good infrastructure for accelerating exploratory research!\n",
        "\n",
        "The core design principle I've followed is to enable exploratory analysis - one of the most fun parts of mechanistic interpretability compared to normal ML is the extremely short feedback loops! The point of this library is to keep the gap between having an experiment idea and seeing the results as small as possible, to make it easy for **research to feel like play** and to enter a flow state. This notebook demonstrates how the library works and how to use it, but if you want to see how well it works for exploratory research, check out [my notebook analysing Indirect Objection Identification](TODO: link) or [my recording of myself doing research](https://www.youtube.com/watch?v=yo4QvDn-vsU)!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_fgnWeL_xXU"
      },
      "source": [
        "## Loading and Running Models\n",
        "\n",
        "TransformerLens comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. For this demo notebook we'll look at GPT-2 Small, an 80M parameter model, see the Available Models section for info on the rest."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "vAYl8bNG_xXU"
      },
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "doPI1DTG_xXU",
        "outputId": "5cb94ad5-37a3-44d7-fa39-ae81a853c4db"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model gpt2-small into HookedTransformer\n"
          ]
        }
      ],
      "source": [
        "model = HookedTransformer.from_pretrained(\"gpt2-small\", device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDkx_Vv6_xXU"
      },
      "source": [
        "To try the model the model out, let's find the loss on this text! Models can be run on a single string or a tensor of tokens (shape: [batch, position], all integers), and the possible return types are: \n",
        "* \"logits\" (shape [batch, position, d_vocab], floats), \n",
        "* \"loss\" (the cross-entropy loss when predicting the next token), \n",
        "* \"both\" (a tuple of (logits, loss)) \n",
        "* None (run the model, but don't calculate the logits - this is faster when we only want to use intermediate activations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "R1Uy16E__xXU",
        "outputId": "6e9b4eb2-999b-4cd4-8f0a-673cff6869a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model loss: tensor(4.1652)\n"
          ]
        }
      ],
      "source": [
        "model_description_text = \"\"\"## Loading Models\n",
        "\n",
        "HookedTransformer comes loaded with >40 open source GPT-style models. You can load any of them in with `HookedTransformer.from_pretrained(MODEL_NAME)`. See [model_details.md](TODO: link) for a description of all supported models. Each model is loaded into the consistent HookedTransformer architecture, designed to be clean, consistent and interpretability-friendly. \n",
        "\n",
        "For this demo notebook we'll look at GPT-2 Small, an 80M parameter model. To try the model the model out, let's find the loss on this paragraph!\"\"\"\n",
        "loss = model(model_description_text, return_type=\"loss\")\n",
        "print(\"Model loss:\", loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FUsZHlI_xXU"
      },
      "source": [
        "## Caching all Activations\n",
        "\n",
        "The first basic operation when doing mechanistic interpretability is to break open the black box of the model and look at all of the internal activations of a model. This can be done with `logits, cache = model.run_with_cache(tokens)`. Let's try this out on the first line of the abstract of the GPT-2 paper.\n",
        "\n",
        "<details><summary>On `remove_batch_dim`</summary>\n",
        "\n",
        "Every activation inside the model begins with a batch dimension. Here, because we only entered a single batch dimension, that dimension is always length 1 and kinda annoying, so passing in the `remove_batch_dim=True` keyword removes it. `gpt2_cache_no_batch_dim = gpt2_cache.remove_batch_dim()` would have achieved the same effect.\n",
        "</details?>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "EcSVecbl_xXU",
        "outputId": "4138f066-985d-4172-aaaa-27bf974c8bc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cpu\n"
          ]
        }
      ],
      "source": [
        "gpt2_text = \"Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets.\"\n",
        "gpt2_tokens = model.to_tokens(gpt2_text)\n",
        "print(gpt2_tokens.device)\n",
        "gpt2_logits, gpt2_cache = model.run_with_cache(gpt2_tokens, remove_batch_dim=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Yf4rTsS_xXV"
      },
      "source": [
        "Let's visualize the attention pattern of all the heads in layer 0, using [Alan Cooney's CircuitsVis library](https://github.com/alan-cooney/CircuitsVis) (based on [Anthropic's PySvelte library](TODO)). \n",
        "\n",
        "We look this the attention pattern in `gpt2_cache`, an `ActivationCache` object, by entering in the name of the activation, followed by the layer index (here, the activation is called \"attn\" and the layer index is 0). This has shape [head_index, destination_position, source_position], and we use the `model.to_str_tokens` method to convert the text to a list of tokens as strings, since there is an attention weight between each pair of tokens.\n",
        "\n",
        "This visualization is interactive! Try hovering over a token or head, and click to lock. The grid on the top left and for each head is the attention pattern as a destination position by source position grid. It's lower triangular because GPT-2 has **causal attention**, attention can only look backwards, so information can only move forwards in the network.\n",
        "\n",
        "See the ActivationCache section for more on what `gpt2_cache` can do (TODO link)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "zrOWeVSq_xXV",
        "outputId": "5a02b5ae-d8e2-48f1-92d9-ab59ffeb423a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'transformer_lens.ActivationCache.ActivationCache'>\n",
            "torch.Size([12, 33, 33])\n"
          ]
        }
      ],
      "source": [
        "print(type(gpt2_cache))\n",
        "attention_pattern = gpt2_cache[\"pattern\", 0, \"attn\"]\n",
        "print(attention_pattern.shape)\n",
        "gpt2_str_tokens = model.to_str_tokens(gpt2_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "KP3iGNDA_xXV",
        "outputId": "050a5e57-b6ae-4100-9794-3d7ef687dc23"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Layer 0 Head Attention Patterns:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-289b0702-821a\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.34.0/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-289b0702-821a\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\"<|endoftext|>\", \"Natural\", \" language\", \" processing\", \" tasks\", \",\", \" such\", \" as\", \" question\", \" answering\", \",\", \" machine\", \" translation\", \",\", \" reading\", \" comprehension\", \",\", \" and\", \" summar\", \"ization\", \",\", \" are\", \" typically\", \" approached\", \" with\", \" supervised\", \" learning\", \" on\", \" tasks\", \"pe\", \"cific\", \" datasets\", \".\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9639418721199036, 0.03605814278125763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8389372229576111, 0.11828789114952087, 0.042774882167577744, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.47436124086380005, 0.13382017612457275, 0.27371737360954285, 0.11810117214918137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.35606440901756287, 0.10184912383556366, 0.2305423617362976, 0.2039739489555359, 0.10757020860910416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6660142540931702, 0.1686638593673706, 0.045356716960668564, 0.038854990154504776, 0.06775479018688202, 0.013355256989598274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38626936078071594, 0.285109281539917, 0.07609006762504578, 0.05908379703760147, 0.07223362475633621, 0.039796382188797, 0.08141743391752243, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3775393068790436, 0.18838828802108765, 0.11723991483449936, 0.08685600757598877, 0.06669192016124725, 0.03500018268823624, 0.09693007171154022, 0.03135441243648529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4869752824306488, 0.06781307607889175, 0.07952883839607239, 0.08480790257453918, 0.1590261459350586, 0.029577823355793953, 0.025685936212539673, 0.01647462323307991, 0.05011046677827835, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29065510630607605, 0.0401349775493145, 0.1461484581232071, 0.0994059219956398, 0.15389196574687958, 0.039001621305942535, 0.024988962337374687, 0.031841278076171875, 0.10222820192575455, 0.0717034712433815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3962409496307373, 0.09694171696901321, 0.027270665392279625, 0.023551328107714653, 0.03723446652293205, 0.006502413656562567, 0.08118760585784912, 0.013088458217680454, 0.0699058547616005, 0.24043117463588715, 0.007645337376743555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24864791333675385, 0.13802063465118408, 0.09235319495201111, 0.08676130324602127, 0.13819687068462372, 0.059141963720321655, 0.032238639891147614, 0.03158239275217056, 0.030489452183246613, 0.03873484209179878, 0.06671836227178574, 0.0371144600212574, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19148442149162292, 0.16172589361667633, 0.07445931434631348, 0.07740955054759979, 0.021961132064461708, 0.033921293914318085, 0.05125030130147934, 0.019519247114658356, 0.03132449463009834, 0.04020150005817413, 0.03874271363019943, 0.21578840911388397, 0.042211707681417465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3704316318035126, 0.08681416511535645, 0.024584650993347168, 0.02161632478237152, 0.03238872066140175, 0.005422732327133417, 0.07275230437517166, 0.011272783391177654, 0.063296839594841, 0.21726828813552856, 0.006367158144712448, 0.029603825882077217, 0.050998445600271225, 0.0071820481680333614, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19737640023231506, 0.04603997990489006, 0.043999847024679184, 0.13373447954654694, 0.05424824729561806, 0.02547573298215866, 0.027563508599996567, 0.021570958197116852, 0.05171830952167511, 0.06458097696304321, 0.028064647689461708, 0.23551581799983978, 0.019129808992147446, 0.02996351383626461, 0.021017691120505333, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0890723392367363, 0.01928837224841118, 0.1665354073047638, 0.07281266897916794, 0.04738639295101166, 0.02448790706694126, 0.028987349942326546, 0.01937035657465458, 0.026673005893826485, 0.07316633313894272, 0.02570459619164467, 0.042423538863658905, 0.05869465693831444, 0.028932664543390274, 0.1811905801296234, 0.09527374058961868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28167295455932617, 0.06441286206245422, 0.01800856553018093, 0.016169633716344833, 0.023183874785900116, 0.003753295633941889, 0.054722465574741364, 0.007909763604402542, 0.04616466537117958, 0.16947290301322937, 0.004361641593277454, 0.021011358126997948, 0.035490743815898895, 0.004932562820613384, 0.09555212408304214, 0.1472632735967636, 0.005917462054640055, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21305474638938904, 0.05912363901734352, 0.03382088616490364, 0.027476882562041283, 0.028393574059009552, 0.008422904647886753, 0.04008535295724869, 0.011629271320998669, 0.0529518648982048, 0.15404655039310455, 0.009831810370087624, 0.036101941019296646, 0.04737289994955063, 0.011069179512560368, 0.0997246578335762, 0.13971348106861115, 0.013185359537601471, 0.013994944281876087, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1587168425321579, 0.043878890573978424, 0.08712153136730194, 0.08998464047908783, 0.03073856793344021, 0.034148961305618286, 0.024917222559452057, 0.03139196336269379, 0.024823885411024094, 0.019790321588516235, 0.03625486046075821, 0.020694393664598465, 0.04284068942070007, 0.038209013640880585, 0.06234661489725113, 0.1091969758272171, 0.041376013308763504, 0.04916762188076973, 0.05440099909901619, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10485507547855377, 0.12122298032045364, 0.0648748055100441, 0.08768722414970398, 0.03434056043624878, 0.017483940348029137, 0.034151770174503326, 0.015289139933884144, 0.023312101140618324, 0.0283065028488636, 0.01872044801712036, 0.028111891821026802, 0.04190533235669136, 0.02098953351378441, 0.046785011887550354, 0.08659635484218597, 0.023631848394870758, 0.024273131042718887, 0.16702404618263245, 0.010438216850161552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2290196567773819, 0.05184384435415268, 0.013585195876657963, 0.012337245978415012, 0.018005413934588432, 0.00277036358602345, 0.042381320148706436, 0.005856258329004049, 0.03614484891295433, 0.1303924173116684, 0.0031534244772046804, 0.015672601759433746, 0.027800431475043297, 0.0035543229896575212, 0.07460807263851166, 0.11298273503780365, 0.004272277466952801, 0.006832202430814505, 0.18569737672805786, 0.01807365193963051, 0.005016353912651539, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18869921565055847, 0.03438713401556015, 0.022344740107655525, 0.019972724840044975, 0.016354044899344444, 0.006856550462543964, 0.02085905522108078, 0.005696007050573826, 0.03415916487574577, 0.07260990142822266, 0.007857217453420162, 0.018040230497717857, 0.026904486119747162, 0.009020397439599037, 0.06876441091299057, 0.17578724026679993, 0.01072007231414318, 0.009284548461437225, 0.19256405532360077, 0.025180349126458168, 0.012639050371944904, 0.021299341693520546, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11958326399326324, 0.022259412333369255, 0.03294713422656059, 0.020170224830508232, 0.035653289407491684, 0.013459896668791771, 0.017516467720270157, 0.010057867504656315, 0.025856422260403633, 0.059559524059295654, 0.015084492973983288, 0.01500872801989317, 0.05317461118102074, 0.01659758761525154, 0.041555244475603104, 0.13129346072673798, 0.01929665356874466, 0.015855036675930023, 0.17925113439559937, 0.01618380844593048, 0.022295497357845306, 0.015463380143046379, 0.10187691450119019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14615534245967865, 0.026727527379989624, 0.01662454754114151, 0.018987668678164482, 0.06278634071350098, 0.015317178331315517, 0.01979224570095539, 0.014227776788175106, 0.025458162650465965, 0.04530350863933563, 0.016364354640245438, 0.03749300166964531, 0.013288652524352074, 0.017496546730399132, 0.03994579240679741, 0.05881757661700249, 0.01926097460091114, 0.024616051465272903, 0.03821970894932747, 0.021577801555395126, 0.020949892699718475, 0.07973218709230423, 0.05017609894275665, 0.17068098485469818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11321665346622467, 0.03674636408686638, 0.011786588467657566, 0.010274868458509445, 0.02037067338824272, 0.005243887193500996, 0.015918847173452377, 0.0052667888812720776, 0.02489175647497177, 0.06593263149261475, 0.005933662410825491, 0.018209053203463554, 0.021020205691456795, 0.006667496636509895, 0.034828756004571915, 0.1374211311340332, 0.007927065715193748, 0.008618664927780628, 0.11377188563346863, 0.013557435013353825, 0.009277830831706524, 0.026121361181139946, 0.08499343693256378, 0.19073909521102905, 0.011263742111623287, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13337701559066772, 0.02621663361787796, 0.03827155753970146, 0.0715256929397583, 0.053177665919065475, 0.013925855979323387, 0.007084188982844353, 0.013450167141854763, 0.009841455146670341, 0.011789766140282154, 0.013537587597966194, 0.0381549671292305, 0.0419330894947052, 0.013882300816476345, 0.03707146644592285, 0.13838478922843933, 0.014846328645944595, 0.031569547951221466, 0.05598174408078194, 0.015536674298346043, 0.01595636084675789, 0.045455530285835266, 0.016699636355042458, 0.025325750932097435, 0.03671891987323761, 0.08028541505336761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10608962178230286, 0.019107704982161522, 0.024468697607517242, 0.02749636396765709, 0.016365809366106987, 0.005011425353586674, 0.010413089767098427, 0.006081137806177139, 0.005301063414663076, 0.011143163777887821, 0.00456538749858737, 0.018969886004924774, 0.004321119748055935, 0.0048149703070521355, 0.02940942719578743, 0.028682023286819458, 0.005097256042063236, 0.007234350778162479, 0.03412588685750961, 0.01037058886140585, 0.005643267650157213, 0.007283581420779228, 0.029389575123786926, 0.01003879215568304, 0.009134520776569843, 0.5466634035110474, 0.012777921743690968, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10467309504747391, 0.03321940079331398, 0.015341254882514477, 0.00937355775386095, 0.0265954677015543, 0.005787802394479513, 0.013571370393037796, 0.004554886370897293, 0.028058920055627823, 0.02610723301768303, 0.006353440694510937, 0.013315827585756779, 0.026628252118825912, 0.006888868752866983, 0.06204749643802643, 0.05890701711177826, 0.008068058639764786, 0.0075570945627987385, 0.0852278396487236, 0.01707577146589756, 0.009256883524358273, 0.019695747643709183, 0.12617811560630798, 0.13061511516571045, 0.011351046152412891, 0.08984367549419403, 0.04638150706887245, 0.007325300015509129, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08561902493238449, 0.021438920870423317, 0.05641260743141174, 0.05706656351685524, 0.019802186638116837, 0.00672749150544405, 0.0058091613464057446, 0.00451626256108284, 0.0031647509895265102, 0.017615189775824547, 0.006174588575959206, 0.08767962455749512, 0.012299853377044201, 0.0063504488207399845, 0.017522117123007774, 0.14295229315757751, 0.006585482973605394, 0.007875686511397362, 0.030078943818807602, 0.013907486572861671, 0.007376685738563538, 0.00768478075042367, 0.022160859778523445, 0.012385034002363682, 0.011890060268342495, 0.08669771999120712, 0.19902536273002625, 0.013594482094049454, 0.02958623133599758, 0.0, 0.0, 0.0, 0.0], [0.14064578711986542, 0.013298694975674152, 0.015702947974205017, 0.01735786162316799, 0.02233150042593479, 0.029672296717762947, 0.04172084853053093, 0.018995443359017372, 0.038277097046375275, 0.04863562062382698, 0.030946895480155945, 0.0160239115357399, 0.02088090591132641, 0.03243822976946831, 0.03055812232196331, 0.022808322682976723, 0.035377614200115204, 0.03145163878798485, 0.03497108817100525, 0.018679076805710793, 0.03821907192468643, 0.022578861564397812, 0.06819566339254379, 0.042140960693359375, 0.028620891273021698, 0.0377500019967556, 0.018578048795461655, 0.03376872465014458, 0.036416951566934586, 0.012956850230693817, 0.0, 0.0, 0.0], [0.07168620079755783, 0.06924455612897873, 0.01930689997971058, 0.014161988161504269, 0.016823207959532738, 0.019380588084459305, 0.0192574430257082, 0.0220036618411541, 0.013706511817872524, 0.035783782601356506, 0.01846517249941826, 0.052071668207645416, 0.020085200667381287, 0.01986212283372879, 0.02066211774945259, 0.04725160077214241, 0.02107672393321991, 0.036787260323762894, 0.024324096739292145, 0.0038275488186627626, 0.02392067387700081, 0.008533217012882233, 0.026241617277264595, 0.02738005667924881, 0.03461206331849098, 0.022884154692292213, 0.10047900676727295, 0.06913493573665619, 0.02547456882894039, 0.06495591998100281, 0.030615419149398804, 0.0, 0.0], [0.0691077709197998, 0.03701232373714447, 0.03862113133072853, 0.0593331977725029, 0.015923554077744484, 0.007918554358184338, 0.010371055454015732, 0.0066156755201518536, 0.002520078793168068, 0.026019345968961716, 0.007905225269496441, 0.029651982709765434, 0.04000627249479294, 0.008451342582702637, 0.010741154663264751, 0.050275642424821854, 0.009428859688341618, 0.013601027429103851, 0.050369229167699814, 0.03176712617278099, 0.010793017223477364, 0.007216794416308403, 0.006478426046669483, 0.014800583943724632, 0.02158595621585846, 0.15769484639167786, 0.08884761482477188, 0.0190169345587492, 0.02972932532429695, 0.03316134214401245, 0.05088368058204651, 0.034150827676057816, 0.0], [0.143755242228508, 0.016811029985547066, 0.009386664256453514, 0.006830304395407438, 0.01165685523301363, 0.0015672296285629272, 0.019711490720510483, 0.002398040611296892, 0.021235886961221695, 0.046836864203214645, 0.0016905141528695822, 0.005827189423143864, 0.01197999157011509, 0.0018251381115987897, 0.04231327399611473, 0.05491368845105171, 0.002178658964112401, 0.0024170123506337404, 0.09604062885046005, 0.00575287826359272, 0.0025577894411981106, 0.007121229078620672, 0.08889932185411453, 0.10852086544036865, 0.005179047118872404, 0.036577291786670685, 0.024719931185245514, 0.0037347592879086733, 0.031077083200216293, 0.016887931153178215, 0.09450864046812057, 0.07171519100666046, 0.0033723236992955208]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00042466612649150193, 0.9995753169059753, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005621915915980935, 0.01640729419887066, 0.9830304980278015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0011627591447904706, 0.021682024002075195, 0.003762045642361045, 0.9733932018280029, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.7244368286337703e-05, 0.00017202268645633012, 0.0002814393083099276, 0.0027421489357948303, 0.9967671632766724, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008268442004919052, 0.0002398546494077891, 7.361889583989978e-05, 6.43773382762447e-05, 0.0001756634155754, 0.9911779761314392, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012215052265673876, 0.005400451831519604, 0.0016716349637135863, 0.0004077567136846483, 0.0006163669168017805, 0.0010931201977655292, 0.9895892143249512, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012459784047678113, 0.0009121194598264992, 0.0005976704997010529, 0.0001365688513033092, 0.0003304107813164592, 0.0015722803073003888, 0.0038808132521808147, 0.9913241863250732, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00028217461658641696, 0.004068182315677404, 0.0026605194434523582, 0.0013093118323013186, 0.008030491881072521, 0.00028790938085876405, 0.00022922962671145797, 0.0003948425000999123, 0.9827372431755066, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [3.473947072052397e-05, 0.00039538557757623494, 0.00013272775686345994, 0.0002585236507002264, 0.0010855629807338119, 9.198080078931525e-05, 0.00032670944347046316, 0.0005427475553005934, 0.006105939392000437, 0.9910257458686829, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003378510009497404, 5.090866034151986e-05, 1.6452089766971767e-05, 1.692622754490003e-05, 4.18141862610355e-05, 0.493940532207489, 0.000129812877275981, 0.0008837349014356732, 3.221221413696185e-05, 2.7252222935203463e-05, 0.501481831073761, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.416041237069294e-05, 0.0013417191803455353, 0.0012613561702892184, 0.0021450724452733994, 0.004042354878038168, 0.0004830532125197351, 0.0001158263476099819, 0.00015203582006506622, 2.692530870262999e-05, 0.0001267510378966108, 0.0003128990938421339, 0.9899078607559204, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00037822354352101684, 0.0009837574325501919, 0.039341218769550323, 0.0027322531677782536, 0.0036680190823972225, 0.00011039189848816022, 0.00012931069068145007, 0.00021743540128227323, 0.00010623314301483333, 0.0007748204516246915, 6.647665577474982e-05, 0.0003148670948576182, 0.9511768817901611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002101717283949256, 2.4436794774373993e-05, 7.788794391672127e-06, 8.65166111907456e-06, 2.014007441175636e-05, 0.29971402883529663, 7.525275577791035e-05, 0.0004898315528407693, 1.8459588318364695e-05, 1.5344630810432136e-05, 0.32833895087242126, 4.1758154111448675e-05, 6.469192612712504e-06, 0.3691370487213135, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00014968313917052, 0.00011296608136035502, 0.000362948514521122, 0.00018591259140521288, 0.00016460906772408634, 4.1432256693951786e-05, 2.876477265090216e-05, 7.786943024257198e-05, 0.0009200984495691955, 0.010340160690248013, 2.7572339604375884e-05, 1.7833272067946382e-05, 0.00033054465893656015, 2.437568036839366e-05, 0.9872152805328369, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00010753756941994652, 0.0021781749092042446, 0.002042599953711033, 0.0042519038543105125, 0.006989853456616402, 2.511863385734614e-05, 0.0007779012667015195, 0.0005783525994047523, 0.002937830053269863, 0.03322520852088928, 1.7199152352986857e-05, 0.0008936442318372428, 0.0015238471096381545, 1.4656741768703796e-05, 0.006222610827535391, 0.938213586807251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013634890783578157, 1.3226022929302417e-05, 4.013938905700343e-06, 4.803066531167133e-06, 1.0257472240482457e-05, 0.196659654378891, 4.5272634451976046e-05, 0.00027762635727413, 1.171447820524918e-05, 9.473329555476084e-06, 0.22919608652591705, 2.6430674552102573e-05, 4.101836111658486e-06, 0.2657606899738312, 8.515346962667536e-06, 4.536025699053425e-06, 0.30660006403923035, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000892476353328675, 0.00013490785204339772, 4.779840674018487e-05, 5.803725071018562e-05, 0.00010480164200998843, 0.012799120508134365, 0.00071682472480461, 0.032579563558101654, 2.6450034056324512e-05, 0.00011185085168108344, 0.011884260922670364, 4.010274278698489e-05, 5.555409370572306e-05, 0.01237786840647459, 0.00010783470497699454, 5.404365947470069e-05, 0.013122137635946274, 0.9148864150047302, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [5.075300123280613e-06, 4.3961139454040676e-05, 3.398498665774241e-05, 7.94028674135916e-05, 5.477891318150796e-05, 7.921521500975359e-07, 9.313324881077278e-06, 7.727072443231009e-06, 8.597270789323375e-05, 0.00012274044274818152, 5.141469614500238e-07, 1.7027905414579436e-06, 3.8341757317539304e-05, 4.45096731027661e-07, 0.000139282361487858, 0.0003275801136624068, 3.994782389327156e-07, 3.948119683627738e-06, 0.9990440011024475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [9.346242586616427e-05, 0.0018395825754851103, 0.0025233232881873846, 0.01808730512857437, 0.0029363888315856457, 0.0002733591536525637, 4.872993304161355e-05, 0.00042127881897613406, 0.00015624420484527946, 0.0009748375159688294, 0.00020533663337118924, 0.001022887765429914, 0.0019548090640455484, 0.00019470402912702411, 0.0011294237338006496, 0.0016656133811920881, 0.00018734129844233394, 0.0009503475739620626, 0.0004455174785107374, 0.9648895859718323, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010859731119126081, 8.512308340868913e-06, 2.530730625949218e-06, 3.062529003727832e-06, 5.975133717583958e-06, 0.13292637467384338, 3.345322693348862e-05, 0.0001889127161120996, 8.47778755996842e-06, 6.540445610880852e-06, 0.16445913910865784, 1.8130085663869977e-05, 3.0627422802353976e-06, 0.1971915364265442, 6.429841050703544e-06, 3.4447357393219136e-06, 0.23317161202430725, 0.002279622247442603, 3.713436171892681e-06, 3.528802335495129e-05, 0.2685582935810089, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000419795309426263, 0.00011804993118857965, 0.000142405042424798, 3.7967813113937154e-05, 0.00019043147040065378, 0.0017651236848905683, 0.0005709825782105327, 0.0005008855951018631, 8.840797818265855e-05, 0.00014208692300599068, 0.00166398286819458, 3.348111567902379e-05, 2.441393917251844e-05, 0.0017546507297083735, 6.520306487800553e-05, 2.41426496359054e-05, 0.0018299160292372108, 0.0015691034495830536, 3.974889114033431e-05, 0.00015712386812083423, 0.0018554485868662596, 0.987006664276123, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.501791696995497e-05, 0.0014379785861819983, 6.345151632558554e-05, 0.0001086482880054973, 0.00015633198199793696, 3.210129079889157e-06, 0.0022032829001545906, 0.00022076703316997737, 5.2403021982172504e-05, 4.8815974878380075e-05, 2.264463546453044e-06, 1.5327264918596484e-05, 4.157136572757736e-06, 2.0228428638802143e-06, 6.2968028942123055e-06, 4.848669050261378e-05, 1.992902980418876e-06, 3.247019776608795e-05, 0.0012695096665993333, 1.963266549864784e-05, 1.809100240279804e-06, 0.0005810307338833809, 0.9936450719833374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.45016586733982e-05, 0.0006139398319646716, 0.0009361217962577939, 0.0008487811428494751, 0.0028506452217698097, 1.0365071830165107e-05, 0.00021614317665807903, 0.00017397581541445106, 0.0020508463494479656, 0.005805288907140493, 8.05523814051412e-06, 8.0866229836829e-05, 0.000770243932493031, 7.288198048627237e-06, 0.0010576173663139343, 0.002275601727887988, 6.663267868134426e-06, 0.00011621062003541738, 0.000597245991230011, 8.736296877032146e-05, 6.3323500398837496e-06, 6.096475772210397e-05, 6.090595343266614e-05, 0.9812840819358826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005848737782798707, 0.00015909112698864192, 1.0836430192284752e-05, 7.365470082731918e-05, 0.00011349514534231275, 0.0008256660075858235, 0.0003191101422999054, 0.01852998323738575, 1.0226610356767196e-05, 4.958722638548352e-05, 0.000771641090977937, 4.45481164206285e-05, 9.864987987384666e-06, 0.0008067156304605305, 2.2673822968499735e-05, 1.246411648025969e-05, 0.000844910042360425, 0.008790099062025547, 3.579234180506319e-05, 3.6628523957915604e-05, 0.0008917537634260952, 0.001079178531654179, 0.0003708448784891516, 0.00010837117588380352, 0.9654980301856995, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.6459925973322242e-05, 0.00017152438522316515, 3.208317866665311e-05, 0.00010234182263957337, 0.002631880808621645, 9.886194675345905e-06, 3.250848749303259e-05, 3.741761247511022e-05, 0.0001263151498278603, 4.9912203394342214e-05, 8.302540663862601e-06, 8.443406113656238e-05, 3.1279658287530765e-05, 7.633363566128537e-06, 1.0101352927449625e-05, 5.667386358254589e-05, 7.442136393365217e-06, 2.768962804111652e-05, 1.841832090576645e-05, 2.879437261071871e-06, 6.840434252808336e-06, 4.279871063772589e-06, 0.00043176571489311755, 0.00017617468256503344, 8.99557417142205e-05, 0.9958257675170898, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [2.6368121325504035e-05, 0.00018008741608355194, 0.000180829840246588, 0.0003046480705961585, 0.00039391053724102676, 4.674852971220389e-05, 2.723179386521224e-05, 4.873513171332888e-05, 0.000291317468509078, 0.0004206174344290048, 3.8046542613301426e-05, 0.00025245180586352944, 5.606737613561563e-05, 3.820379060925916e-05, 0.0015365143772214651, 0.001253725029528141, 3.593435030779801e-05, 2.3036636775941588e-05, 0.00018036275287158787, 0.00012266100384294987, 3.5178003599867225e-05, 6.924710760358721e-05, 0.0001126733623095788, 0.000850740063469857, 0.00014361929788719863, 0.00023528024030383676, 0.9930956959724426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0007048251572996378, 5.6753728131297976e-05, 3.3512797017465346e-06, 7.472657671314664e-06, 1.785946005838923e-05, 0.0008156509138643742, 4.9529389798408374e-05, 0.0013530978467315435, 3.741763430298306e-05, 0.00014377965999301523, 0.0007639314280822873, 9.99164785753237e-06, 2.884430159610929e-06, 0.0007552200695499778, 0.00010159582598134875, 3.1205381674226373e-06, 0.0008060936816036701, 0.001140674576163292, 1.4343090697366279e-05, 9.910148037306499e-06, 0.0008649174124002457, 8.663265907671303e-05, 3.469533839961514e-05, 0.00010265821038046852, 0.007663467433303595, 4.1878953197738156e-05, 3.4908596262539504e-06, 0.9844048619270325, 0.0, 0.0, 0.0, 0.0, 0.0], [2.028822564170696e-05, 2.9542541597038507e-05, 5.03748124174308e-05, 0.0009778323583304882, 0.3728250563144684, 6.661296083620982e-06, 1.573361805640161e-05, 3.981230111094192e-05, 0.0002235319698229432, 0.00012674290337599814, 5.118417448102264e-06, 0.00024116106214933097, 1.297378184972331e-05, 4.800675014848821e-06, 2.332158328499645e-05, 7.723515591351315e-05, 4.5934175432194024e-06, 1.9647057342808694e-05, 0.00021129852393642068, 1.1453472325229086e-05, 4.380487098387675e-06, 1.4442634892475326e-05, 3.676730557344854e-05, 0.00011845292465295643, 3.797710087383166e-05, 0.000780275440774858, 0.0004048357077408582, 1.0429539543110877e-05, 0.6236652135848999, 0.0, 0.0, 0.0, 0.0], [2.058288737316616e-05, 8.83124303072691e-05, 0.00020454842888284475, 0.00030188984237611294, 8.223895565606654e-05, 1.9704415535670705e-05, 0.00014085126167628914, 2.8963200747966766e-05, 7.669355909456499e-06, 3.7246169085847214e-05, 1.673677070357371e-05, 6.404696614481509e-05, 0.0006910577649250627, 1.6027615856728517e-05, 0.00015603665087837726, 0.00014825927792117, 1.5700084986747243e-05, 9.155390580417588e-05, 8.525107114110142e-05, 4.904517481918447e-06, 1.5784382412675768e-05, 5.293117283144966e-05, 0.0005298356991261244, 0.0005658396985381842, 6.167311948956922e-05, 6.729484448442236e-05, 0.0003077442815992981, 1.0369158189860173e-05, 6.799316906835884e-05, 0.9960988759994507, 0.0, 0.0, 0.0], [1.3371342902246397e-05, 0.0009821539279073477, 0.00041548223816789687, 0.00011442336108302698, 0.00038730859523639083, 5.660875558533007e-06, 0.0012746304273605347, 0.0005708085955120623, 0.0006383635918609798, 0.0005776684265583754, 4.127733063796768e-06, 9.16176759346854e-06, 9.142841008724645e-05, 3.7748673094029073e-06, 1.3575358025263995e-05, 0.0002916185185313225, 3.4474919630156364e-06, 7.89922196418047e-05, 0.003189126728102565, 4.885097041551489e-06, 3.165298494423041e-06, 1.4087480849411804e-05, 0.00015671442088205367, 0.00035448986454866827, 0.00017265502538066357, 0.0013050615089014173, 0.00021867705800104886, 2.6776768208947033e-05, 0.0002646023058332503, 1.333455293206498e-05, 0.9888004064559937, 0.0, 0.0], [2.7455167582957074e-05, 0.00016438323655165732, 7.996446220204234e-05, 0.0011915016220882535, 0.0007883652579039335, 2.6584270926832687e-06, 3.005776488862466e-05, 7.457997526216786e-06, 0.0001494103780714795, 2.8857297365902923e-05, 1.873834776233707e-06, 0.00033288131817243993, 5.160875298315659e-05, 1.757754830578051e-06, 0.00012656602484639734, 0.00014267080405261368, 1.6954520560830133e-06, 2.1952700990368612e-05, 0.00023040891392156482, 4.429355976753868e-05, 1.6103398365885369e-06, 2.7008076358470134e-05, 0.0002388488792348653, 0.00019046089437324554, 9.49615514400648e-06, 0.0004466450773179531, 0.00022095853637438267, 5.379206868383335e-06, 0.0006956426659598947, 0.00015470826474484056, 0.0002548548800405115, 0.9943286776542664, 0.0], [0.006231040693819523, 9.722769755171612e-05, 6.87158990331227e-06, 2.1151136024855077e-05, 5.828053326695226e-05, 0.007238905411213636, 2.0987936295568943e-05, 0.0002545907336752862, 6.243889220058918e-05, 2.0924722775816917e-05, 0.007872143760323524, 5.5853193771326914e-05, 9.868757842923515e-06, 0.009169397875666618, 7.202989218058065e-05, 7.068667400744744e-06, 0.010345972143113613, 0.0013096511829644442, 3.803680374403484e-05, 8.022704423638061e-05, 0.012053268030285835, 4.071991133969277e-05, 3.686068112074281e-06, 3.471339005045593e-05, 0.0005061817937530577, 8.918950334191322e-05, 2.911206138378475e-05, 0.0012772815534844995, 8.489640458719805e-05, 0.00018447372713126242, 0.00013425714860204607, 6.813054642407224e-05, 0.9425214529037476]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9430296421051025, 0.05697039142251015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9185556173324585, 0.03280001878738403, 0.048644375056028366, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8779289126396179, 0.056434210389852524, 0.042711883783340454, 0.02292497828602791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.804131805896759, 0.029098201543092728, 0.07556731253862381, 0.056435927748680115, 0.03476677089929581, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4943104088306427, 0.020183533430099487, 0.027966585010290146, 0.018319079652428627, 0.0314420722424984, 0.4077783226966858, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6057478785514832, 0.029242418706417084, 0.09491506963968277, 0.07609344273805618, 0.06614663451910019, 0.08705785870552063, 0.040796682238578796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.448382169008255, 0.045424290001392365, 0.07401473820209503, 0.06864848732948303, 0.09376634657382965, 0.08774266391992569, 0.06534271687269211, 0.11667852848768234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.49169281125068665, 0.13782063126564026, 0.03955018147826195, 0.061533182859420776, 0.04539967328310013, 0.040731415152549744, 0.06228703632950783, 0.05861859396100044, 0.06236648932099342, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5404127836227417, 0.044426631182432175, 0.03957848623394966, 0.041888054460287094, 0.07529863715171814, 0.04669500142335892, 0.0484752282500267, 0.055005162954330444, 0.08293061703443527, 0.025289500132203102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2827352285385132, 0.014234162867069244, 0.017647746950387955, 0.011433067731559277, 0.02174171805381775, 0.26665398478507996, 0.015403537079691887, 0.047349244356155396, 0.017767522484064102, 0.013926048763096333, 0.2911075949668884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.34496626257896423, 0.04611676186323166, 0.05771545320749283, 0.11131857335567474, 0.11289370805025101, 0.02793033979833126, 0.03859192878007889, 0.0565653033554554, 0.05864056572318077, 0.06648585200309753, 0.026114385575056076, 0.052660904824733734, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46992143988609314, 0.03201569244265556, 0.10772895812988281, 0.02700679562985897, 0.04465888440608978, 0.022773558273911476, 0.023117100819945335, 0.02549155056476593, 0.049502722918987274, 0.026573937386274338, 0.019708890467882156, 0.06337961554527283, 0.08812085539102554, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20661723613739014, 0.010379286482930183, 0.01226192805916071, 0.008319089189171791, 0.016007086262106895, 0.19527748227119446, 0.011453287675976753, 0.03475668281316757, 0.013073586858808994, 0.010938962921500206, 0.2160249799489975, 0.0058664968237280846, 0.023642314597964287, 0.23538152873516083, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3399351239204407, 0.029161576181650162, 0.09540709108114243, 0.03395186364650726, 0.08440455794334412, 0.012559524737298489, 0.02935865893959999, 0.024564139544963837, 0.10622438043355942, 0.046892132610082626, 0.011469585821032524, 0.006369195878505707, 0.11145269870758057, 0.011317990720272064, 0.056931525468826294, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40408799052238464, 0.024195652455091476, 0.03891008347272873, 0.01472743321210146, 0.024456581100821495, 0.03845001012086868, 0.03923038765788078, 0.0371713787317276, 0.06030011177062988, 0.04198543354868889, 0.037167176604270935, 0.01639126054942608, 0.03928966075181961, 0.037729810923337936, 0.13448581099510193, 0.011421306058764458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15876419842243195, 0.008203892968595028, 0.009295258671045303, 0.006221715360879898, 0.011793212033808231, 0.15069794654846191, 0.008851967751979828, 0.026313506066799164, 0.010186925530433655, 0.008433728478848934, 0.1676224023103714, 0.004420037381350994, 0.01821179687976837, 0.18350820243358612, 0.02092736028134823, 0.006447780877351761, 0.2001000940799713, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17539186775684357, 0.020569637417793274, 0.018291451036930084, 0.009298023767769337, 0.017377909272909164, 0.04253479093313217, 0.020701609551906586, 0.05044389143586159, 0.02543805167078972, 0.01721818931400776, 0.04311534762382507, 0.013349838554859161, 0.028528571128845215, 0.0459725558757782, 0.03408818691968918, 0.019834190607070923, 0.04992843046784401, 0.36791735887527466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26033350825309753, 0.017148351296782494, 0.037452761083841324, 0.07594790309667587, 0.04674701392650604, 0.018068524077534676, 0.03134648874402046, 0.03741513937711716, 0.07175811380147934, 0.05872533842921257, 0.017078718170523643, 0.040305912494659424, 0.057063572108745575, 0.017113585025072098, 0.10491339862346649, 0.04670538380742073, 0.017230205237865448, 0.02468215860426426, 0.019963949918746948, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.30547067523002625, 0.05190498009324074, 0.04346894845366478, 0.021846864372491837, 0.021017208695411682, 0.033904749900102615, 0.04190473631024361, 0.03909284994006157, 0.02887197583913803, 0.02300342358648777, 0.03205800801515579, 0.02333448827266693, 0.07110599428415298, 0.03290087729692459, 0.06164195016026497, 0.03183262422680855, 0.03376762196421623, 0.04571494087576866, 0.03501521423459053, 0.022141899913549423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12242932617664337, 0.006105288863182068, 0.006670298054814339, 0.004581777844578028, 0.009337792173027992, 0.11381801217794418, 0.006783366668969393, 0.019719887524843216, 0.007580860052257776, 0.006613638252019882, 0.12765932083129883, 0.0035026606637984514, 0.014002337120473385, 0.14000998437404633, 0.015684891492128372, 0.0050926050171256065, 0.1533621847629547, 0.0352742001414299, 0.02246563695371151, 0.006954455282539129, 0.17235152423381805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20538488030433655, 0.03477849066257477, 0.014682930894196033, 0.031183311715722084, 0.030931297689676285, 0.02195277437567711, 0.032908666878938675, 0.05740530043840408, 0.05587517470121384, 0.04864276200532913, 0.023520732298493385, 0.015108567662537098, 0.02738632634282112, 0.024518446996808052, 0.06060459092259407, 0.03477614000439644, 0.02613779343664646, 0.051684360951185226, 0.06281401962041855, 0.020291483029723167, 0.028601698577404022, 0.09081020951271057, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23978620767593384, 0.031323496252298355, 0.050377488136291504, 0.015869423747062683, 0.039014603942632675, 0.022805552929639816, 0.04285357519984245, 0.028882302343845367, 0.0404626689851284, 0.034107182174921036, 0.022644545882940292, 0.03923073410987854, 0.07238563895225525, 0.022345496341586113, 0.04958099126815796, 0.031933341175317764, 0.023325612768530846, 0.045213647186756134, 0.030554339289665222, 0.022876501083374023, 0.02473227120935917, 0.05549479275941849, 0.014199567958712578, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23581679165363312, 0.020558303222060204, 0.043750084936618805, 0.029704876244068146, 0.03703879192471504, 0.014953470788896084, 0.04004311561584473, 0.027184350416064262, 0.04576187580823898, 0.03809260204434395, 0.014181861653923988, 0.03789157792925835, 0.06518244743347168, 0.014182926155626774, 0.05489495024085045, 0.023720959201455116, 0.014592771418392658, 0.025570036843419075, 0.07356180995702744, 0.03918236494064331, 0.014925510622560978, 0.04628868028521538, 0.02780104987323284, 0.015118874609470367, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14254964888095856, 0.012010158970952034, 0.016881300136446953, 0.020742563530802727, 0.032451778650283813, 0.029626017436385155, 0.030295224860310555, 0.05620865523815155, 0.029608750715851784, 0.02948182448744774, 0.03263985365629196, 0.010038232430815697, 0.040786225348711014, 0.03462784364819527, 0.03391636162996292, 0.02015575021505356, 0.0368431881070137, 0.060646962374448776, 0.047447092831134796, 0.03252529352903366, 0.04040032997727394, 0.05947761610150337, 0.03129403293132782, 0.04792555421590805, 0.07141982018947601, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2222907692193985, 0.059111353009939194, 0.03702673688530922, 0.0405900813639164, 0.0272544976323843, 0.01917477697134018, 0.03171544149518013, 0.02046213299036026, 0.0381133109331131, 0.019927890971302986, 0.01853891648352146, 0.015436707064509392, 0.04536489397287369, 0.019355762749910355, 0.050358280539512634, 0.033281419426202774, 0.02017975226044655, 0.036794379353523254, 0.04331384226679802, 0.02847641333937645, 0.02131730318069458, 0.04771285504102707, 0.013107034377753735, 0.026336055248975754, 0.030211374163627625, 0.03454805910587311, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28438127040863037, 0.023870093747973442, 0.04641881212592125, 0.010260093957185745, 0.033909883350133896, 0.01830146834254265, 0.023708900436758995, 0.024766821414232254, 0.025767754763364792, 0.02296881005167961, 0.016735462471842766, 0.013406944461166859, 0.04598658159375191, 0.016673246398568153, 0.0810638889670372, 0.05033264309167862, 0.016726156696677208, 0.019904814660549164, 0.030325336381793022, 0.010146312415599823, 0.01731821894645691, 0.019040968269109726, 0.011081121861934662, 0.052046436816453934, 0.0333535373210907, 0.03880885988473892, 0.012695503421127796, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11854352056980133, 0.013350939378142357, 0.013422029092907906, 0.030273213982582092, 0.026162942871451378, 0.020776214078068733, 0.02128022350370884, 0.03798934444785118, 0.03536442294716835, 0.03665176033973694, 0.022004015743732452, 0.015380032360553741, 0.030023129656910896, 0.023299694061279297, 0.03000757098197937, 0.014861365780234337, 0.024521930143237114, 0.03999807685613632, 0.039001524448394775, 0.036007702350616455, 0.02685118466615677, 0.06795872002840042, 0.038931310176849365, 0.05725810304284096, 0.0665983334183693, 0.030756842344999313, 0.02334321290254593, 0.05938265845179558, 0.0, 0.0, 0.0, 0.0, 0.0], [0.25091859698295593, 0.011351881548762321, 0.023033957928419113, 0.018849778920412064, 0.013208827935159206, 0.016430404037237167, 0.03763701021671295, 0.021519416943192482, 0.03823148086667061, 0.031223079189658165, 0.016081079840660095, 0.01717902533710003, 0.08239651471376419, 0.015775414183735847, 0.04969315603375435, 0.03397902473807335, 0.016469120979309082, 0.02565610222518444, 0.053269289433956146, 0.02438097447156906, 0.017032889649271965, 0.031062550842761993, 0.014834257774055004, 0.04310779646039009, 0.02781837247312069, 0.015127929858863354, 0.012749838642776012, 0.026852669194340706, 0.01412958838045597, 0.0, 0.0, 0.0, 0.0], [0.1925693303346634, 0.02283366024494171, 0.01449588779360056, 0.028055034577846527, 0.03290427103638649, 0.018577102571725845, 0.02377675659954548, 0.014988928101956844, 0.027755485847592354, 0.019952144473791122, 0.018426906317472458, 0.02680850401520729, 0.04026201367378235, 0.018957369029521942, 0.019980253651738167, 0.03905026614665985, 0.019494349136948586, 0.0307143721729517, 0.07932326942682266, 0.036197640001773834, 0.020379580557346344, 0.023319313302636147, 0.018723852932453156, 0.056927718222141266, 0.02392755262553692, 0.039243247359991074, 0.02178351581096649, 0.020374197512865067, 0.041788291186094284, 0.00840925145894289, 0.0, 0.0, 0.0], [0.1891937106847763, 0.014352036640048027, 0.027829371392726898, 0.018936030566692352, 0.05455232411623001, 0.024302400648593903, 0.02075257897377014, 0.03050178475677967, 0.01690044067800045, 0.029904022812843323, 0.023111792281270027, 0.021664872765541077, 0.033359840512275696, 0.023050488904118538, 0.027927059680223465, 0.026253489777445793, 0.024257948622107506, 0.02439500391483307, 0.017666662111878395, 0.022081078961491585, 0.0254414901137352, 0.023996757343411446, 0.015941407531499863, 0.021863490343093872, 0.04876279830932617, 0.010234061628580093, 0.02576296776533127, 0.0619233064353466, 0.06737111508846283, 0.018184151500463486, 0.00952550582587719, 0.0, 0.0], [0.27111950516700745, 0.054605744779109955, 0.03970595821738243, 0.03512892872095108, 0.020313765853643417, 0.00818385649472475, 0.022556660696864128, 0.014036435633897781, 0.026198500767350197, 0.03235533460974693, 0.00718503026291728, 0.010813497006893158, 0.05162610486149788, 0.007057542912662029, 0.04232778400182724, 0.018505994230508804, 0.006952997762709856, 0.012550858780741692, 0.037855178117752075, 0.014787383377552032, 0.007130758836865425, 0.018956726416945457, 0.010758984833955765, 0.02354053221642971, 0.01625620573759079, 0.007915413938462734, 0.03652312234044075, 0.014562265016138554, 0.019251450896263123, 0.010911823250353336, 0.0343966968357563, 0.06592897325754166, 0.0], [0.14836527407169342, 0.014506884850561619, 0.007646299432963133, 0.01103875320404768, 0.026302486658096313, 0.013194822706282139, 0.027184901759028435, 0.017392193898558617, 0.01612214744091034, 0.032880786806344986, 0.014050263911485672, 0.007167352829128504, 0.0161200612783432, 0.014647050760686398, 0.01845093071460724, 0.013102104887366295, 0.015283220447599888, 0.014543000608682632, 0.05961224064230919, 0.02238389290869236, 0.016796963289380074, 0.10928267240524292, 0.07263997942209244, 0.08393712341785431, 0.02524900622665882, 0.03774845600128174, 0.01622694544494152, 0.02495386451482773, 0.0421658493578434, 0.004525821655988693, 0.014491353183984756, 0.019145682454109192, 0.022841641679406166]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09646998345851898, 0.9035300612449646, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0432523749768734, 0.08177726715803146, 0.8749703168869019, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09995394945144653, 0.02531275525689125, 0.020108118653297424, 0.8546252250671387, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024889379739761353, 0.00320735527202487, 0.0018421636195853353, 0.02236148901283741, 0.9476996660232544, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10732363909482956, 0.017841702327132225, 0.019553320482373238, 0.04333319142460823, 0.10211507230997086, 0.709833025932312, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00642632320523262, 0.0004479778581298888, 0.0001475652097724378, 0.00046936573926359415, 0.0014411847805604339, 0.003859680611640215, 0.9872079491615295, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010354559635743499, 0.0001990190357901156, 0.00016020357725210488, 6.937281432328746e-05, 0.0003867412160616368, 0.005171590019017458, 0.8964061141014099, 0.09657150506973267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0012883326271548867, 0.00032334530260413885, 0.00026527236332185566, 0.0002549061318859458, 0.00020129822951275855, 0.00010049052798422053, 0.0005700899637304246, 0.00040913207340054214, 0.9965870380401611, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00018741657549981028, 1.6196667274925858e-05, 4.2281239984731656e-06, 0.00028759046108461916, 1.1125746823381633e-05, 9.805691661313176e-06, 0.00015567266382277012, 7.632956840097904e-05, 0.00348692387342453, 0.9957646131515503, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015741532668471336, 0.0006393645890057087, 0.0004571522877085954, 0.0009912531822919846, 0.002114035189151764, 0.018089795485138893, 0.04712047427892685, 0.07010910660028458, 0.06152845174074173, 0.27690398693084717, 0.5063048601150513, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0006166559760458767, 0.000520705827511847, 4.611511030816473e-05, 0.0014612986706197262, 0.0005623743636533618, 4.447586979949847e-05, 0.0003653964668046683, 0.000286015507299453, 0.004506112076342106, 0.005816496443003416, 0.0007244516164064407, 0.9850499033927917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010478593176230788, 3.062814721488394e-05, 0.0001734501274768263, 5.423119000624865e-05, 6.388423207681626e-05, 1.1261106919846497e-05, 1.716897531878203e-05, 1.3931281500845216e-05, 0.0020760430488735437, 0.0002692678535822779, 0.00015268487913999707, 0.0036844410933554173, 0.992405116558075, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010746271349489689, 0.0002535446546971798, 0.00015322914987336844, 0.0002928698668256402, 0.0005376792978495359, 0.004423544276505709, 0.009884358383715153, 0.012843026779592037, 0.012738646939396858, 0.05966848134994507, 0.10772973299026489, 0.024745728820562363, 0.07808692008256912, 0.6778959035873413, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00041218046681024134, 2.6218816856271587e-05, 1.7755941371433437e-05, 0.00019181940297130495, 3.2979589832393685e-06, 3.912686224794015e-06, 1.0438119716127403e-05, 4.9068735279433895e-06, 0.0005868582520633936, 0.0030381500255316496, 3.6931392969563603e-05, 0.0007724316092208028, 0.009622628800570965, 0.00016094428428914398, 0.9851115345954895, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00010886572999879718, 3.584548949220334e-06, 5.191637683310546e-06, 3.083834599237889e-05, 1.8408711184747517e-05, 7.954743637128558e-07, 3.3532044199091615e-06, 6.574359304067912e-06, 0.0007270944188348949, 0.0018232448492199183, 7.853252100176178e-06, 0.00030627541127614677, 0.006975052412599325, 3.485084016574547e-05, 0.02829268015921116, 0.96165531873703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006073986645787954, 8.815236651571468e-05, 4.5837816287530586e-05, 8.250484825111926e-05, 0.0001272043155040592, 0.0009312531328760087, 0.0016700031701475382, 0.0019381921738386154, 0.0021017047110944986, 0.0101316561922431, 0.016047285869717598, 0.0038353940472006798, 0.012242107652127743, 0.10037077218294144, 0.07853121310472488, 0.10997345298528671, 0.655809223651886, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002186297671869397, 1.189295289805159e-05, 1.67359448823845e-05, 2.0815392417716794e-05, 2.1162533812457696e-05, 0.0007714920793659985, 0.0008652117685414851, 0.000556030310690403, 0.0001565594575367868, 0.0019499874906614423, 0.010891200043261051, 0.00039543077582493424, 0.001514469855464995, 0.0691632553935051, 0.004308092407882214, 0.004035159945487976, 0.49397045373916626, 0.4091656804084778, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.9798004359472543e-05, 4.972416718374006e-07, 9.165728620530444e-09, 1.3230464901425876e-07, 5.601831887247499e-08, 1.1903875218877147e-08, 6.153127856123319e-07, 4.068573034032852e-08, 4.517450179264415e-06, 1.4017736248206347e-05, 1.645694567287137e-07, 1.086386419046903e-06, 7.44408180253231e-06, 1.005665353659424e-06, 2.7594942366704345e-05, 0.00031378064886666834, 6.201154519658303e-06, 8.21336197986966e-06, 0.9995947480201721, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [6.497144113382092e-06, 8.691988000464335e-07, 1.4051367998035857e-06, 3.7704620581280324e-07, 3.329172955091053e-07, 6.1049398425439e-08, 2.542995858334507e-08, 1.1067719185575697e-07, 3.6708745028590783e-05, 5.704787895410846e-07, 6.172324447106803e-07, 0.00015627923130523413, 0.00014737916353624314, 3.0103678909654263e-06, 6.130625843070447e-05, 0.0017769880359992385, 1.8545939383329824e-05, 3.426297553232871e-05, 0.9233517050743103, 0.07440295070409775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.003649000544101, 3.7494446587515995e-05, 1.7470742022851482e-05, 2.844920709321741e-05, 3.312434637336992e-05, 0.00019569240976125002, 0.0002647596993483603, 0.00024690417922101915, 0.0002803326351568103, 0.0013949176063761115, 0.001791026908904314, 0.0004246801545377821, 0.0014623672468587756, 0.009717772714793682, 0.00805393885821104, 0.013200555928051472, 0.06315961480140686, 0.07152768224477768, 0.07523118704557419, 0.024205129593610764, 0.7250778675079346, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00064108642982319, 3.925694272766123e-06, 1.1607285159698222e-06, 1.537540924800851e-06, 9.072582543012686e-07, 8.058159437496215e-06, 1.3223896530689672e-05, 4.6198761083360296e-06, 3.67883694707416e-05, 4.565153722069226e-05, 7.577745418529958e-05, 2.2741603970644064e-05, 5.668425728799775e-05, 0.0004737495619338006, 0.00036407497827894986, 0.0004239998816046864, 0.003757303114980459, 0.00436309352517128, 0.007395204156637192, 0.005022608209401369, 0.055756114423274994, 0.9215315580368042, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005377669585868716, 8.122527788145817e-07, 2.301849235664122e-06, 1.470038341722102e-06, 1.87036073384661e-06, 6.024463914400258e-07, 4.127816737309331e-06, 1.6705681673556683e-06, 2.047191856036079e-06, 7.430003461195156e-05, 3.594218696889584e-06, 7.1714803198119625e-06, 3.052811371162534e-05, 1.8927716155303642e-05, 0.0002543860173318535, 0.00019496306777000427, 0.00012626512034330517, 0.000182304996997118, 0.0037352212239056826, 0.0006727108266204596, 0.0015798314707353711, 0.011177684180438519, 0.9813893437385559, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [8.512815838912502e-05, 4.870893803854415e-07, 1.2458967830752954e-06, 2.0811256717934157e-07, 1.9242898474658432e-07, 3.704009898797267e-08, 4.5552567939921573e-07, 5.5045351388116615e-08, 1.5320408692787169e-06, 5.0832995839300565e-06, 2.1400897765033733e-07, 1.4595349284718395e-06, 1.4768359505978879e-05, 1.0635034186634584e-06, 2.9461394660756923e-05, 5.728821997763589e-05, 6.615712663915474e-06, 7.844194442441221e-06, 0.0001506391417933628, 0.00022637045185547322, 7.97358006821014e-05, 0.00013040920021012425, 0.010868440382182598, 0.9883312582969666, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.000416634080465883, 2.2503838863485726e-06, 5.2705763664562255e-06, 2.7549111109692603e-05, 3.9630711398785934e-05, 7.514747721870663e-06, 8.100968443613965e-06, 1.1581404578464571e-05, 1.3206137737142853e-05, 6.750733155058697e-05, 2.5212779291905463e-05, 1.4860752344247885e-05, 7.521735096815974e-05, 0.00011457513755885884, 0.0002574535319581628, 0.0003666874545160681, 0.0006904731853865087, 0.001451848424039781, 0.0032460277434438467, 0.0006353446515277028, 0.008745609782636166, 0.015190966427326202, 0.018150264397263527, 0.13303391635417938, 0.8174023032188416, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.6739333659643307e-05, 8.245334015555272e-08, 3.257398617506624e-08, 2.8737504820242066e-08, 1.6217818199493195e-07, 1.5275355425004022e-09, 2.327868919849152e-08, 5.576728856482305e-09, 1.2825864814658416e-07, 1.3429499290396052e-07, 6.160561394352726e-09, 2.5232716893697216e-07, 3.205763050573296e-06, 2.6277326981016813e-08, 7.619933626301645e-07, 7.901974640844855e-06, 1.3940466203621327e-07, 1.955263257968909e-07, 0.00032022647792473435, 8.173685728252167e-07, 1.5630065490768175e-06, 3.763170525417081e-06, 0.0007450534612871706, 0.003715225262567401, 3.934413325623609e-05, 0.995144248008728, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00023842244991101325, 5.040101314079948e-06, 1.320854153163964e-06, 8.599819921073504e-06, 5.166068604012253e-06, 1.3470885562583135e-07, 6.71082943881629e-07, 2.699698029573483e-07, 6.34574962532497e-06, 4.41781448898837e-05, 2.7444934858067427e-07, 2.017121369135566e-05, 1.6526642866665497e-05, 8.122799499687972e-07, 0.0001077098204405047, 0.00012397383397910744, 3.50758546119323e-06, 8.505808182235342e-06, 6.980288162594661e-05, 0.00026813714066520333, 2.8792896046070382e-05, 0.00011957906826864928, 0.00012961667380295694, 0.007425930816680193, 0.0007109678699634969, 0.0058150021359324455, 0.9848405718803406, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0004089948197361082, 6.095393132454774e-07, 1.9963890736107714e-06, 9.310338100476656e-06, 3.76905654775328e-06, 1.9876615624525584e-06, 4.576555170388019e-07, 1.0690731642171158e-06, 3.087182449235115e-06, 9.2114050858072e-06, 4.724364316643914e-06, 1.2070041748302174e-06, 5.095595497550676e-06, 1.804716703190934e-05, 4.857854946749285e-05, 4.169393650954589e-05, 0.00010537471098359674, 0.00022466671362053603, 0.00011528689356055111, 0.00024928984930738807, 0.0013673165813088417, 0.001619037240743637, 0.0013275148812681437, 0.0034917867742478848, 0.06305630505084991, 0.006469035055488348, 0.09512616693973541, 0.8262884020805359, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00031804663012735546, 1.418178726453334e-06, 3.1854943927100976e-07, 2.782305045911926e-06, 5.388598219724372e-05, 4.300585487726494e-08, 5.386290808928607e-07, 8.497791270656307e-08, 5.999702352710301e-07, 4.11033352065715e-06, 7.099384902176098e-08, 8.454338740193634e-07, 2.52661493505002e-06, 2.198390376406678e-07, 6.098233825468924e-06, 2.1129922970430925e-05, 9.411764949618373e-07, 9.950347248377511e-07, 3.041363015654497e-05, 2.976994665004895e-06, 8.453843292954843e-06, 2.6007011911133304e-05, 0.00023055827477946877, 0.0013383362675085664, 0.0002175821573473513, 0.007014374248683453, 0.017924079671502113, 0.0005302675999701023, 0.9722621440887451, 0.0, 0.0, 0.0, 0.0], [1.850534800951209e-07, 5.565691907349901e-09, 7.262724310841406e-10, 7.746026930988137e-09, 1.5079624660074842e-08, 8.228505654628293e-10, 1.7915414707303512e-09, 1.734557830701533e-09, 1.0962638796740976e-08, 5.003293424010735e-08, 3.922815849932704e-09, 1.0013942386422059e-07, 2.3337879184737176e-08, 1.6555285142771936e-08, 5.1713588078428074e-08, 1.6146054804266896e-07, 9.827919456029122e-08, 2.7074386821368535e-07, 5.412023256212706e-06, 2.508412251245318e-07, 1.1806022257587756e-06, 2.8004360501654446e-06, 4.6103800741548184e-06, 0.00022679337416775525, 5.794219396193512e-05, 0.000451956526376307, 0.0001197653473354876, 0.00023980850528459996, 0.0018740134546533227, 0.9970144033432007, 0.0, 0.0, 0.0], [4.347461799625307e-05, 2.9688831091334578e-06, 3.9193795942082943e-07, 9.117064223573834e-07, 4.3644891434269084e-07, 4.68656358165731e-09, 9.240376641628245e-08, 1.2419347328318509e-08, 7.188977235728089e-08, 6.304536555035156e-07, 6.1089115988011145e-09, 1.998717635842695e-07, 1.4578756690752925e-06, 1.4727342012577083e-08, 1.7957289344394667e-07, 2.6422681912663393e-05, 5.112656253913883e-08, 4.710664569529399e-08, 0.0015761193353682756, 2.198947186116129e-06, 3.333991571707884e-07, 2.1814830688526854e-06, 0.0001624040596652776, 0.001777766039595008, 1.174917179014301e-05, 0.0036160883028060198, 6.199481867952272e-05, 3.516822835081257e-05, 0.0011488688178360462, 0.016085589304566383, 0.975442111492157, 0.0, 0.0], [0.0011562901781871915, 6.1990253925614525e-06, 6.137784112070221e-07, 3.1026431770442287e-06, 8.228386150221922e-07, 4.3659866832967964e-08, 9.363257191807861e-08, 2.773544238721115e-08, 5.109511107548315e-07, 1.2355227454463602e-06, 3.825290662007319e-08, 1.1272287565589068e-06, 1.9008884919458069e-06, 8.714058452596873e-08, 5.398163011705037e-06, 7.39340384825482e-06, 3.1288612944990746e-07, 3.0146230756145087e-07, 0.00013595329073723406, 1.3343799764697906e-05, 2.399650384177221e-06, 3.8712849345756695e-05, 0.0008219630108214915, 0.0006131264963187277, 2.151834087271709e-05, 0.004140722099691629, 0.0008760876953601837, 0.00011684650235110894, 0.0021590657997876406, 0.02752520702779293, 0.04434964060783386, 0.9179999232292175, 0.0], [0.003194286720827222, 4.681668724515475e-05, 3.499118975014426e-05, 2.7043015506933443e-05, 1.4000273040437605e-05, 1.7085087165469304e-05, 8.051880286075175e-06, 3.9098213164834306e-06, 1.0716941687860526e-05, 1.7930700778379105e-05, 1.4207461390469689e-05, 2.6911238819593564e-05, 2.6411980798002332e-05, 3.1100178603082895e-05, 1.1198552783753257e-05, 5.485475776367821e-05, 0.00011474460188765079, 0.00010493618901818991, 0.00028063077479600906, 0.0002083909057546407, 0.0009329267195425928, 0.0018990085227414966, 0.0011267595691606402, 0.0019257264211773872, 0.0039171441458165646, 0.0074994368478655815, 0.010695935226976871, 0.02319984696805477, 0.030398163944482803, 0.12729355692863464, 0.04325316101312637, 0.1766810268163681, 0.5669289827346802]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2530863285064697, 0.7469136118888855, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3067157566547394, 0.3290640115737915, 0.3642202317714691, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0741698369383812, 0.16189706325531006, 0.05432597175240517, 0.7096071243286133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1668822467327118, 0.039017848670482635, 0.03822457790374756, 0.213983416557312, 0.541891872882843, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19483649730682373, 0.21119239926338196, 0.051505573093891144, 0.0870383009314537, 0.22999566793441772, 0.2254316210746765, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13900136947631836, 0.02974863164126873, 0.03860689699649811, 0.05133270099759102, 0.19284243881702423, 0.0801236554980278, 0.4683443307876587, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08969783782958984, 0.04080602526664734, 0.03473304957151413, 0.08414527028799057, 0.09911056607961655, 0.07059445977210999, 0.13616584241390228, 0.4447469115257263, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05986521765589714, 0.02019040659070015, 0.01878528855741024, 0.1058470830321312, 0.057948797941207886, 0.02751770429313183, 0.056663043797016144, 0.08226768672466278, 0.5709148049354553, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010379263199865818, 0.004698239732533693, 0.004143982194364071, 0.007291391026228666, 0.00625670375302434, 0.003318049479275942, 0.009175020270049572, 0.018754439428448677, 0.033931612968444824, 0.9020512700080872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07270123809576035, 0.047764889895915985, 0.010500338859856129, 0.01615944877266884, 0.046178292483091354, 0.0524946004152298, 0.0518457293510437, 0.21189486980438232, 0.03515823185443878, 0.17267194390296936, 0.28263044357299805, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015126874670386314, 0.004731185268610716, 0.002322629326954484, 0.006575725972652435, 0.018362466245889664, 0.0033396603539586067, 0.008784218691289425, 0.007409723941236734, 0.006289810407906771, 0.07638946920633316, 0.012003478594124317, 0.8386646509170532, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03233194723725319, 0.010628949850797653, 0.002615161705762148, 0.0011762925423681736, 0.0030932938680052757, 0.0015055496478453279, 0.0070797149091959, 0.0028344676829874516, 0.005003898870199919, 0.012032576836645603, 0.003987776581197977, 0.051872383803129196, 0.8658380508422852, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05045585706830025, 0.026538828387856483, 0.005783263128250837, 0.008087278343737125, 0.021799398586153984, 0.025670336559414864, 0.022779027000069618, 0.09139888733625412, 0.015017371624708176, 0.07093915343284607, 0.12448617815971375, 0.12668335437774658, 0.061021383851766586, 0.34933963418006897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05474136769771576, 0.05296839773654938, 0.003988122567534447, 0.012351610697805882, 0.004415630362927914, 0.003596274182200432, 0.011385702528059483, 0.009828062728047371, 0.014749796129763126, 0.07078845798969269, 0.011209184303879738, 0.051640503108501434, 0.1973431408405304, 0.027129266411066055, 0.47386443614959717, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00716307433322072, 0.0049638389609754086, 0.0027692222502082586, 0.001942440401762724, 0.010544737800955772, 0.0014144884189590812, 0.0036636535078287125, 0.0031499513424932957, 0.00548131950199604, 0.02161405235528946, 0.003922200761735439, 0.0793507844209671, 0.22477854788303375, 0.009166963398456573, 0.02740269899368286, 0.5926719903945923, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.034611061215400696, 0.015296325087547302, 0.003095114603638649, 0.0038990885950624943, 0.009989406913518906, 0.011446868069469929, 0.009189342148602009, 0.033565063029527664, 0.005761213134974241, 0.025580983608961105, 0.043846819549798965, 0.04521357640624046, 0.02055337466299534, 0.11962341517210007, 0.15673229098320007, 0.13173440098762512, 0.32986170053482056, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01725388504564762, 0.00439752172678709, 0.004968680441379547, 0.007064308039844036, 0.006634903606027365, 0.0069109490141272545, 0.011386907659471035, 0.01617850363254547, 0.02031773142516613, 0.020396815612912178, 0.02342209219932556, 0.018909132108092308, 0.04024204611778259, 0.06200679391622543, 0.11645354330539703, 0.07975108921527863, 0.16864493489265442, 0.37506017088890076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0024591137189418077, 0.00024919668794609606, 3.4004558983724564e-05, 0.00021132899564690888, 0.0002006681461352855, 0.00012589232937898487, 0.00036506782635115087, 0.0003735979553312063, 0.00013798574218526483, 0.00047022095532156527, 0.00027151821996085346, 0.0034170662984251976, 0.0006049802759662271, 0.0005454595666378736, 0.0008937652455642819, 0.0015852100914344192, 0.0012150744441896677, 0.0028212678153067827, 0.9840185046195984, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.00015282395179383457, 0.03892352432012558, 0.00047151875332929194, 0.0003731815377250314, 3.271788591518998e-05, 1.0635606486175675e-05, 3.824563009402482e-06, 1.3244662113720551e-05, 6.418913835659623e-05, 2.734435474849306e-05, 2.8319833290879615e-05, 0.0022295645903795958, 0.0013866368681192398, 7.16350696166046e-05, 0.00014025518612470478, 0.008246419951319695, 0.00019678605895023793, 0.00020546656742226332, 0.8976384401321411, 0.04978356510400772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.026458123698830605, 0.010410879738628864, 0.00213325466029346, 0.002456019399687648, 0.005542603321373463, 0.0059545766562223434, 0.004195831716060638, 0.013451534323394299, 0.0023519471287727356, 0.009471959434449673, 0.0154334157705307, 0.014804987236857414, 0.006865768693387508, 0.037873197346925735, 0.04744937643408775, 0.045472852885723114, 0.1019919291138649, 0.08666393905878067, 0.06470173597335815, 0.1016075387597084, 0.39470866322517395, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021157901734113693, 0.0035484267864376307, 0.0017441109521314502, 0.00350930611602962, 0.004201900213956833, 0.002577777486294508, 0.004015855025500059, 0.004598999861627817, 0.0039551579393446445, 0.00645583588629961, 0.005907888989895582, 0.003733161836862564, 0.009013406932353973, 0.013927980326116085, 0.029895786195993423, 0.020082321017980576, 0.03804445639252663, 0.0688309594988823, 0.08152894675731659, 0.038340210914611816, 0.15160246193408966, 0.48332712054252625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0014211667003110051, 0.0004069005080964416, 0.00027144193882122636, 0.0014869217993691564, 0.0006475154077634215, 0.00025671461480669677, 0.0002729756524786353, 0.0005059845861978829, 0.0001753748074406758, 0.0012059577275067568, 0.000590287905652076, 0.0004307387280277908, 0.0003699952794704586, 0.0013322816230356693, 0.0007594246999360621, 0.0025183011312037706, 0.003599972929805517, 0.0047567179426550865, 0.011892912909388542, 0.0034102031495422125, 0.013851060532033443, 0.05907310172915459, 0.8907639980316162, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.005058850161731243, 0.0008823498501442373, 0.001535822288133204, 0.001690981793217361, 0.001343870535492897, 0.0004949694266542792, 0.000657223688904196, 0.0005521580460481346, 0.0005812085000798106, 0.002054555807262659, 0.0008380855433642864, 0.0010067472467198968, 0.0011300805490463972, 0.0016139892395585775, 0.005022862926125526, 0.014576834626495838, 0.0037143321242183447, 0.0046814726665616035, 0.007913792505860329, 0.006778375711292028, 0.012291330844163895, 0.02773294784128666, 0.10214629024267197, 0.7957008481025696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004004707094281912, 0.00045769792632199824, 0.0015645246021449566, 0.0031372106168419123, 0.001993749290704727, 0.0003789266920648515, 0.0004957793862558901, 0.0005698022432625294, 0.0010052089346572757, 0.00234322901815176, 0.0005772158619947731, 0.001675617415457964, 0.0030787389259785414, 0.0012285264674574137, 0.0052783372811973095, 0.006328069604933262, 0.0031517355237156153, 0.0053193275816738605, 0.021383140236139297, 0.006147717125713825, 0.012332228943705559, 0.07402164489030838, 0.12114028632640839, 0.5611114501953125, 0.16127510368824005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001128651900216937, 4.9889011279447004e-05, 4.129102308070287e-05, 9.249593858839944e-05, 5.5136115406639874e-05, 3.8617141399299726e-05, 5.592005982180126e-05, 0.00014896210632286966, 5.509204129339196e-06, 6.817406392656267e-05, 3.6204415664542466e-05, 4.623956556315534e-05, 0.0010654986836016178, 5.9198639064561576e-05, 7.787223148625344e-05, 0.00038421127828769386, 0.00011692402767948806, 0.00022428152442444116, 0.0005083971191197634, 0.00015628105029463768, 0.0003233136376366019, 0.0007086784462444484, 0.0009207401308231056, 0.0008559678099118173, 0.004414420109242201, 0.9884169697761536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0024299665819853544, 0.0036596294958144426, 0.0006344964494928718, 0.0006202631630003452, 0.0029876730404794216, 0.00034763594157993793, 0.0005059153190813959, 0.0004114957700949162, 0.00021321566600818187, 0.0006509343511424959, 0.00039556898991577327, 0.16986429691314697, 0.002188894897699356, 0.0006812365027144551, 0.0008683748310431838, 0.00890512764453888, 0.0014638531720265746, 0.001339457929134369, 0.006495121866464615, 0.007638711482286453, 0.004619386978447437, 0.0031617842614650726, 0.014200896956026554, 0.046396657824516296, 0.022786056622862816, 0.21939264237880707, 0.47714072465896606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015610331669449806, 0.0009648057166486979, 0.0010281818686053157, 0.007148520555347204, 0.005316654220223427, 0.0005432681646198034, 0.0007839888567104936, 0.00047083001118153334, 0.002298034494742751, 0.0054631014354527, 0.0005483069107867777, 0.0008661507745273411, 0.0009035553084686399, 0.0009251964511349797, 0.008375936187803745, 0.001447812537662685, 0.0020614934619516134, 0.003693381557241082, 0.017008870840072632, 0.006138685159385204, 0.007041890639811754, 0.04128703474998474, 0.06922490149736404, 0.26617175340652466, 0.05024215206503868, 0.1144513413310051, 0.13764874637126923, 0.23233507573604584, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02189534902572632, 0.0013514558086171746, 0.0013727983459830284, 0.005157731473445892, 0.010932818055152893, 0.0004452223074622452, 0.0016178302466869354, 0.0005713265854865313, 0.0007700271671637893, 0.003093542531132698, 0.00036163401091471314, 0.0009329034364782274, 0.0019032389391213655, 0.0005254384595900774, 0.0007036198512651026, 0.0015780648682266474, 0.0010200472315773368, 0.0009346728911623359, 0.005167576018720865, 0.0007855422445572913, 0.00287967873737216, 0.006337130907922983, 0.049514178186655045, 0.021886898204684258, 0.03004681132733822, 0.06941608339548111, 0.029953857883810997, 0.06111639738082886, 0.6677281260490417, 0.0, 0.0, 0.0, 0.0], [0.00029466647538356483, 8.085111039690673e-05, 0.0001199260659632273, 0.00017153994122054428, 0.0003899008734151721, 8.630251977592707e-05, 5.2537317969836295e-05, 6.61633675917983e-05, 0.00011378381896065548, 6.200092320796102e-05, 0.0001123799302149564, 0.0005867322906851768, 9.784972644411027e-05, 0.00021334855409804732, 0.0004044419911224395, 0.0004726233019027859, 0.0004730738000944257, 0.0005199523875489831, 0.0008571154903620481, 0.00022720191918779165, 0.0015976758440956473, 0.0014780618948861957, 0.0016501415520906448, 0.009339656680822372, 0.012792651541531086, 0.012606930918991566, 0.02165958099067211, 0.02886751852929592, 0.07784780114889145, 0.826757550239563, 0.0, 0.0, 0.0], [0.0006396527169272304, 0.00019920180784538388, 0.0003378455003257841, 0.0003228616260457784, 0.00038839824264869094, 2.2511394490720704e-05, 0.00011559643462533131, 1.968368269444909e-05, 8.749767584959045e-05, 0.00016675726510584354, 2.135315298801288e-05, 0.00018257660849485546, 0.0028796165715903044, 3.270821616752073e-05, 6.355970981530845e-05, 0.000907847483176738, 6.314180791378021e-05, 4.954530959366821e-05, 0.0050101811066269875, 0.00021555724379140884, 0.00018923268362414092, 0.00023579872504342347, 0.0021191712003201246, 0.0034752467181533575, 0.0004957506316713989, 0.01267300732433796, 0.0026274563279002905, 0.001192638766951859, 0.02801154926419258, 0.14214588701725006, 0.7951081395149231, 0.0, 0.0], [0.005492154974490404, 0.0014179638819769025, 6.310038588708267e-05, 0.0009282363462261856, 0.0004675870295614004, 9.701230010250583e-05, 5.6787015637382865e-05, 6.82404133840464e-05, 4.054810415254906e-05, 0.00021011468197684735, 6.53469905955717e-05, 0.00011957778770010918, 9.082314500119537e-05, 9.077344293473288e-05, 0.00013827483053319156, 0.0004122828249819577, 0.00016900571063160896, 0.00013570695591624826, 0.0024283200036734343, 0.00020654787658713758, 0.0004959595971740782, 0.00016561945085413754, 0.0018253992311656475, 0.0016808173386380076, 0.0010187330190092325, 0.015071463771164417, 0.0034083700738847256, 0.003384196199476719, 0.018865471705794334, 0.010037451982498169, 0.008259578607976437, 0.923088550567627, 0.0], [0.009386123158037663, 0.0017380556091666222, 0.0018091689562425017, 0.0014350239653140306, 0.00168128393124789, 0.001979130320250988, 0.0014143451116979122, 0.002112781163305044, 0.0012016964610666037, 0.0010467666434124112, 0.0012532500550150871, 0.0012125717476010323, 0.0006952434778213501, 0.0016850200481712818, 0.0012904978357255459, 0.0014949428150430322, 0.0030524677131325006, 0.006176418624818325, 0.00399315170943737, 0.0029945229180157185, 0.008827606216073036, 0.009542383253574371, 0.010477354750037193, 0.021451447159051895, 0.06221356615424156, 0.026861032471060753, 0.03390641137957573, 0.11987634003162384, 0.05027465149760246, 0.02679688297212124, 0.031165711581707, 0.10728646069765091, 0.4436677098274231]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.105205237865448, 0.894794762134552, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.039072152227163315, 0.0020172176882624626, 0.9589106440544128, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015578613616526127, 0.0008392521413043141, 0.0006979755708016455, 0.9828841686248779, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008856476284563541, 9.934298759617377e-06, 1.1174854989803862e-05, 0.0003024709003511816, 0.9908199906349182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.35445019602775574, 0.03058924898505211, 0.05988965556025505, 0.022903505712747574, 0.047475941479206085, 0.48469144105911255, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.044769033789634705, 0.001566468272358179, 0.00037738564424216747, 0.00025073293363675475, 0.00040889729280024767, 0.0002606566122267395, 0.952366828918457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09058354794979095, 0.0011470768367871642, 0.0060273525305092335, 0.0005468479357659817, 0.0017094239592552185, 0.0037850849330425262, 0.0026845417451113462, 0.8935161232948303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.006010730750858784, 2.428906736895442e-05, 0.0003126671363133937, 1.8682932932279073e-05, 0.00029792427085340023, 9.904524631565437e-06, 6.61954800307285e-06, 7.912213391136902e-07, 0.9933183193206787, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.004546829964965582, 0.00020019362273160368, 0.0002992021618410945, 0.0013363925972953439, 0.0003267661377321929, 8.741366741560341e-07, 1.7415688489563763e-05, 2.7834195748255297e-07, 0.00029665185138583183, 0.9929754734039307, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12262555956840515, 0.02392653562128544, 0.03827650099992752, 0.01617193967103958, 0.030366208404302597, 0.29375919699668884, 0.10661347955465317, 0.07059957832098007, 0.04190003126859665, 0.017833082005381584, 0.2379278838634491, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0008121037972159684, 0.0001154663841589354, 0.00013500906061381102, 5.539286939892918e-05, 0.0001317410496994853, 1.0027177950178157e-06, 1.3799076441500802e-06, 1.6001614255856111e-07, 1.8972953057527775e-06, 5.419661306405033e-07, 4.753096050080785e-07, 0.9987448453903198, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013333045644685626, 0.00010533048043726012, 0.0007669594488106668, 0.0009021886507980525, 3.5708430914382916e-06, 8.120491088448034e-07, 1.4038076301403635e-07, 9.56129397877703e-08, 1.6865054703885107e-06, 2.9789067411911674e-05, 4.3813446382046095e-07, 1.196580001305847e-06, 0.9968544840812683, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08159803599119186, 0.021388843655586243, 0.028877172619104385, 0.012675776146352291, 0.02493157796561718, 0.22996479272842407, 0.10401231050491333, 0.06472102552652359, 0.03460090607404709, 0.015198727138340473, 0.18564170598983765, 0.020528055727481842, 0.005003849044442177, 0.17085722088813782, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0013488945551216602, 8.921244443627074e-05, 0.0006859979475848377, 0.0006963235209695995, 2.1248761186143383e-05, 3.763407789847406e-07, 6.226761684047233e-07, 4.3072364519503026e-07, 7.091919997037621e-06, 0.0013103761011734605, 1.6113857270738663e-07, 9.548797379466123e-07, 0.00016453684656880796, 1.1531989940749554e-07, 0.9956735968589783, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001086744712665677, 0.0004072468145750463, 0.00010700269194785506, 0.0006782655837014318, 0.00011028441076632589, 1.318414888373809e-07, 2.660250402186648e-07, 4.619545563855354e-07, 8.409649126406293e-06, 0.0007784856716170907, 5.109410139425563e-08, 8.792001722213172e-07, 4.9342655984219164e-05, 3.4892963185484405e-08, 0.00011441121023381129, 0.9966579675674438, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.058680951595306396, 0.01878572441637516, 0.024067310616374016, 0.011013146489858627, 0.021456390619277954, 0.19110558927059174, 0.09902723133563995, 0.060658685863018036, 0.030162731185555458, 0.014801926910877228, 0.15370340645313263, 0.016615159809589386, 0.00438538147136569, 0.14085333049297333, 0.015014993958175182, 0.005390200298279524, 0.13427789509296417, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07323036342859268, 0.028374604880809784, 0.02105824649333954, 0.0076118698343634605, 0.010053575038909912, 0.06553945690393448, 0.14559738337993622, 0.08620841801166534, 0.010241254232823849, 0.007425738964229822, 0.04526273161172867, 0.0066214557737112045, 0.0011621786979958415, 0.03923851624131203, 0.008256865665316582, 0.003065684577450156, 0.03612261265516281, 0.4049290716648102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [7.94449370005168e-05, 1.8382123698756914e-06, 3.7134324060161816e-08, 1.5575218640151434e-05, 3.472372327451012e-06, 1.3165072365595165e-09, 2.7038657535172206e-08, 1.7186684075909398e-08, 9.232245190560207e-08, 1.350640468444908e-05, 4.4787887287789374e-10, 2.0416770496467507e-09, 5.5420287026208825e-08, 3.0638608317090643e-10, 4.2177114778496616e-07, 7.065880708978511e-06, 2.424439260462208e-10, 1.4397331105087119e-09, 0.9998784065246582, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0029344952199608088, 0.0005883852718397975, 0.004250301048159599, 0.00402846047654748, 0.00011592989176278934, 4.331626314524328e-06, 6.7739006226474885e-06, 7.07646831870079e-05, 0.00024952704552561045, 0.0003159344778396189, 2.082445917039877e-06, 4.4092172174714506e-05, 0.00030446963501162827, 1.47026048580301e-06, 0.0001976140047190711, 0.0013599283993244171, 1.2478676580940373e-06, 6.875182407384273e-06, 0.0001926120457937941, 0.9853248000144958, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04056526720523834, 0.017898287624120712, 0.02060096152126789, 0.00982110109180212, 0.017882799729704857, 0.15653996169567108, 0.09208226203918457, 0.05628826469182968, 0.02666132152080536, 0.014471277594566345, 0.12537632882595062, 0.01398676261305809, 0.003927926998585463, 0.11453620344400406, 0.013395361602306366, 0.004846654832363129, 0.10880253463983536, 0.04953395947813988, 0.005186405498534441, 0.004420870915055275, 0.10317545384168625, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.024536311626434326, 0.00312433042563498, 0.0009699293877929449, 4.631534102372825e-05, 0.0001343473995802924, 0.000645393505692482, 0.011668854393064976, 0.0003271342720836401, 0.00045730415149591863, 2.3976099328137934e-05, 0.0003445419133640826, 4.996414281777106e-05, 2.5757384719327092e-05, 0.00028217723593115807, 6.52531161904335e-06, 4.981794972991338e-06, 0.00023553981736768037, 0.0002664513885974884, 2.531108293624129e-05, 2.9761067708022892e-05, 0.00018809233733918518, 0.9566069841384888, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.002075989730656147, 0.00014591730723623186, 5.2898827561875805e-05, 4.21806144004222e-05, 4.73870532005094e-05, 4.142315219723969e-07, 0.00012964261986780912, 2.4250950446003117e-05, 1.803400891731144e-06, 1.483925302636635e-06, 1.4489951638552157e-07, 4.199916020297678e-06, 4.106833273453958e-08, 1.0260460925337611e-07, 8.285432500088064e-07, 5.781875529464742e-07, 7.695129511375853e-08, 6.669596785968679e-08, 6.47259657853283e-05, 1.5700886706326855e-06, 5.641808797918202e-08, 4.403027560329065e-06, 0.997401237487793, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005159112042747438, 0.0001181460902444087, 3.2705549529055133e-05, 0.00021188639220781624, 4.5635246351594105e-05, 2.924881243870914e-07, 3.057933781747124e-06, 7.607346930171843e-08, 0.000137536451802589, 0.0005283530335873365, 1.1575992431289706e-07, 4.028577222925378e-06, 2.0792888335563475e-06, 8.805391615851477e-08, 0.00010051658318843693, 4.431615161593072e-05, 6.963698950812613e-08, 9.373004417057018e-08, 3.3377655199728906e-05, 1.5888841517153196e-05, 5.610664288724365e-08, 7.05262834799214e-08, 9.389279512106441e-06, 0.9981963038444519, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06915251910686493, 0.01403214130550623, 0.005599116440862417, 0.0012576878070831299, 0.0008468025480397046, 0.009601178579032421, 0.008965795859694481, 0.05247703194618225, 0.00025028441450558603, 0.0009063888574019074, 0.005419919267296791, 0.0007309239590540528, 0.00019188012811355293, 0.004396584816277027, 0.005964726209640503, 0.0003558548050932586, 0.003877020673826337, 0.009727254509925842, 0.001102352631278336, 0.0003644298994913697, 0.00341396383009851, 0.0030523643363267183, 0.00569826178252697, 0.00029557369998656213, 0.7923198342323303, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.001046158722601831, 1.9557141058612615e-05, 1.7358959212288028e-06, 3.103052222286351e-05, 2.7307925847708248e-05, 1.0180698062356441e-08, 4.815767624677392e-07, 1.7494281223662256e-07, 7.31240987761339e-08, 7.983084060469992e-07, 3.4643101720632785e-09, 4.9067130021285266e-06, 1.0718952836441531e-07, 2.3525483783259915e-09, 2.3834189732951927e-07, 1.3731360013480298e-06, 1.828867168818249e-09, 6.247609984910696e-09, 9.208356459566858e-06, 1.63843026257382e-06, 1.2523899695082719e-09, 8.271274221094416e-10, 8.456778232357465e-06, 3.1186766591417836e-06, 9.241420073635709e-09, 0.9988435506820679, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0010544901015236974, 0.00021373135678004473, 0.00028157062479294837, 0.0011373325251042843, 0.00014783885853830725, 2.0068437152076513e-06, 4.8313372644770425e-06, 3.482321744741057e-06, 5.100321232021088e-06, 0.00012420427810866386, 9.301493264501914e-07, 1.0582552931737155e-05, 1.1688230188156012e-05, 7.077485406625783e-07, 0.003411579178646207, 0.00019905164663214236, 5.995607352815568e-07, 4.643020190542302e-07, 1.9585064364946447e-05, 0.00011087729217251763, 4.7500358846264135e-07, 3.2808694072627986e-07, 4.5353199311648495e-06, 5.161694571143016e-05, 3.247464519517962e-07, 1.307132242800435e-05, 0.9931889772415161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02420078031718731, 0.0028201311361044645, 0.004319292027503252, 0.0009108293452300131, 0.0017753959400579333, 0.00380991050042212, 0.002953318180516362, 0.016714762896299362, 0.0013931727735325694, 0.005003185011446476, 0.0022760885767638683, 0.0005876419600099325, 0.0009901640005409718, 0.0018066911725327373, 0.0031312357168644667, 0.00023389299167320132, 0.0016082704532891512, 0.009910299442708492, 0.0028539509512484074, 0.0002049512550001964, 0.001371264341287315, 0.006918998900800943, 0.0010336552513763309, 0.00014528397878166288, 0.017489735037088394, 0.00046553119318559766, 0.0003024083562195301, 0.884769082069397, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0005256073782220483, 3.528641173033975e-06, 1.7037637007888407e-06, 9.408189362147823e-05, 0.6647970080375671, 1.6531461710656004e-07, 9.511085750091297e-07, 8.886883620107255e-07, 1.701669862086419e-05, 3.7421796150738373e-05, 6.832259913380767e-08, 9.810881238081492e-06, 7.548715785787863e-08, 4.963256827750229e-08, 2.039385208263411e-06, 7.68653171689948e-06, 4.0668474809990585e-08, 2.674423882353949e-08, 3.343185017001815e-05, 4.3410705075075384e-07, 3.007985327485585e-08, 3.206054088877863e-08, 4.900570729660103e-06, 3.060936705878703e-06, 1.4603154241399352e-08, 2.214541564171668e-05, 9.21115315577481e-06, 3.164269202216019e-08, 0.3344285786151886, 0.0, 0.0, 0.0, 0.0], [0.004677004646509886, 0.0002802129602059722, 8.346085814991966e-05, 3.157012088195188e-06, 4.065350367454812e-07, 1.3343526461540023e-07, 2.6423560484545305e-05, 1.14399881567806e-07, 2.787240305224259e-07, 3.7496099594136467e-06, 4.6529063268963e-08, 1.7213949377037352e-06, 1.8165634685374243e-07, 3.148475968828279e-08, 1.8334247897655587e-07, 5.2147463236451586e-08, 2.521590403148366e-08, 3.078440968806717e-08, 7.142057256714907e-08, 1.6640109734566977e-08, 1.7768222448921733e-08, 8.634540904495225e-08, 2.8009699235553853e-06, 8.454152293779771e-07, 3.098357126418705e-08, 2.927871776137181e-07, 5.676238856722193e-07, 5.05543118478613e-09, 2.9141034474378102e-08, 0.9949179887771606, 0.0, 0.0, 0.0], [5.6368149671470746e-05, 0.00020023771503474563, 1.631914187782968e-06, 1.5089314047145308e-06, 1.7779770132619888e-05, 5.956453108524329e-10, 6.940396360732848e-06, 5.034030081674246e-09, 2.1682936335309932e-07, 1.716429096632055e-06, 1.9326304445677067e-10, 1.8911427090984034e-08, 9.53887777654927e-08, 1.2946885230569194e-10, 8.07718159023807e-09, 4.386632645037025e-05, 9.768721942471359e-11, 5.256090562788529e-10, 6.932191172381863e-05, 7.505895638360016e-08, 7.069291679817624e-11, 3.9880625024579786e-08, 1.061030798155116e-05, 2.2120773337519495e-06, 8.688650909860485e-10, 9.928469808073714e-06, 5.390105695113334e-09, 1.1354481244119086e-10, 3.060043582081562e-06, 3.8945776736909465e-07, 0.9995740056037903, 0.0, 0.0], [0.0003556972660589963, 1.9489458281896077e-05, 9.160958143183962e-06, 1.3085189493722282e-05, 2.3808875994291157e-05, 2.9718961513935938e-08, 4.1605944716138765e-05, 1.775652407332018e-07, 6.896221975694061e-07, 1.050622017828573e-06, 1.2230765733534099e-08, 4.290527613193262e-06, 3.2958271845018317e-07, 9.211071905212975e-09, 1.7466363715357147e-06, 4.682780854636803e-06, 7.44272554698e-09, 9.67378266381047e-09, 3.9665173972025514e-05, 7.324863986468699e-07, 5.543690839715509e-09, 1.0396706073834139e-07, 1.4401617590920068e-05, 8.873331353242975e-06, 5.9894045278952035e-09, 1.5627783795935102e-05, 2.981124680445646e-06, 6.874342872720263e-09, 4.854655344388448e-06, 7.07227172824787e-06, 6.210673745954409e-06, 0.999423623085022, 0.0], [0.05455273762345314, 0.016429057344794273, 0.016008341684937477, 0.006986881606280804, 0.024884166195988655, 0.07376927882432938, 0.07933787256479263, 0.055945541709661484, 0.027268195524811745, 0.01336877141147852, 0.055903106927871704, 0.007223020773380995, 0.00542658194899559, 0.050123270601034164, 0.008279060944914818, 0.0030609609093517065, 0.046822961419820786, 0.047518473118543625, 0.002844945527613163, 0.007037976291030645, 0.042827170342206955, 0.0513819083571434, 0.02728237211704254, 0.02637530490756035, 0.02615918032824993, 0.011997992172837257, 0.00622012373059988, 0.036487795412540436, 0.011124706827104092, 0.005218565929681063, 0.0025592059828341007, 0.002604029607027769, 0.1469704806804657]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.919265866279602, 0.08073416352272034, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4543740749359131, 0.42650413513183594, 0.11912176758050919, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5267521739006042, 0.22321629524230957, 0.12420622259378433, 0.12582534551620483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3903489112854004, 0.17099398374557495, 0.0687270313501358, 0.1837182193994522, 0.18621183931827545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19744211435317993, 0.25436848402023315, 0.14488451182842255, 0.18925853073596954, 0.20821957290172577, 0.005826787557452917, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19473563134670258, 0.13241828978061676, 0.1439192295074463, 0.1512073427438736, 0.30024486780166626, 0.02058463916182518, 0.056889984756708145, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12696664035320282, 0.21597391366958618, 0.16089320182800293, 0.1969882994890213, 0.1884463131427765, 0.013544921763241291, 0.07520710676908493, 0.021979697048664093, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2912123203277588, 0.059542302042245865, 0.11079402267932892, 0.09717851132154465, 0.223456010222435, 0.016262590885162354, 0.08719445765018463, 0.024762263521552086, 0.08959748595952988, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.22981953620910645, 0.03802673891186714, 0.16013826429843903, 0.11780407279729843, 0.1618119329214096, 0.03552818298339844, 0.07561113685369492, 0.020756034180521965, 0.10670631378889084, 0.05379777029156685, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13314422965049744, 0.1885865032672882, 0.10808096826076508, 0.15994970500469208, 0.17739221453666687, 0.0035980171523988247, 0.025062108412384987, 0.01286293100565672, 0.07453099638223648, 0.11275234818458557, 0.004039992578327656, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16746394336223602, 0.1435522437095642, 0.06118569150567055, 0.17358072102069855, 0.10113836824893951, 0.01755347289144993, 0.03239532187581062, 0.007995505817234516, 0.054534487426280975, 0.07883204519748688, 0.017258232459425926, 0.1445099413394928, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26996827125549316, 0.13029056787490845, 0.06605679541826248, 0.06156144663691521, 0.08437129855155945, 0.019846610724925995, 0.016312796622514725, 0.01296591479331255, 0.032314643263816833, 0.052547961473464966, 0.019868606701493263, 0.1369563341140747, 0.09693869203329086, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09820346534252167, 0.13618649542331696, 0.08005771785974503, 0.12152855098247528, 0.13579389452934265, 0.0024323537945747375, 0.017789499834179878, 0.009086490608751774, 0.05637529119849205, 0.08521632105112076, 0.0027533387765288353, 0.15203793346881866, 0.09946718066930771, 0.0030713954474776983, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1701289564371109, 0.08885710686445236, 0.090896837413311, 0.08085794746875763, 0.11427345126867294, 0.010901481844484806, 0.0147884301841259, 0.005497368052601814, 0.051512569189071655, 0.05330684036016464, 0.010595818050205708, 0.09442270547151566, 0.1309812217950821, 0.011153671890497208, 0.07182560861110687, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.103658527135849, 0.03588537871837616, 0.08803562074899673, 0.06087935343384743, 0.08348327875137329, 0.030805768445134163, 0.029615916311740875, 0.010918696410953999, 0.04049994423985481, 0.04288608953356743, 0.03219296410679817, 0.1500813364982605, 0.12658774852752686, 0.0349021814763546, 0.05825428292155266, 0.07131283730268478, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07047096639871597, 0.09600996226072311, 0.05621574446558952, 0.0869988352060318, 0.09934139251708984, 0.0016298789996653795, 0.012408633716404438, 0.0063804276287555695, 0.04101400822401047, 0.06348709017038345, 0.00184822257142514, 0.11387647688388824, 0.07328983396291733, 0.00207129237242043, 0.07246171683073044, 0.2001277357339859, 0.0023677798453718424, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05659438669681549, 0.08906213939189911, 0.04703238233923912, 0.09457873553037643, 0.08793899416923523, 0.0019721644930541515, 0.017767708748579025, 0.005990444216877222, 0.06014862284064293, 0.0883495956659317, 0.0022637699730694294, 0.10461518913507462, 0.06079373136162758, 0.0025566185358911753, 0.07818631827831268, 0.19496017694473267, 0.0029299899470061064, 0.00425905454903841, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1597062051296234, 0.026096433401107788, 0.045151758939027786, 0.03497577831149101, 0.08030673116445541, 0.0546903982758522, 0.040785420686006546, 0.026513049378991127, 0.027251925319433212, 0.03919817879796028, 0.053514353930950165, 0.04842919111251831, 0.0626426562666893, 0.05558211728930473, 0.05559668317437172, 0.04792961850762367, 0.05676327273249626, 0.04892301559448242, 0.035943206399679184, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08663119375705719, 0.095574289560318, 0.03673806041479111, 0.06946714967489243, 0.056814588606357574, 0.008190209046006203, 0.022195877507328987, 0.00764282513409853, 0.031468283385038376, 0.0755520686507225, 0.008965267799794674, 0.05263477936387062, 0.044911086559295654, 0.009935076348483562, 0.04481378570199013, 0.15198466181755066, 0.01093341875821352, 0.007993689738214016, 0.16402561962604523, 0.013528101146221161, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04783451557159424, 0.06669353693723679, 0.037989143282175064, 0.06040371209383011, 0.06899050623178482, 0.001026729354634881, 0.008172682486474514, 0.004158172756433487, 0.028268305584788322, 0.04407235607504845, 0.0011586500331759453, 0.07832919806241989, 0.050829093903303146, 0.0013004938373342156, 0.04904304817318916, 0.14145000278949738, 0.0014934343053027987, 0.0025566858239471912, 0.17534080147743225, 0.12910188734531403, 0.0017870557494461536, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04114089906215668, 0.047660794109106064, 0.03586997464299202, 0.0666610524058342, 0.07969272136688232, 0.002365075284615159, 0.02034020982682705, 0.004854497034102678, 0.06436208635568619, 0.06737151741981506, 0.002707370789721608, 0.05596053972840309, 0.05648665875196457, 0.0030767188873142004, 0.053543899208307266, 0.15165917575359344, 0.0035397184547036886, 0.004673932678997517, 0.164027139544487, 0.0596814788877964, 0.004154939204454422, 0.01016952097415924, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.066288061439991, 0.04092539846897125, 0.027322659268975258, 0.04505366086959839, 0.06575221568346024, 0.015657572075724602, 0.027881871908903122, 0.013711255043745041, 0.03174295648932457, 0.05280745029449463, 0.015447781421244144, 0.03218110650777817, 0.03290105611085892, 0.016298850998282433, 0.02663663774728775, 0.09772847592830658, 0.01711421087384224, 0.0169194545596838, 0.09847066551446915, 0.04787910357117653, 0.018412543460726738, 0.01533159427344799, 0.17753542959690094, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09801428020000458, 0.026328787207603455, 0.035868093371391296, 0.0571305938065052, 0.06943873316049576, 0.012128964066505432, 0.02090906724333763, 0.008747964166104794, 0.026999665424227715, 0.03726678714156151, 0.011716349981725216, 0.08499535173177719, 0.055013351142406464, 0.012196698226034641, 0.03563910722732544, 0.08789657801389694, 0.012970106676220894, 0.010851025581359863, 0.06408386677503586, 0.059639330953359604, 0.014492185786366463, 0.006093740463256836, 0.0976143628358841, 0.05396498367190361, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02769325114786625, 0.04073771461844444, 0.031223522499203682, 0.058790404349565506, 0.056018292903900146, 0.001571477740071714, 0.011908047832548618, 0.0033397867809981108, 0.051765553653240204, 0.052912406623363495, 0.0018259849166497588, 0.06030033156275749, 0.028357191011309624, 0.00208780774846673, 0.050864819437265396, 0.07898831367492676, 0.0023987051099538803, 0.00290482840500772, 0.11135732382535934, 0.04586011916399002, 0.0028849192894995213, 0.011431239545345306, 0.12340569496154785, 0.13488471508026123, 0.006487553473562002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15390782058238983, 0.023786796256899834, 0.05320243909955025, 0.03963884338736534, 0.05711057782173157, 0.015628943219780922, 0.020206071436405182, 0.009252366609871387, 0.021311843767762184, 0.029029976576566696, 0.014741901308298111, 0.06123832240700722, 0.10112597793340683, 0.015157810412347317, 0.043353963643312454, 0.04090716689825058, 0.0157326553016901, 0.020073406398296356, 0.053749360144138336, 0.03468617796897888, 0.016989368945360184, 0.006368487607687712, 0.04042016342282295, 0.03106040135025978, 0.013937157578766346, 0.06738198548555374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0855509340763092, 0.035057391971349716, 0.05885251984000206, 0.03497907891869545, 0.05178742855787277, 0.002511992584913969, 0.004151224158704281, 0.0020859157666563988, 0.014081133529543877, 0.024491358548402786, 0.002376010175794363, 0.09535107016563416, 0.07766135036945343, 0.0024908827617764473, 0.045726750046014786, 0.05774114280939102, 0.0026603820733726025, 0.0033477614633738995, 0.04799828305840492, 0.026788843795657158, 0.0029587186872959137, 0.004655786789953709, 0.03607095405459404, 0.02159917913377285, 0.0047886366955935955, 0.22052210569381714, 0.03371306508779526, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.020706716924905777, 0.03719408065080643, 0.019723711535334587, 0.03894222900271416, 0.03182702139019966, 0.001225368701852858, 0.008815767243504524, 0.002405294217169285, 0.0345795601606369, 0.047298040241003036, 0.0013953783782199025, 0.05148203298449516, 0.020486747846007347, 0.0015608379617333412, 0.038261182606220245, 0.048637501895427704, 0.0017825295217335224, 0.0025446093641221523, 0.0654134526848793, 0.041885584592819214, 0.002096062758937478, 0.009401390329003334, 0.08460042625665665, 0.09019993245601654, 0.005403281655162573, 0.22932574152946472, 0.056922681629657745, 0.005882924422621727, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08582808077335358, 0.039671722799539566, 0.01484179962426424, 0.05653618276119232, 0.059606172144412994, 0.006987516302615404, 0.010110095143318176, 0.0028877253644168377, 0.015322837047278881, 0.025928771123290062, 0.006830449681729078, 0.03354886919260025, 0.02109353058040142, 0.007213154342025518, 0.01932336762547493, 0.052982211112976074, 0.007658615242689848, 0.005157838575541973, 0.06523176282644272, 0.01817001774907112, 0.008534703403711319, 0.004368167370557785, 0.026912471279501915, 0.04962795600295067, 0.0044043357484042645, 0.19388677179813385, 0.03933219611644745, 0.005363549571484327, 0.11263907700777054, 0.0, 0.0, 0.0, 0.0], [0.07040343433618546, 0.05926164984703064, 0.01533484272658825, 0.024139029905200005, 0.01572885923087597, 0.019492100924253464, 0.01731152832508087, 0.017303068190813065, 0.02056325227022171, 0.0451558381319046, 0.02181585505604744, 0.07079729437828064, 0.03006398305296898, 0.023540589958429337, 0.020684493705630302, 0.021163657307624817, 0.026445118710398674, 0.023540062829852104, 0.05663728341460228, 0.03369477391242981, 0.030707307159900665, 0.015993351116776466, 0.01547408476471901, 0.030803600326180458, 0.031805578619241714, 0.07969612628221512, 0.03437484800815582, 0.034537140280008316, 0.03062436915934086, 0.06290686130523682, 0.0, 0.0, 0.0], [0.05338500067591667, 0.029621507972478867, 0.021030088886618614, 0.023695703595876694, 0.033436402678489685, 0.018393823876976967, 0.02406071498990059, 0.02281004749238491, 0.01397236343473196, 0.028813544660806656, 0.019059738144278526, 0.01537198293954134, 0.025335311889648438, 0.019971968606114388, 0.014075730927288532, 0.03884446248412132, 0.021346215158700943, 0.019321419298648834, 0.05632651969790459, 0.021360037848353386, 0.02339460887014866, 0.023136645555496216, 0.06416387110948563, 0.06724687665700912, 0.034695107489824295, 0.04349025711417198, 0.03032722882926464, 0.03285708650946617, 0.05327508971095085, 0.0659981369972229, 0.04118252545595169, 0.0, 0.0], [0.181770920753479, 0.019533121958374977, 0.028002891689538956, 0.02911018393933773, 0.04134832322597504, 0.018019501119852066, 0.010471716523170471, 0.008382908068597317, 0.015335788019001484, 0.021839454770088196, 0.016893338412046432, 0.03806137293577194, 0.03643866255879402, 0.017464250326156616, 0.02612902969121933, 0.04011501744389534, 0.018317202106118202, 0.0124882273375988, 0.05136968567967415, 0.019443873316049576, 0.019889753311872482, 0.00357644772157073, 0.026232967153191566, 0.018429184332489967, 0.011723734438419342, 0.03272990882396698, 0.034808170050382614, 0.012035029008984566, 0.06030295789241791, 0.007873307913541794, 0.042272984981536865, 0.07959005981683731, 0.0], [0.015003396198153496, 0.020334873348474503, 0.007813431322574615, 0.01655358262360096, 0.016466109082102776, 0.00023809645790606737, 0.0019598975777626038, 0.0006898260326124728, 0.006483212113380432, 0.010734067298471928, 0.00024435322848148644, 0.014634690247476101, 0.009247620590031147, 0.00026059136143885553, 0.009339923970401287, 0.04272657260298729, 0.00028863915940746665, 0.0003811672795563936, 0.054749369621276855, 0.09452756494283676, 0.0003421133733354509, 0.0017270207172259688, 0.04745931550860405, 0.026613749563694, 0.0010238648392260075, 0.1653798520565033, 0.01418711431324482, 0.0013092333683744073, 0.04770653694868088, 0.011198488064110279, 0.22050784528255463, 0.13942605257034302, 0.000441861804574728]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9333657622337341, 0.06663426011800766, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.324523001909256, 0.5923725366592407, 0.0831044390797615, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14159728586673737, 0.5143812894821167, 0.2814413607120514, 0.06258003413677216, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2526225745677948, 0.05834057554602623, 0.10458862036466599, 0.4411163032054901, 0.1433318853378296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18051543831825256, 0.09828897565603256, 0.10873959958553314, 0.12262710183858871, 0.09995540231466293, 0.38987353444099426, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17192307114601135, 0.023321017622947693, 0.12332700937986374, 0.06076493114233017, 0.059717144817113876, 0.3475916087627411, 0.21335533261299133, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09627020359039307, 0.04683617874979973, 0.028257282450795174, 0.032392483204603195, 0.058650508522987366, 0.15829341113567352, 0.3099649250507355, 0.26933494210243225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09948227554559708, 0.06464175134897232, 0.028671789914369583, 0.07305674254894257, 0.029787158593535423, 0.1396002173423767, 0.20754747092723846, 0.2658022940158844, 0.09141024947166443, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05632041022181511, 0.02988590858876705, 0.013419928029179573, 0.0024075512774288654, 0.013245493173599243, 0.04564543813467026, 0.05522564798593521, 0.10027103126049042, 0.6579984426498413, 0.02558010071516037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04266997054219246, 0.010426479391753674, 0.010652228258550167, 0.013446654193103313, 0.009353158995509148, 0.04015978425741196, 0.07105191051959991, 0.13737663626670837, 0.10423263162374496, 0.2173660844564438, 0.34326446056365967, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.029683902859687805, 0.0025616432540118694, 0.004407490137964487, 0.03424866124987602, 0.014238757081329823, 0.028001822531223297, 0.022288037464022636, 0.07105974107980728, 0.040262315422296524, 0.5543039441108704, 0.173330619931221, 0.02561298757791519, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.040395934134721756, 0.008631574921309948, 0.005494233686476946, 0.003864143043756485, 0.023363124579191208, 0.027207037433981895, 0.025677388533949852, 0.050485216081142426, 0.051056157797575, 0.05041779577732086, 0.14164376258850098, 0.5115278363227844, 0.06023577228188515, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.02870638109743595, 0.004808458499610424, 0.0047236415557563305, 0.005382594186812639, 0.003456041682511568, 0.014470160007476807, 0.024037392809987068, 0.04162619262933731, 0.03567284718155861, 0.07060722261667252, 0.10830989480018616, 0.05811595544219017, 0.20621225237846375, 0.3938707709312439, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05342640355229378, 0.011923315934836864, 0.015572299249470234, 0.002112323185428977, 0.005851035472005606, 0.022170936688780785, 0.038184694945812225, 0.027411598712205887, 0.054642777889966965, 0.049108292907476425, 0.1232837438583374, 0.052871737629175186, 0.07545670121908188, 0.3701530396938324, 0.09783113747835159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.008346744813024998, 0.0002785278484225273, 0.0011786039685830474, 0.0021395517978817225, 0.002476179739460349, 0.002918301383033395, 0.004576913546770811, 0.003965605515986681, 0.004973044153302908, 0.011347800493240356, 0.013014082796871662, 0.00554596958681941, 0.020895622670650482, 0.038385458290576935, 0.8750313520431519, 0.004926211666315794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021700337529182434, 0.0026786725502461195, 0.002383928745985031, 0.0028156274929642677, 0.0015562367625534534, 0.006139400415122509, 0.009039836935698986, 0.01374086458235979, 0.012412197887897491, 0.024686545133590698, 0.03406897559762001, 0.01756138913333416, 0.060829490423202515, 0.12004058808088303, 0.11516799032688141, 0.1266370713710785, 0.42854082584381104, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.019681405276060104, 0.002866191091015935, 0.002752868691459298, 0.0023641984444111586, 0.0022205775603652, 0.004776973742991686, 0.00501540070399642, 0.0070207808166742325, 0.006602982524782419, 0.015824856236577034, 0.021779129281640053, 0.02237582579255104, 0.039055537432432175, 0.07342644780874252, 0.0722755640745163, 0.08103643357753754, 0.2556320130825043, 0.36529284715652466, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.034438736736774445, 0.002429509535431862, 0.0027038853149861097, 0.0010108962887898088, 0.0028212477918714285, 0.009310695342719555, 0.007474076002836227, 0.010526420548558235, 0.01934257708489895, 0.006583397276699543, 0.030525268986821175, 0.008166533894836903, 0.023945726454257965, 0.08047976344823837, 0.04428941011428833, 0.048865195363759995, 0.23266272246837616, 0.3870975077152252, 0.047326505184173584, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0036976884584873915, 0.006450962740927935, 0.0004921752843074501, 0.00030696034082211554, 0.00012400816194713116, 0.0010727392509579659, 0.0008858809596858919, 0.001380637288093567, 0.0008200196316465735, 0.0002731192798819393, 0.002783062867820263, 0.03529435396194458, 0.0035613777581602335, 0.007148324977606535, 0.0016474139411002398, 0.01673169806599617, 0.019172845408320427, 0.022339284420013428, 0.8733664751052856, 0.0024509693030267954, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013628816232085228, 0.00143844832200557, 0.0012188699329271913, 0.0012915877159684896, 0.0006118035526014864, 0.002140042372047901, 0.002746963407844305, 0.003336049150675535, 0.003091872902587056, 0.005988915450870991, 0.007237226702272892, 0.003547737840563059, 0.012891972437500954, 0.022229185327887535, 0.020824246108531952, 0.02629534900188446, 0.07648169249296188, 0.16261443495750427, 0.12638677656650543, 0.10034449398517609, 0.4056535065174103, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.021433025598526, 0.0007291649235412478, 0.0010889691766351461, 0.0008559627458453178, 0.0010556622873991728, 0.002354247961193323, 0.0018633343279361725, 0.0024869919288903475, 0.0040113674476742744, 0.0036456300877034664, 0.006636627949774265, 0.0024591570254415274, 0.008106647990643978, 0.019501373171806335, 0.013945511542260647, 0.01683163270354271, 0.06727868318557739, 0.10203751921653748, 0.07506158947944641, 0.0378551222383976, 0.3474159240722656, 0.2633458971977234, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018374405801296234, 0.0016340158181264997, 0.00026486621936783195, 0.0006277092033997178, 0.0007212882046587765, 0.0022947140969336033, 0.0014375800965353847, 0.0024335726629942656, 0.0010225039441138506, 0.00217309407889843, 0.005665971897542477, 0.0005857067881152034, 0.004730312619358301, 0.015789639204740524, 0.009121068753302097, 0.006447888910770416, 0.04965189844369888, 0.0722111389040947, 0.04530424252152443, 0.022536270320415497, 0.23380529880523682, 0.2810092568397522, 0.22215759754180908, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016010282561182976, 0.00041907624108716846, 0.0016793180257081985, 0.0020331591367721558, 0.0011126056779175997, 0.002304121619090438, 0.0016414901474490762, 0.002045233966782689, 0.0008826450211927295, 0.0035318126901984215, 0.004660749342292547, 0.001146125141531229, 0.004025290720164776, 0.011729470454156399, 0.006906012073159218, 0.00597897544503212, 0.0341360904276371, 0.053919967263936996, 0.07536915689706802, 0.015544159337878227, 0.1530730426311493, 0.2167249172925949, 0.23267333209514618, 0.1524529606103897, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.016415540128946304, 0.0010881023481488228, 0.0013027028180658817, 0.001233562477864325, 0.0010325247421860695, 0.0013605287531390786, 0.001135276979766786, 0.0009713214822113514, 0.0017580209532752633, 0.0023549972102046013, 0.0022677979432046413, 0.0026169978082180023, 0.003284963546320796, 0.005564449355006218, 0.007287736050784588, 0.00658267131075263, 0.016911093145608902, 0.0233481265604496, 0.03336372971534729, 0.012060885317623615, 0.08410517126321793, 0.10776961594820023, 0.12741312384605408, 0.2500959038734436, 0.2886751592159271, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.03111771121621132, 0.0015063020400702953, 0.00108393095433712, 0.002421510173007846, 0.0013826875947415829, 0.0037937178276479244, 0.0016339272260665894, 0.0031195187475532293, 0.0012812622589990497, 0.002068521222099662, 0.0047601149417459965, 0.0008757303585298359, 0.008592112921178341, 0.009353640489280224, 0.011993246152997017, 0.00596300745382905, 0.022028209641575813, 0.03411330655217171, 0.009586739353835583, 0.013681797310709953, 0.08417066186666489, 0.089178167283535, 0.10806825011968613, 0.15560758113861084, 0.37236881256103516, 0.02024945244193077, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01483848225325346, 0.0005757307517342269, 0.004743668250739574, 0.00045677414163947105, 0.0007412295672111213, 0.0014999997802078724, 0.000886608730070293, 0.0008452087640762329, 0.00010359250154579058, 0.0009065133635886014, 0.0018096824642270803, 0.060291074216365814, 0.0015377590898424387, 0.0033079383429139853, 0.0013713664375245571, 0.002082675462588668, 0.008643991313874722, 0.009088861756026745, 0.007760951295495033, 0.005808315239846706, 0.03386513888835907, 0.03987175598740578, 0.053745511919260025, 0.03694768249988556, 0.12259424477815628, 0.571870744228363, 0.013804479502141476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018569983541965485, 0.001214316114783287, 0.0012880476424470544, 0.0010612544137984514, 0.000749005121178925, 0.0010485624661669135, 0.0008687906665727496, 0.0005973867955617607, 0.0012938266154378653, 0.0014042533002793789, 0.0011055124923586845, 0.0005003444384783506, 0.0020262598991394043, 0.002146313199773431, 0.002873795572668314, 0.0026413716841489077, 0.005802995525300503, 0.008191362954676151, 0.008339066058397293, 0.005543309263885021, 0.026558691635727882, 0.03308912739157677, 0.032507386058568954, 0.09199851751327515, 0.10410501807928085, 0.16861799359321594, 0.20895428955554962, 0.26690319180488586, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01665445603430271, 0.0007248662877827883, 0.0008980676648207009, 0.003161686472594738, 0.000552282843273133, 0.0016607267316430807, 0.0007486131507903337, 0.0009652821463532746, 0.00038427996332757175, 0.002257774118334055, 0.0015545642236247659, 0.0007037004688754678, 0.001468819915316999, 0.0027998813893646, 0.003013980807736516, 0.0053690471686422825, 0.0065523493103682995, 0.00870157964527607, 0.00713887857273221, 0.006516327615827322, 0.026185549795627594, 0.03929194435477257, 0.031550083309412, 0.024343039840459824, 0.10670772194862366, 0.15159790217876434, 0.13674211502075195, 0.32013973593711853, 0.09161471575498581, 0.0, 0.0, 0.0, 0.0], [0.037226807326078415, 0.0007442718488164246, 0.0019180329982191324, 0.0014290612889453769, 0.0025597342755645514, 0.003292826935648918, 0.0009007261833176017, 0.0015445125754922628, 0.00035571990883909166, 0.00044927390990778804, 0.0026688247453421354, 0.0014046627329662442, 0.0017281024483963847, 0.00454272935166955, 0.0012941043823957443, 0.004677667748183012, 0.009138227440416813, 0.008992970921099186, 0.005343654192984104, 0.00203183782286942, 0.030930962413549423, 0.021215246990323067, 0.03639981895685196, 0.056794557720422745, 0.11195976287126541, 0.0114581398665905, 0.03353293985128403, 0.26588472723960876, 0.24695447087287903, 0.09262561798095703, 0.0, 0.0, 0.0], [0.044088803231716156, 0.0011082951677963138, 0.0033875813242048025, 0.004853175021708012, 0.002864324487745762, 0.004500849638134241, 0.0014518098905682564, 0.0021924914326518774, 0.0007902384386397898, 0.0006717375363223255, 0.0035771471448242664, 0.0005131764337420464, 0.012861737050116062, 0.005116838496178389, 0.003014250658452511, 0.005887454375624657, 0.010453402996063232, 0.013818317092955112, 0.008691029623150826, 0.011229273863136768, 0.0314009003341198, 0.023181604221463203, 0.030444325879216194, 0.03520680218935013, 0.09999874979257584, 0.006090156733989716, 0.1471424549818039, 0.16393040120601654, 0.12749527394771576, 0.1935773342847824, 0.0004599921521730721, 0.0, 0.0], [0.0371740460395813, 0.005006896331906319, 0.0016112314769998193, 0.0028301412239670753, 0.0005118498229421675, 0.002707122825086117, 0.0020970338955521584, 0.0012328801676630974, 0.0006310889148153365, 0.0017632442759349942, 0.0017868842696771026, 0.0002528337063267827, 0.0010720737045630813, 0.0026448334101587534, 0.001566158956848085, 0.0038084101397544146, 0.005276747513562441, 0.006930262316018343, 0.01331082358956337, 0.0035816491581499577, 0.018304504454135895, 0.02668478712439537, 0.019971473142504692, 0.03215579688549042, 0.07432058453559875, 0.02392229065299034, 0.08761484920978546, 0.18007564544677734, 0.027492549270391464, 0.059722401201725006, 0.1912270337343216, 0.1627119779586792, 0.0], [0.008116954937577248, 0.0008723053033463657, 0.0003707293944898993, 0.0004734859976451844, 0.00017833005404099822, 0.0003994698927272111, 0.0002951174683403224, 0.00018311997700948268, 0.0001671454665483907, 0.0002370250440435484, 0.0002533356018830091, 0.0001285500475205481, 0.000202131865080446, 0.0003994500730186701, 0.0001651181373745203, 0.00046228792052716017, 0.0008780232747085392, 0.0011140485294163227, 0.002070885617285967, 0.001963680377230048, 0.0035278573632240295, 0.00447520986199379, 0.004463495220988989, 0.0073125362396240234, 0.014446251094341278, 0.03503045067191124, 0.02001412957906723, 0.04388568177819252, 0.019436746835708618, 0.11325757950544357, 0.07277638465166092, 0.1619681566953659, 0.4804742634296417]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.947374701499939, 0.052625276148319244, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8785884380340576, 0.055394675582647324, 0.066016785800457, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7270768880844116, 0.1370987892150879, 0.048660945147275925, 0.08716339617967606, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.46517449617385864, 0.13429327309131622, 0.1535816490650177, 0.17472541332244873, 0.0722251757979393, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17035992443561554, 0.011841952800750732, 0.02365088276565075, 0.02174585685133934, 0.025856347754597664, 0.7465450167655945, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1421457678079605, 0.04383278265595436, 0.05021180212497711, 0.04516509547829628, 0.05872132256627083, 0.511152982711792, 0.14877018332481384, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08473357558250427, 0.020369846373796463, 0.037461645901203156, 0.03195912018418312, 0.03419603779911995, 0.43500640988349915, 0.1189408153295517, 0.2373325675725937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.342307984828949, 0.03593643382191658, 0.0725921094417572, 0.10352762043476105, 0.08196526020765305, 0.07694552093744278, 0.1419631540775299, 0.09896723181009293, 0.04579462110996246, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3711487352848053, 0.06278741359710693, 0.058069489896297455, 0.055662743747234344, 0.1312483847141266, 0.07535620033740997, 0.10726627707481384, 0.07924018800258636, 0.03010733425617218, 0.029113231226801872, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06430107355117798, 0.005169105716049671, 0.011012978851795197, 0.009054896421730518, 0.011748061515390873, 0.31884998083114624, 0.036849524825811386, 0.08319555968046188, 0.01206816453486681, 0.014676237478852272, 0.43307435512542725, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23777928948402405, 0.03812996670603752, 0.01974334567785263, 0.07508031278848648, 0.14420782029628754, 0.06244921311736107, 0.07018392533063889, 0.06523610651493073, 0.04883219301700592, 0.034600332379341125, 0.06439764052629471, 0.13935978710651398, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2901446521282196, 0.040857698768377304, 0.06965622305870056, 0.06467745453119278, 0.11515603214502335, 0.0727943629026413, 0.05406035855412483, 0.04095999896526337, 0.03646126762032509, 0.037097007036209106, 0.08112155646085739, 0.062494393438100815, 0.03451892361044884, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04191601648926735, 0.0033880749251693487, 0.007082352880388498, 0.005899209063500166, 0.007586178369820118, 0.20389774441719055, 0.02329680696129799, 0.05173078179359436, 0.00781682226806879, 0.009731397964060307, 0.27760177850723267, 0.008386765606701374, 0.016172707080841064, 0.3354933559894562, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15129360556602478, 0.04240788519382477, 0.043833814561367035, 0.09706773608922958, 0.14943550527095795, 0.044398728758096695, 0.061744991689920425, 0.04199517145752907, 0.018608827143907547, 0.031412456184625626, 0.05092639848589897, 0.06660281121730804, 0.09416217356920242, 0.057612959295511246, 0.04849692061543465, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29958033561706543, 0.07432074099779129, 0.030061950907111168, 0.01673649623990059, 0.03856480121612549, 0.05106865614652634, 0.09408661723136902, 0.05606793239712715, 0.05458230525255203, 0.04522043839097023, 0.056380681693553925, 0.015378035604953766, 0.04082847759127617, 0.06084691733121872, 0.030811045318841934, 0.03546449542045593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.030439993366599083, 0.002493396168574691, 0.00529821403324604, 0.004223393741995096, 0.0055174445733428, 0.14508451521396637, 0.01603417843580246, 0.0359865166246891, 0.005758060608059168, 0.007398183457553387, 0.19613467156887054, 0.0060908920131623745, 0.011465116403996944, 0.2373802810907364, 0.008031781762838364, 0.005861499346792698, 0.27680185437202454, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015668949112296104, 0.0019133498426526785, 0.006747936364263296, 0.005610876251012087, 0.006710362154990435, 0.1085851639509201, 0.013437001965939999, 0.03067483752965927, 0.008698048070073128, 0.005958770867437124, 0.15696406364440918, 0.0064012594521045685, 0.014357184059917927, 0.1922048032283783, 0.00993881095200777, 0.008154570125043392, 0.2286137342453003, 0.17936035990715027, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2751208245754242, 0.04635615274310112, 0.025782739743590355, 0.02037109062075615, 0.04519127309322357, 0.04741503298282623, 0.05800836160778999, 0.05449463054537773, 0.02185584418475628, 0.028674164786934853, 0.050547994673252106, 0.03578944131731987, 0.014343996532261372, 0.05505194514989853, 0.019970880821347237, 0.02507065422832966, 0.059836700558662415, 0.10591260343790054, 0.010205723345279694, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12210453301668167, 0.02900637313723564, 0.027926092967391014, 0.03076372668147087, 0.06431961059570312, 0.050758663564920425, 0.05746712163090706, 0.038544490933418274, 0.028814472258090973, 0.03641040623188019, 0.05530694127082825, 0.027555689215660095, 0.050623819231987, 0.06039692088961601, 0.03790908306837082, 0.034162599593400955, 0.06550323218107224, 0.08224613219499588, 0.08903128653764725, 0.011148842982947826, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01992274634540081, 0.0015991278924047947, 0.0035362676717340946, 0.0027606100775301456, 0.003674747422337532, 0.09198568761348724, 0.009868206456303596, 0.021762743592262268, 0.0036746228579431772, 0.004800304304808378, 0.12327175587415695, 0.003948220983147621, 0.007727703545242548, 0.14901947975158691, 0.005094496998935938, 0.0038495524786412716, 0.1743377447128296, 0.1450376659631729, 0.003139181761071086, 0.0039015659131109715, 0.21708746254444122, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.025169149041175842, 0.002589144976809621, 0.0079044783487916, 0.002818173263221979, 0.008317489176988602, 0.0628475621342659, 0.01609738916158676, 0.02483358606696129, 0.007658046670258045, 0.01131709385663271, 0.08450997620820999, 0.012135408818721771, 0.02204836532473564, 0.10017257183790207, 0.015103278681635857, 0.020152268931269646, 0.1190679520368576, 0.14148981869220734, 0.006576645188033581, 0.007074060849845409, 0.1485065221786499, 0.15361104905605316, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08679644763469696, 0.01581868715584278, 0.009913141839206219, 0.005941127426922321, 0.011749101802706718, 0.04402248561382294, 0.049411576241254807, 0.047211602330207825, 0.022448528558015823, 0.028144190087914467, 0.05249365419149399, 0.013664882630109787, 0.03804450482130051, 0.05856489762663841, 0.016008714213967323, 0.028531767427921295, 0.06752391904592514, 0.10004057735204697, 0.0260744821280241, 0.01816527731716633, 0.08061446994543076, 0.14588400721549988, 0.03293199837207794, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12203317135572433, 0.02679692953824997, 0.034712761640548706, 0.016600849106907845, 0.05098602548241615, 0.033623430877923965, 0.03376292064785957, 0.03289441391825676, 0.022763613611459732, 0.03469546139240265, 0.03710818663239479, 0.02750304713845253, 0.030508041381835938, 0.040704939514398575, 0.019409801810979843, 0.033388108015060425, 0.046037882566452026, 0.08141947537660599, 0.02579292468726635, 0.017713336274027824, 0.05326380580663681, 0.06955217570066452, 0.07340095937252045, 0.03532778471708298, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.012632717378437519, 0.0031967321410775185, 0.008999628014862537, 0.003787106368690729, 0.005590217188000679, 0.06018739938735962, 0.010173037648200989, 0.02606341242790222, 0.00828034058213234, 0.005998442880809307, 0.08480239659547806, 0.007755735889077187, 0.014807655476033688, 0.1007935106754303, 0.011636019684374332, 0.007885105907917023, 0.1206134483218193, 0.1281987875699997, 0.006487120874226093, 0.0032305698841810226, 0.15047509968280792, 0.1055770069360733, 0.015480626374483109, 0.019026348367333412, 0.07832160592079163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11176060885190964, 0.018635598942637444, 0.05622371658682823, 0.04677281901240349, 0.0373648926615715, 0.02299340069293976, 0.031365297734737396, 0.02640891633927822, 0.043359220027923584, 0.05663560703396797, 0.021786384284496307, 0.023822180926799774, 0.03064165636897087, 0.023289235308766365, 0.05430489778518677, 0.06881183385848999, 0.024489358067512512, 0.04777087643742561, 0.036271777004003525, 0.022222520783543587, 0.026998933404684067, 0.021766114979982376, 0.034424878656864166, 0.028240244835615158, 0.05891920626163483, 0.024719856679439545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10639810562133789, 0.03710797801613808, 0.08273958414793015, 0.017856869846582413, 0.020950008183717728, 0.01830524019896984, 0.022620096802711487, 0.01819228008389473, 0.021851202473044395, 0.012518540024757385, 0.019214089959859848, 0.030340079218149185, 0.07187622040510178, 0.020788028836250305, 0.039635222405195236, 0.06151586398482323, 0.0231793075799942, 0.04398040100932121, 0.0461922362446785, 0.02066795527935028, 0.027080902829766273, 0.04386553168296814, 0.032767388969659805, 0.04369062930345535, 0.05080842226743698, 0.040294937789440155, 0.02556297555565834, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.015084551647305489, 0.0038223871961236, 0.005877267569303513, 0.0049718511290848255, 0.00537014938890934, 0.05223636329174042, 0.016127698123455048, 0.031829919666051865, 0.008211134932935238, 0.011473823338747025, 0.06704677641391754, 0.010528380051255226, 0.01059822179377079, 0.07938897609710693, 0.007211717311292887, 0.006271470803767443, 0.09142352640628815, 0.10866515338420868, 0.00397381791844964, 0.005621672607958317, 0.1098332330584526, 0.0923776850104332, 0.02296050451695919, 0.019314922392368317, 0.08864132314920425, 0.018464721739292145, 0.011022252030670643, 0.09165035933256149, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09854131191968918, 0.039670638740062714, 0.050521381199359894, 0.04794393479824066, 0.019531885161995888, 0.015683403238654137, 0.025074761360883713, 0.014997918158769608, 0.02772345207631588, 0.016418185085058212, 0.016389019787311554, 0.02631654404103756, 0.03435925394296646, 0.017757998779416084, 0.023362642154097557, 0.037635158747434616, 0.01940559223294258, 0.03442169725894928, 0.04902298003435135, 0.030190538614988327, 0.02215910516679287, 0.03754067420959473, 0.038793157786130905, 0.040324095636606216, 0.04231260344386101, 0.06829199194908142, 0.025314101949334145, 0.043300725519657135, 0.036995258182287216, 0.0, 0.0, 0.0, 0.0], [0.10688826441764832, 0.036595750600099564, 0.005083083175122738, 0.01016042847186327, 0.013297340832650661, 0.041752055287361145, 0.02624681405723095, 0.03012859635055065, 0.013482414186000824, 0.01822318695485592, 0.045826252549886703, 0.02555878832936287, 0.015618805773556232, 0.05039893090724945, 0.011656995862722397, 0.011659113690257072, 0.05515516921877861, 0.05091667175292969, 0.01920679770410061, 0.013998396694660187, 0.06383474171161652, 0.029088912531733513, 0.016710439696907997, 0.026095252484083176, 0.05484987422823906, 0.03806675970554352, 0.03896065428853035, 0.05375230684876442, 0.02367916889488697, 0.05310805141925812, 0.0, 0.0, 0.0], [0.10871610045433044, 0.06286420673131943, 0.023045113310217857, 0.012493227608501911, 0.009688262827694416, 0.02372078038752079, 0.02561734803020954, 0.028282906860113144, 0.019523121416568756, 0.01518714427947998, 0.02461574226617813, 0.018855787813663483, 0.013071926310658455, 0.025697434321045876, 0.010639774613082409, 0.007336433511227369, 0.028488237410783768, 0.04087512195110321, 0.02369394153356552, 0.028460795059800148, 0.031781140714883804, 0.03399214893579483, 0.01690450683236122, 0.04699752479791641, 0.054809220135211945, 0.06048164144158363, 0.021137841045856476, 0.05147766321897507, 0.01530436147004366, 0.0786886066198349, 0.03755192458629608, 0.0, 0.0], [0.16510622203350067, 0.013307332992553711, 0.02282034233212471, 0.03222499042749405, 0.02832779474556446, 0.011912100948393345, 0.02175912633538246, 0.012591327540576458, 0.025371326133608818, 0.01592349261045456, 0.011745045892894268, 0.021340113133192062, 0.022658511996269226, 0.012097012251615524, 0.0587739497423172, 0.04114295169711113, 0.0128661273047328, 0.021896688267588615, 0.02738575078547001, 0.01541272085160017, 0.014822257682681084, 0.01319876965135336, 0.02906014956533909, 0.04031302034854889, 0.02303692139685154, 0.05035790055990219, 0.046500664204359055, 0.03221074864268303, 0.04574138671159744, 0.01144817378371954, 0.05148481950163841, 0.04716219753026962, 0.0], [0.01986270397901535, 0.000694137648679316, 0.0017670622328296304, 0.0019038207828998566, 0.002311576856300235, 0.059243783354759216, 0.00516494968906045, 0.008714549243450165, 0.002490671817213297, 0.00305796111933887, 0.07585150003433228, 0.0023905490525066853, 0.0031773424707353115, 0.08945117890834808, 0.003065291326493025, 0.001620917464606464, 0.10312002897262573, 0.06259164959192276, 0.0020582994911819696, 0.0022322151344269514, 0.12742038071155548, 0.02470388635993004, 0.003510193433612585, 0.0049941749311983585, 0.02831711620092392, 0.005874200724065304, 0.0062153153121471405, 0.03686564788222313, 0.00543809961527586, 0.004061869811266661, 0.0047355154529213905, 0.004534380044788122, 0.2925589680671692]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9726406335830688, 0.027359358966350555, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8850566744804382, 0.0816064327955246, 0.03333686664700508, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7844979166984558, 0.09045997262001038, 0.107209712266922, 0.01783236674964428, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6679064631462097, 0.10135099291801453, 0.1099792942404747, 0.06990879774093628, 0.05085453391075134, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5862791538238525, 0.06960925459861755, 0.06798028945922852, 0.060589682310819626, 0.08339763432741165, 0.1321440488100052, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5015710592269897, 0.062278714030981064, 0.07209992408752441, 0.0559091791510582, 0.08146876096725464, 0.12033160775899887, 0.10634069889783859, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.40016093850135803, 0.05525398254394531, 0.06347686797380447, 0.05413789674639702, 0.07119616866111755, 0.11230529099702835, 0.12489724159240723, 0.11857165396213531, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4018532335758209, 0.08579413592815399, 0.05644885450601578, 0.07181129604578018, 0.08318914473056793, 0.07845646888017654, 0.09191881865262985, 0.07768874615430832, 0.052839312702417374, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38271597027778625, 0.07976947724819183, 0.06207752600312233, 0.06360236555337906, 0.08211027830839157, 0.07804950326681137, 0.08597432076931, 0.07506486028432846, 0.06327847391366959, 0.027357226237654686, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3404168486595154, 0.04535195976495743, 0.04658915847539902, 0.040391724556684494, 0.05489042028784752, 0.08179005235433578, 0.08906515687704086, 0.09645888954401016, 0.05564972758293152, 0.06398677825927734, 0.08540932089090347, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3118618130683899, 0.09539995342493057, 0.07292010635137558, 0.0697687640786171, 0.07777836173772812, 0.06735039502382278, 0.06161404773592949, 0.054810769855976105, 0.05252016708254814, 0.05022519826889038, 0.07299347221851349, 0.012756966054439545, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.36652839183807373, 0.06308672577142715, 0.039478451013565063, 0.04609524458646774, 0.05740760266780853, 0.06348337978124619, 0.052384622395038605, 0.055016182363033295, 0.04584020748734474, 0.062286946922540665, 0.0658990889787674, 0.058063607662916183, 0.02442951127886772, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.28564441204071045, 0.03804025799036026, 0.03942391276359558, 0.03423864766955376, 0.046393983066082, 0.06771665811538696, 0.07328074425458908, 0.07855428010225296, 0.04789666086435318, 0.0559365451335907, 0.07124943286180496, 0.03568926081061363, 0.04956045001745224, 0.07637479901313782, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2977542281150818, 0.05259547382593155, 0.043479401618242264, 0.05845864117145538, 0.04568732902407646, 0.05259403958916664, 0.044561967253685, 0.04597471281886101, 0.06316148489713669, 0.07768764346837997, 0.05499139055609703, 0.03466469794511795, 0.05020103231072426, 0.0588613860309124, 0.019326552748680115, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2885313034057617, 0.04616912081837654, 0.028233056887984276, 0.03824241831898689, 0.04330628365278244, 0.05174916982650757, 0.04318271577358246, 0.04393117129802704, 0.05616343021392822, 0.06773824244737625, 0.05490175262093544, 0.08139308542013168, 0.04325691983103752, 0.0590481236577034, 0.03989905118942261, 0.014254092238843441, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2372136265039444, 0.031615808606147766, 0.033245667815208435, 0.029231952503323555, 0.03861812502145767, 0.05549167841672897, 0.059664975851774216, 0.06321781128644943, 0.04073701798915863, 0.0475267730653286, 0.058378200978040695, 0.030399683862924576, 0.042871590703725815, 0.06284616887569427, 0.053212907165288925, 0.04736897721886635, 0.06835900247097015, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2098696231842041, 0.02729126252233982, 0.0311566349118948, 0.02661503292620182, 0.031651128083467484, 0.05273229628801346, 0.052745040506124496, 0.05635900795459747, 0.04076547548174858, 0.042782388627529144, 0.05619123950600624, 0.030678285285830498, 0.04398961365222931, 0.060914721339941025, 0.050136979669332504, 0.04157319292426109, 0.06652858853340149, 0.07801948487758636, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.23853811621665955, 0.04414582625031471, 0.04643508046865463, 0.0345127247273922, 0.03572184219956398, 0.04369330033659935, 0.04304974526166916, 0.03498827666044235, 0.04371808096766472, 0.039521973580121994, 0.04457850754261017, 0.02459946647286415, 0.06251184642314911, 0.04753483831882477, 0.04375610128045082, 0.04995935037732124, 0.05090805143117905, 0.052662428468465805, 0.01916448399424553, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1772913932800293, 0.042214635759592056, 0.03081870824098587, 0.030886102467775345, 0.033868514001369476, 0.04521188139915466, 0.04290100559592247, 0.032215576618909836, 0.04018864780664444, 0.049269068986177444, 0.04801621288061142, 0.03814246132969856, 0.051265131682157516, 0.05206840857863426, 0.04744121804833412, 0.03924538195133209, 0.05630479380488396, 0.05462939292192459, 0.059918683022260666, 0.028102930635213852, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1904827505350113, 0.025203483179211617, 0.02678748033940792, 0.02367269992828369, 0.031104041263461113, 0.04345345124602318, 0.04649331793189049, 0.048206452280282974, 0.03323281556367874, 0.03892754018306732, 0.045491475611925125, 0.024442745372653008, 0.03570159524679184, 0.049077678471803665, 0.04303567111492157, 0.03905567526817322, 0.0537174716591835, 0.06658953428268433, 0.043208274990320206, 0.0317646823823452, 0.06035125255584717, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14975357055664062, 0.02347457781434059, 0.025508420541882515, 0.028942452743649483, 0.03517160564661026, 0.038801923394203186, 0.04594280943274498, 0.03962727636098862, 0.03708780184388161, 0.03872301056981087, 0.04190719127655029, 0.0216880664229393, 0.043282557278871536, 0.04592543840408325, 0.03893955424427986, 0.04007613658905029, 0.050600215792655945, 0.06083047017455101, 0.04372164234519005, 0.030235206708312035, 0.05728613957762718, 0.06247400864958763, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17513403296470642, 0.027519263327121735, 0.029514042660593987, 0.020715339109301567, 0.02687353454530239, 0.03454614803195, 0.03853079304099083, 0.03303779661655426, 0.04181559756398201, 0.0401003472507, 0.03696896508336067, 0.030468929558992386, 0.03995396941900253, 0.040237054228782654, 0.0448261983692646, 0.03708265349268913, 0.044201456010341644, 0.04931036755442619, 0.04125744476914406, 0.028610650449991226, 0.04997343569993973, 0.052692800760269165, 0.036629192531108856, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13285982608795166, 0.0407080352306366, 0.04111678898334503, 0.03484426811337471, 0.04087608680129051, 0.02756722830235958, 0.02829367108643055, 0.02252042479813099, 0.038754191249608994, 0.04037807509303093, 0.02908940799534321, 0.029609233140945435, 0.05670114979147911, 0.03118187189102173, 0.037661176174879074, 0.056886401027441025, 0.03396710008382797, 0.03781304135918617, 0.04951416701078415, 0.031945567578077316, 0.03759901970624924, 0.04326634481549263, 0.04305670037865639, 0.03379025682806969, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14709354937076569, 0.018218692392110825, 0.022143632173538208, 0.021621398627758026, 0.027072906494140625, 0.03385418280959129, 0.03787611797451973, 0.03801962733268738, 0.02793252095580101, 0.03090185485780239, 0.03606921434402466, 0.019023140892386436, 0.0346580371260643, 0.039169859141111374, 0.0337555967271328, 0.026136627420783043, 0.042978327721357346, 0.053137391805648804, 0.034635089337825775, 0.02014027163386345, 0.048783592879772186, 0.049908000975847244, 0.04624392092227936, 0.04715674743056297, 0.06346967071294785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17602775990962982, 0.014882152900099754, 0.026751147583127022, 0.024088069796562195, 0.035667527467012405, 0.0270942784845829, 0.024984238669276237, 0.021471869200468063, 0.02635936439037323, 0.023845119401812553, 0.027566973119974136, 0.01686953566968441, 0.07777427136898041, 0.029109202325344086, 0.04799698665738106, 0.02900252677500248, 0.03132336214184761, 0.036155957728624344, 0.03959563747048378, 0.017007999122142792, 0.03483908623456955, 0.034437146037817, 0.04149220138788223, 0.09124384075403214, 0.04052302613854408, 0.0038906028494238853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.18501751124858856, 0.027787020429968834, 0.02509761042892933, 0.02701900713145733, 0.029199199751019478, 0.030337035655975342, 0.032221753150224686, 0.023015059530735016, 0.0267928633838892, 0.022793851792812347, 0.030674850568175316, 0.02122086100280285, 0.05388803407549858, 0.032593000680208206, 0.03073764406144619, 0.025725232437253, 0.03486606478691101, 0.03793580085039139, 0.02867204323410988, 0.026148945093154907, 0.03875334560871124, 0.04179498925805092, 0.03863157704472542, 0.053371548652648926, 0.03982073441147804, 0.025711633265018463, 0.010172729380428791, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.12446313351392746, 0.016768889501690865, 0.020105110481381416, 0.01926703006029129, 0.024457816034555435, 0.030203374102711678, 0.03127392381429672, 0.03254605084657669, 0.02318602241575718, 0.023508412763476372, 0.031733203679323196, 0.01708051562309265, 0.02908206358551979, 0.03428216278553009, 0.0357913076877594, 0.025820566341280937, 0.03753117099404335, 0.046182457357645035, 0.029500020667910576, 0.017913782969117165, 0.04274732619524002, 0.0424346923828125, 0.039002422243356705, 0.041418399661779404, 0.05949509143829346, 0.03595736622810364, 0.030984854325652122, 0.05726287513971329, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14491011202335358, 0.025843989104032516, 0.0311769749969244, 0.019462723284959793, 0.012575745582580566, 0.025274598971009254, 0.022020692005753517, 0.021267281845211983, 0.03165541589260101, 0.0276641808450222, 0.026251591742038727, 0.022091029211878777, 0.038517944514751434, 0.028266869485378265, 0.028877612203359604, 0.03686540201306343, 0.030699972063302994, 0.03204502910375595, 0.034116316586732864, 0.020355401560664177, 0.03446316719055176, 0.03542863205075264, 0.030728429555892944, 0.04441900551319122, 0.037935998290777206, 0.057348672300577164, 0.03889702260494232, 0.041112035512924194, 0.019728178158402443, 0.0, 0.0, 0.0, 0.0], [0.13168421387672424, 0.025460129603743553, 0.02796238288283348, 0.024109823629260063, 0.01686883717775345, 0.03140677511692047, 0.02324732206761837, 0.02207169309258461, 0.027362516149878502, 0.026370491832494736, 0.03204534202814102, 0.028775854036211967, 0.04614201560616493, 0.03455975279211998, 0.03132110834121704, 0.029437759891152382, 0.03749070316553116, 0.03597423806786537, 0.03067103959619999, 0.026778316125273705, 0.041662368923425674, 0.032743968069553375, 0.030348775908350945, 0.03265199810266495, 0.038056518882513046, 0.034575652331113815, 0.022936107590794563, 0.0369032546877861, 0.02596617490053177, 0.014414878562092781, 0.0, 0.0, 0.0], [0.16563694179058075, 0.022809254005551338, 0.029107755050063133, 0.024944668635725975, 0.02435288019478321, 0.025031834840774536, 0.01848665252327919, 0.018573664128780365, 0.028443658724427223, 0.02919994480907917, 0.025318214669823647, 0.03220498561859131, 0.025753291323781013, 0.026890527456998825, 0.031273938715457916, 0.03305153548717499, 0.028835464268922806, 0.029910407960414886, 0.02755882777273655, 0.02462463453412056, 0.032053638249635696, 0.026493733748793602, 0.021709594875574112, 0.031640179455280304, 0.03707480430603027, 0.0479946993291378, 0.02793213538825512, 0.034756094217300415, 0.03804941102862358, 0.021535655483603477, 0.008750848472118378, 0.0, 0.0], [0.1679420918226242, 0.025983456522226334, 0.032223351299762726, 0.019469354301691055, 0.023735286667943, 0.023115739226341248, 0.01997361332178116, 0.016550220549106598, 0.029796769842505455, 0.0284946970641613, 0.02310883067548275, 0.022087078541517258, 0.04216555133461952, 0.024390174075961113, 0.020749147981405258, 0.03873591870069504, 0.026269828900694847, 0.027289122343063354, 0.019617419689893723, 0.020242849364876747, 0.029203226789832115, 0.030413340777158737, 0.025762639939785004, 0.04084987938404083, 0.03332669660449028, 0.021795323118567467, 0.026310527697205544, 0.031013470143079758, 0.0368925966322422, 0.019377145916223526, 0.04355607554316521, 0.009558649733662605, 0.0], [0.11396824568510056, 0.017752891406416893, 0.016061974689364433, 0.015585057437419891, 0.01839612051844597, 0.024367082864046097, 0.02605387009680271, 0.023999106138944626, 0.02191949635744095, 0.026251133531332016, 0.024730540812015533, 0.015323019586503506, 0.025614576414227486, 0.026337891817092896, 0.024982875213027, 0.02464265376329422, 0.028590315952897072, 0.0329514816403389, 0.027568640187382698, 0.018487535417079926, 0.03220851346850395, 0.03191468492150307, 0.03630724549293518, 0.034154053777456284, 0.036785610020160675, 0.03297772258520126, 0.026278913021087646, 0.038098886609077454, 0.029801934957504272, 0.01818324252963066, 0.036320701241493225, 0.03568092733621597, 0.05770309641957283]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8058414459228516, 0.19415852427482605, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6373865008354187, 0.09672046452760696, 0.26589298248291016, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5870267748832703, 0.09922715276479721, 0.07913827151060104, 0.2346077859401703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41947415471076965, 0.06312594562768936, 0.08123069256544113, 0.08381501585245132, 0.3523542881011963, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5308530330657959, 0.09590736031532288, 0.07779337465763092, 0.0666155219078064, 0.07293297350406647, 0.15589778125286102, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.38503068685531616, 0.0747809186577797, 0.06978143006563187, 0.047606486827135086, 0.05144915729761124, 0.08776706457138062, 0.28358423709869385, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.29629573225975037, 0.05328267440199852, 0.06944745033979416, 0.04097549617290497, 0.06660830974578857, 0.10218574106693268, 0.10869498550891876, 0.26250961422920227, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3047500252723694, 0.06337987631559372, 0.057534217834472656, 0.04057717323303223, 0.07300141453742981, 0.0737583190202713, 0.07239887118339539, 0.05693604797124863, 0.2576639950275421, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2988077998161316, 0.05552828684449196, 0.05443595349788666, 0.05285298451781273, 0.06568747013807297, 0.07554221153259277, 0.06968098878860474, 0.05411387234926224, 0.09120785444974899, 0.1821424812078476, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.26933014392852783, 0.06352431327104568, 0.05952121317386627, 0.052424706518650055, 0.06052161380648613, 0.1218583881855011, 0.07608656585216522, 0.09753073006868362, 0.04232609272003174, 0.050583258271217346, 0.10629308223724365, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.2207430899143219, 0.06960076093673706, 0.04872344061732292, 0.06675713509321213, 0.06954207271337509, 0.06483783572912216, 0.046523746103048325, 0.03930084407329559, 0.018016092479228973, 0.03580629453063011, 0.05302286893129349, 0.26712584495544434, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1508007049560547, 0.04732475057244301, 0.12551110982894897, 0.06240079924464226, 0.05354628711938858, 0.055783938616514206, 0.04542610049247742, 0.03888601437211037, 0.030199842527508736, 0.033370573073625565, 0.046459510922431946, 0.023437926545739174, 0.2868524491786957, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.20448851585388184, 0.049720507115125656, 0.04838543012738228, 0.043853580951690674, 0.051157206296920776, 0.1007489562034607, 0.0642068162560463, 0.08302560448646545, 0.038804519921541214, 0.04795770347118378, 0.0958237498998642, 0.03239700570702553, 0.044411033391952515, 0.09501936286687851, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16236738860607147, 0.028519190847873688, 0.0478430911898613, 0.04135481268167496, 0.04028485715389252, 0.0520053505897522, 0.04226508364081383, 0.03821169212460518, 0.059868860989809036, 0.06849399954080582, 0.04734276235103607, 0.019715184345841408, 0.06202230602502823, 0.04572194814682007, 0.2439834624528885, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13128934800624847, 0.039628107100725174, 0.059519387781620026, 0.03966321423649788, 0.044063542038202286, 0.043484944850206375, 0.03726546838879585, 0.032400380820035934, 0.03720575571060181, 0.06481604278087616, 0.04047452285885811, 0.025914257392287254, 0.06507598608732224, 0.03922509402036667, 0.0774419903755188, 0.2225320041179657, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15813346207141876, 0.03911252319812775, 0.038564953953027725, 0.03587968647480011, 0.04187795892357826, 0.08161003887653351, 0.05245806649327278, 0.0681268498301506, 0.0344480536878109, 0.043427493423223495, 0.08275991678237915, 0.02933337725698948, 0.04157378524541855, 0.08478686958551407, 0.04881599172949791, 0.03235285356640816, 0.08673811703920364, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13890600204467773, 0.03169631585478783, 0.03251553326845169, 0.03538898378610611, 0.03800034523010254, 0.06974426656961441, 0.05942453816533089, 0.06768237799406052, 0.027843821793794632, 0.03905464708805084, 0.07273099571466446, 0.030109882354736328, 0.04163092374801636, 0.07565013319253922, 0.040188319981098175, 0.0274836253374815, 0.07795906066894531, 0.09399019926786423, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09691288322210312, 0.030687324702739716, 0.03081796132028103, 0.044734325259923935, 0.04171941056847572, 0.03603678569197655, 0.04101705178618431, 0.032104793936014175, 0.04344063624739647, 0.04885553941130638, 0.03457900509238243, 0.016025103628635406, 0.049941666424274445, 0.03463530167937279, 0.06065686047077179, 0.034977883100509644, 0.034543413668870926, 0.03782797232270241, 0.2504861354827881, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1205921471118927, 0.04121064022183418, 0.05098516494035721, 0.05289153382182121, 0.02886129356920719, 0.04706199839711189, 0.0292128324508667, 0.03817001357674599, 0.021046465262770653, 0.029787786304950714, 0.044067222625017166, 0.026503346860408783, 0.04127002879977226, 0.04430736228823662, 0.0308015588670969, 0.04064900428056717, 0.044745102524757385, 0.04212244600057602, 0.028442280367016792, 0.1972716748714447, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1138831079006195, 0.028255395591259003, 0.028351284563541412, 0.026958657428622246, 0.031310874968767166, 0.05977550894021988, 0.0391821525990963, 0.050720397382974625, 0.02768401801586151, 0.03556419163942337, 0.06452129781246185, 0.024267064407467842, 0.03616492822766304, 0.06838168948888779, 0.04139300063252449, 0.02909461408853531, 0.07230047881603241, 0.07349378615617752, 0.03700018301606178, 0.0352863110601902, 0.07641097158193588, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.084538534283638, 0.017416611313819885, 0.018949121236801147, 0.01656063087284565, 0.022296130657196045, 0.04174061119556427, 0.05894864723086357, 0.038933612406253815, 0.03021463006734848, 0.0346423014998436, 0.04831898212432861, 0.016358423978090286, 0.02567453309893608, 0.053096748888492584, 0.020546510815620422, 0.020469823852181435, 0.05704641342163086, 0.06604911386966705, 0.03774643689393997, 0.021413855254650116, 0.06129932776093483, 0.2077389359474182, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11248438060283661, 0.024394843727350235, 0.024523140862584114, 0.02748953551054001, 0.026104671880602837, 0.037315063178539276, 0.06748531013727188, 0.03890376165509224, 0.01798143796622753, 0.028454391285777092, 0.03756530210375786, 0.018326058983802795, 0.02008865587413311, 0.038651708513498306, 0.02328565903007984, 0.02422661893069744, 0.03960610553622246, 0.04833190515637398, 0.045000940561294556, 0.017362147569656372, 0.040091414004564285, 0.05208289623260498, 0.19024406373500824, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0818336009979248, 0.019828448072075844, 0.026873396709561348, 0.029252540320158005, 0.025184454396367073, 0.031927719712257385, 0.02904980443418026, 0.02497251145541668, 0.03350626304745674, 0.04007427394390106, 0.03351730480790138, 0.016036000102758408, 0.032044537365436554, 0.03448063135147095, 0.02851708233356476, 0.035327304154634476, 0.03583225980401039, 0.04058394953608513, 0.02994108758866787, 0.021097904071211815, 0.03671329841017723, 0.03921261057257652, 0.03543240204453468, 0.2387605905532837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10145696252584457, 0.01423705369234085, 0.01698417030274868, 0.018636565655469894, 0.028536254540085793, 0.0365380235016346, 0.02957051806151867, 0.04198320209980011, 0.014166047796607018, 0.030081400647759438, 0.04016638919711113, 0.020075030624866486, 0.03037012368440628, 0.04353443160653114, 0.03039301373064518, 0.020717348903417587, 0.0467711016535759, 0.05495096743106842, 0.028809167444705963, 0.020043710246682167, 0.05092546343803406, 0.040844712406396866, 0.0418580025434494, 0.03848978132009506, 0.1598605513572693, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07937920838594437, 0.020674291998147964, 0.02079973742365837, 0.027988197281956673, 0.04565474018454552, 0.026562578976154327, 0.02539670653641224, 0.021243246272206306, 0.017763806506991386, 0.025200629606842995, 0.027576463297009468, 0.03060709498822689, 0.034380000084638596, 0.028568318113684654, 0.02272694930434227, 0.016051942482590675, 0.029712725430727005, 0.032761767506599426, 0.025702398270368576, 0.01287889014929533, 0.03062613308429718, 0.020369654521346092, 0.03741390258073807, 0.027850396931171417, 0.03571847826242447, 0.2763917148113251, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09605436027050018, 0.023092877119779587, 0.02419964410364628, 0.024974118918180466, 0.036829639226198196, 0.030014384537935257, 0.02106921747326851, 0.018117530271410942, 0.02256927825510502, 0.028343217447400093, 0.030505109578371048, 0.021707670763134956, 0.02821178175508976, 0.03163940832018852, 0.03916611522436142, 0.03627365827560425, 0.033115699887275696, 0.03514394164085388, 0.025745239108800888, 0.015889015048742294, 0.034675464034080505, 0.02677009627223015, 0.027143441140651703, 0.0404508076608181, 0.037603843957185745, 0.03232336416840553, 0.17837099730968475, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.07476937770843506, 0.011021512560546398, 0.010706561617553234, 0.015325453132390976, 0.019903559237718582, 0.032858651131391525, 0.021936070173978806, 0.02945413626730442, 0.01642150618135929, 0.0224765595048666, 0.037223417311906815, 0.017580224201083183, 0.021954135969281197, 0.04046214744448662, 0.0314154215157032, 0.010417775250971317, 0.04429738596081734, 0.05118124559521675, 0.029565539211034775, 0.015394340269267559, 0.049064282327890396, 0.032021958380937576, 0.03090512566268444, 0.04132050648331642, 0.06578270345926285, 0.03978405147790909, 0.0204997006803751, 0.16625669598579407, 0.0, 0.0, 0.0, 0.0, 0.0], [0.051242098212242126, 0.011390025727450848, 0.02026580274105072, 0.024529628455638885, 0.12732434272766113, 0.020083917304873466, 0.017692748457193375, 0.020381992682814598, 0.018548622727394104, 0.023206058889627457, 0.02102605253458023, 0.020521240308880806, 0.025210728868842125, 0.021994248032569885, 0.0171904806047678, 0.020805923268198967, 0.02303529717028141, 0.026458021253347397, 0.031086120754480362, 0.011454474180936813, 0.02435799315571785, 0.022524958476424217, 0.024600783362984657, 0.024588018655776978, 0.028380820527672768, 0.04660610854625702, 0.04362708330154419, 0.020300481468439102, 0.21156594157218933, 0.0, 0.0, 0.0, 0.0], [0.07929293811321259, 0.023537369444966316, 0.022847270593047142, 0.019936036318540573, 0.015462885610759258, 0.025110280141234398, 0.021429041400551796, 0.01507349032908678, 0.009924105368554592, 0.015724757686257362, 0.027188697829842567, 0.020747270435094833, 0.034976329654455185, 0.029102325439453125, 0.021219031885266304, 0.018147600814700127, 0.031058194115757942, 0.03380945324897766, 0.02013755403459072, 0.01232312060892582, 0.0336926095187664, 0.02433612197637558, 0.03108450025320053, 0.03377699479460716, 0.04355441778898239, 0.03296351805329323, 0.03314927965402603, 0.025364909321069717, 0.022163676097989082, 0.22286610305309296, 0.0, 0.0, 0.0], [0.1005966067314148, 0.030963750556111336, 0.025578495115041733, 0.020224059000611305, 0.02713301032781601, 0.0294907558709383, 0.0407448336482048, 0.02140362374484539, 0.016392607241868973, 0.02252069115638733, 0.027555175125598907, 0.013599212281405926, 0.027306705713272095, 0.02780657447874546, 0.011214189231395721, 0.023586517199873924, 0.02839917317032814, 0.030668897554278374, 0.021664191037416458, 0.017032304778695107, 0.029051588848233223, 0.0265499297529459, 0.03297552093863487, 0.026790330186486244, 0.032060641795396805, 0.02729831077158451, 0.017307184636592865, 0.018768809735774994, 0.028226513415575027, 0.017085112631320953, 0.18000461161136627, 0.0, 0.0], [0.08361611515283585, 0.016581889241933823, 0.01582329161465168, 0.022005144506692886, 0.02406255342066288, 0.0210283063352108, 0.022811299189925194, 0.014412390999495983, 0.01557208877056837, 0.015104822814464569, 0.020917728543281555, 0.021798165515065193, 0.025985993444919586, 0.022013215348124504, 0.022025063633918762, 0.018091198056936264, 0.023115094751119614, 0.0254517812281847, 0.025943497195839882, 0.011241885833442211, 0.024466179311275482, 0.022869905456900597, 0.03161345422267914, 0.029542680829763412, 0.023817913606762886, 0.029349008575081825, 0.025922635570168495, 0.017268182709813118, 0.0318446047604084, 0.012037740088999271, 0.027524931356310844, 0.2561413049697876, 0.0], [0.05258442834019661, 0.012508378364145756, 0.012737673707306385, 0.012144193984568119, 0.014467822387814522, 0.024812450632452965, 0.01710914820432663, 0.025162573903799057, 0.016298964619636536, 0.015160076320171356, 0.02982003428041935, 0.014290268532931805, 0.024094784632325172, 0.03374277800321579, 0.021632255986332893, 0.01892409287393093, 0.03819218650460243, 0.04115181043744087, 0.024165770038962364, 0.02063564769923687, 0.044185347855091095, 0.03793707489967346, 0.029636330902576447, 0.03152839094400406, 0.04542306438088417, 0.035963620990514755, 0.030482523143291473, 0.04609915241599083, 0.03206764534115791, 0.01766294427216053, 0.03169601410627365, 0.04462910071015358, 0.10305342078208923]], [[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7707712054252625, 0.22922877967357635, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7813621759414673, 0.10978542268276215, 0.10885243117809296, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.535038948059082, 0.11194943636655807, 0.15013697743415833, 0.20287461578845978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.41803571581840515, 0.1603638082742691, 0.10740312188863754, 0.10625781863927841, 0.20793956518173218, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.4349866509437561, 0.09579858183860779, 0.08251803368330002, 0.10028504580259323, 0.15332913398742676, 0.13308250904083252, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.44621360301971436, 0.056242164224386215, 0.05040305107831955, 0.0708429142832756, 0.09360908716917038, 0.11472616344690323, 0.16796302795410156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3609195649623871, 0.07294464856386185, 0.051910098642110825, 0.07159046828746796, 0.09321669489145279, 0.09543658792972565, 0.14539918303489685, 0.10858282446861267, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.3171406388282776, 0.07683127373456955, 0.05552287399768829, 0.06445363909006119, 0.08586596697568893, 0.10643250495195389, 0.1452113538980484, 0.09439338743686676, 0.054148364812135696, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.1928861290216446, 0.1134386956691742, 0.045305099338293076, 0.08361194282770157, 0.05763271078467369, 0.1191418468952179, 0.11441226303577423, 0.12364520132541656, 0.07640956342220306, 0.07351645827293396, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.25174084305763245, 0.06019410863518715, 0.04930214211344719, 0.0592232421040535, 0.08453337103128433, 0.07307679206132889, 0.12308555841445923, 0.08293970674276352, 0.06380277872085571, 0.07185834646224976, 0.0802430808544159, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.24793611466884613, 0.04855775833129883, 0.061716943979263306, 0.06616780906915665, 0.08788595348596573, 0.05920381098985672, 0.09128594398498535, 0.0686681792140007, 0.06062861904501915, 0.08399353176355362, 0.06569414585828781, 0.058261141180992126, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15039603412151337, 0.07072298973798752, 0.036188047379255295, 0.03812883794307709, 0.054965753108263016, 0.09552790969610214, 0.12780208885669708, 0.09101755172014236, 0.0413394495844841, 0.06619426608085632, 0.11493085324764252, 0.0352681465446949, 0.07751799374818802, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21730750799179077, 0.052248742431402206, 0.04279690235853195, 0.051309894770383835, 0.07021073997020721, 0.060848623514175415, 0.10094165802001953, 0.06729140877723694, 0.05347270146012306, 0.05803238973021507, 0.06386597454547882, 0.04143683239817619, 0.054298095405101776, 0.06593842804431915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.14158989489078522, 0.08735613524913788, 0.041096318513154984, 0.07771319150924683, 0.06641595810651779, 0.06084609031677246, 0.07890869677066803, 0.0646718367934227, 0.04156750813126564, 0.053140249103307724, 0.06033195182681084, 0.04990341141819954, 0.06147688254714012, 0.06152058020234108, 0.05346124619245529, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11277394741773605, 0.05140272155404091, 0.03480076044797897, 0.04410993680357933, 0.04927106574177742, 0.08066017925739288, 0.09371127188205719, 0.07634279876947403, 0.03460072726011276, 0.05556504428386688, 0.09000370651483536, 0.027950894087553024, 0.05808687582612038, 0.09759882837533951, 0.05853048339486122, 0.03459077328443527, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19136787950992584, 0.04588804394006729, 0.037184230983257294, 0.04478849098086357, 0.05845542624592781, 0.05162310600280762, 0.08451798558235168, 0.056617312133312225, 0.04477926343679428, 0.04681585729122162, 0.051775190979242325, 0.03474307060241699, 0.04365592822432518, 0.052577532827854156, 0.045719072222709656, 0.05523940548300743, 0.054252225905656815, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.17809873819351196, 0.03651316463947296, 0.035378050059080124, 0.046600621193647385, 0.05606750398874283, 0.055425312370061874, 0.06580899655818939, 0.05869593098759651, 0.044602055102586746, 0.03845333307981491, 0.055220670998096466, 0.028129257261753082, 0.038876280188560486, 0.05635206773877144, 0.04307199642062187, 0.04859212785959244, 0.05801298841834068, 0.05610082671046257, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11277128010988235, 0.04416340962052345, 0.016790850088000298, 0.039228979498147964, 0.03057023137807846, 0.08429382741451263, 0.05829831585288048, 0.07985582202672958, 0.027740206569433212, 0.024607911705970764, 0.08568023890256882, 0.012135518714785576, 0.021223604679107666, 0.08979956805706024, 0.024395598098635674, 0.01800009422004223, 0.09797865897417068, 0.0856485664844513, 0.046817269176244736, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15648676455020905, 0.0435933880507946, 0.029469136148691177, 0.023382794111967087, 0.03989393264055252, 0.06601443886756897, 0.07239896804094315, 0.04959876835346222, 0.025720076635479927, 0.028729883953928947, 0.07268748432397842, 0.02010435238480568, 0.03894992172718048, 0.07793493568897247, 0.03185012564063072, 0.02957262098789215, 0.08162933588027954, 0.04983029142022133, 0.047570813447237015, 0.014582029543817043, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16675156354904175, 0.040216732770204544, 0.03194630146026611, 0.03915554657578468, 0.04867978394031525, 0.04351935163140297, 0.07027733325958252, 0.046769797801971436, 0.03737051412463188, 0.03785951808094978, 0.04185349866747856, 0.029438776895403862, 0.03496701270341873, 0.04187155142426491, 0.03665957599878311, 0.044189516454935074, 0.04244699329137802, 0.04308999329805374, 0.05540822073817253, 0.024029996246099472, 0.0434984490275383, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.21311715245246887, 0.034521471709012985, 0.026512039825320244, 0.03728467971086502, 0.042002301663160324, 0.05378071218729019, 0.06410717964172363, 0.040195900946855545, 0.029513956978917122, 0.03056204691529274, 0.0481405071914196, 0.01964571699500084, 0.02385595068335533, 0.04712359234690666, 0.027805205434560776, 0.025632811710238457, 0.04690326005220413, 0.0421149916946888, 0.04245335981249809, 0.01620997115969658, 0.047201551496982574, 0.04131561890244484, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.16099098324775696, 0.03149215504527092, 0.025506019592285156, 0.03203630447387695, 0.03811938688158989, 0.05386460945010185, 0.05289203301072121, 0.06297371536493301, 0.03872941806912422, 0.030703023076057434, 0.048614971339702606, 0.016964133828878403, 0.020984860137104988, 0.04743490368127823, 0.02617853879928589, 0.0328170545399189, 0.04874099791049957, 0.03702303394675255, 0.050284430384635925, 0.014425940811634064, 0.04926919564604759, 0.02748514711856842, 0.052469126880168915, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.13924191892147064, 0.04732196405529976, 0.022920068353414536, 0.03782672807574272, 0.03733160346746445, 0.056281935423612595, 0.06773107498884201, 0.06383748352527618, 0.028824560344219208, 0.028368612751364708, 0.05008404701948166, 0.014775171875953674, 0.02105891890823841, 0.04819716140627861, 0.01768510416150093, 0.01545623317360878, 0.0484279990196228, 0.037099551409482956, 0.04247323051095009, 0.018426820635795593, 0.048613302409648895, 0.022673947736620903, 0.059989579021930695, 0.02535295858979225, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.19562353193759918, 0.03233330696821213, 0.02287411503493786, 0.03839076682925224, 0.03449879586696625, 0.046169035136699677, 0.05553746223449707, 0.03960668295621872, 0.023098409175872803, 0.024249287322163582, 0.04043306037783623, 0.016078537330031395, 0.021624932065606117, 0.03903055936098099, 0.026520835235714912, 0.028113454580307007, 0.03823630139231682, 0.035426121205091476, 0.04174571484327316, 0.021097159013152122, 0.037679508328437805, 0.037286948412656784, 0.04423918575048447, 0.022921497002243996, 0.0371846966445446, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05819651111960411, 0.03889738768339157, 0.026005275547504425, 0.023567894473671913, 0.02298000454902649, 0.0538414902985096, 0.058061081916093826, 0.06858772039413452, 0.037459827959537506, 0.02915910631418228, 0.05474670231342316, 0.013592945411801338, 0.02764703892171383, 0.05575406551361084, 0.030779823660850525, 0.0161373820155859, 0.057454273104667664, 0.03968726098537445, 0.03406552970409393, 0.013110971078276634, 0.05977657437324524, 0.01604376919567585, 0.029699308797717094, 0.020566554740071297, 0.08347289264202118, 0.03070864826440811, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15561480820178986, 0.03032318502664566, 0.021067598834633827, 0.03860040381550789, 0.025089265778660774, 0.047195158898830414, 0.04255650192499161, 0.051662612706422806, 0.0303681418299675, 0.020133644342422485, 0.04255694895982742, 0.018511764705181122, 0.01867171935737133, 0.041430287063121796, 0.022022107616066933, 0.02106102928519249, 0.04166616499423981, 0.02724919654428959, 0.0404815673828125, 0.015536420047283173, 0.04219619184732437, 0.017007354646921158, 0.06220949813723564, 0.015703173354268074, 0.054431889206171036, 0.040924131870269775, 0.01572924666106701, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.15218877792358398, 0.03232048824429512, 0.02627759799361229, 0.03215134143829346, 0.03708058223128319, 0.038433320820331573, 0.042185213416814804, 0.03440922126173973, 0.029296180233359337, 0.024752287194132805, 0.034612320363521576, 0.021795062348246574, 0.023419667035341263, 0.03386646881699562, 0.028580572456121445, 0.03440326824784279, 0.03315695375204086, 0.0267103873193264, 0.04213564842939377, 0.01902136579155922, 0.03278576210141182, 0.0268399715423584, 0.033044543117284775, 0.023302637040615082, 0.033900391310453415, 0.03944927081465721, 0.021334243938326836, 0.042546436190605164, 0.0, 0.0, 0.0, 0.0, 0.0], [0.10210391879081726, 0.048305314034223557, 0.027957934886217117, 0.028134869411587715, 0.043688975274562836, 0.03765958547592163, 0.04012179747223854, 0.03589579835534096, 0.02449178136885166, 0.019768044352531433, 0.03467407077550888, 0.015431447885930538, 0.03261655941605568, 0.034143391996622086, 0.027461351826786995, 0.020323237404227257, 0.03445839136838913, 0.015827570110559464, 0.03070586547255516, 0.02036093734204769, 0.03558043763041496, 0.03479030728340149, 0.03518146649003029, 0.028572602197527885, 0.043333835899829865, 0.0389639288187027, 0.0175876896828413, 0.04553377255797386, 0.04632508382201195, 0.0, 0.0, 0.0, 0.0], [0.11841747909784317, 0.026689810678362846, 0.028547655791044235, 0.03516458347439766, 0.035190969705581665, 0.030704839155077934, 0.04254689812660217, 0.02310350351035595, 0.025463134050369263, 0.02967856638133526, 0.02839520201086998, 0.019178245216608047, 0.023833563551306725, 0.02764000929892063, 0.03381749987602234, 0.039640165865421295, 0.027574406936764717, 0.021856172010302544, 0.03966764733195305, 0.03941833972930908, 0.02711559645831585, 0.030498065054416656, 0.03493282198905945, 0.0323205329477787, 0.027840524911880493, 0.03877580910921097, 0.021124783903360367, 0.038611430674791336, 0.03277179226279259, 0.01947995088994503, 0.0, 0.0, 0.0], [0.054337918758392334, 0.01924784667789936, 0.011686465702950954, 0.014118612743914127, 0.013909777626395226, 0.05736802890896797, 0.03454635664820671, 0.04665059223771095, 0.013026447035372257, 0.013271310366690159, 0.05952770635485649, 0.009063145145773888, 0.01736418530344963, 0.06135568022727966, 0.012634634971618652, 0.015145554207265377, 0.06425532698631287, 0.04627060145139694, 0.019556351006031036, 0.012543454766273499, 0.06913559138774872, 0.0348237082362175, 0.030005956068634987, 0.012261828407645226, 0.07985595613718033, 0.01899644546210766, 0.010722928680479527, 0.08654545247554779, 0.01717405952513218, 0.022144146263599396, 0.022453954443335533, 0.0, 0.0], [0.05849645659327507, 0.034377362579107285, 0.013603093102574348, 0.02417042851448059, 0.019005848094820976, 0.06546405702829361, 0.030622974038124084, 0.035732097923755646, 0.018859466537833214, 0.01224147342145443, 0.0628218948841095, 0.01183573342859745, 0.00955235119909048, 0.06431801617145538, 0.021589579060673714, 0.010912971571087837, 0.066659115254879, 0.023997442796826363, 0.02659674547612667, 0.006153563503175974, 0.06984328478574753, 0.0372387059032917, 0.030046699568629265, 0.021686803549528122, 0.04364063963294029, 0.021209634840488434, 0.013490444980561733, 0.04632722586393356, 0.020434463396668434, 0.013362257741391659, 0.0185689777135849, 0.04714025557041168, 0.0], [0.13528689742088318, 0.029733385890722275, 0.026111336424946785, 0.03753574565052986, 0.038881585001945496, 0.03316677734255791, 0.04824390634894371, 0.03046831302344799, 0.026038222014904022, 0.02412370778620243, 0.02789275348186493, 0.02270992286503315, 0.024185288697481155, 0.026114575564861298, 0.023209650069475174, 0.025175301358103752, 0.024614794179797173, 0.02112315222620964, 0.02968505024909973, 0.018167486414313316, 0.023064373061060905, 0.020598091185092926, 0.03441140055656433, 0.021083414554595947, 0.02253885008394718, 0.03228156268596649, 0.01398724876344204, 0.024144934490323067, 0.027002284303307533, 0.012929430231451988, 0.036165352910757065, 0.02684023231267929, 0.032485030591487885]]]}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x16dd7ef70>"
            ]
          },
          "execution_count": 75,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "print(\"Layer 0 Head Attention Patterns:\")\n",
        "cv.attention.attention_patterns(tokens=gpt2_str_tokens, attention=attention_pattern)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP4PhaEn_xXV"
      },
      "source": [
        "## Hooks: Intervening on Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kCkl6d6H_xXV"
      },
      "source": [
        "One of the great things about interpreting neural networks is that we have *full control* over our system. From a computational perspective, we know exactly what operations are going on inside (even if we don't know what they mean!). And we can make precise, surgical edits and see how the model's behaviour and other internals change. This is an extremely powerful tool, because it can let us eg set up careful counterfactuals and causal intervention to easily understand model behaviour. \n",
        "\n",
        "Accordingly, being able to do this is a pretty core operation, and this is one of the main things TransformerLens supports! The key feature here is **hook points**. Every activation inside the transformer is surrounded by a hook point, which allows us to edit or intervene on it. \n",
        "\n",
        "We do this by adding a **hook function** to that activation. The hook function maps `current_activation_value, hook_point` to `new_activation_value`. As the model is run, it computes that activation as normal, and then the hook function is applied to compute a replacement, and that is substituted in for the activation. The hook function can be an arbitrary Python function, so long as it returns a tensor of the correct shape.\n",
        "\n",
        "<details><summary>Relationship to PyTorch hooks</summary>\n",
        "\n",
        "[PyTorch hooks](https://blog.paperspace.com/pytorch-hooks-gradient-clipping-debugging/) are a great and underrated, yet incredibly janky, feature. They can act on a layer, and edit the input or output of that layer, or the gradient when applying autodiff. The key difference is that **Hook points** act on *activations* not layers. This means that you can intervene within a layer on each activation, and don't need to care about the precise layer structure of the transformer. And it's immediately clear exactly how the hook's effect is applied. This adjustment was shamelessly inspired by [Garcon's use of ProbePoints](https://transformer-circuits.pub/2021/garcon/index.html).\n",
        "\n",
        "They also come with a range of other quality of life improvements, like the model having a `model.reset_hooks()` method to remove all hooks, or helper methods to temporarily add hooks for a single forward pass - it is *incredibly* easy to shoot yourself in the foot with standard PyTorch hooks!\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5N0tOdu_xXV"
      },
      "source": [
        "As a basic example, let's ablate head 7 in layer 0 on the text above. \n",
        "\n",
        "We define a `head_ablation_hook` function. This takes the value tensor for attention layer 0, and sets the component with `head_index==7` to zero and returns it (Note - we return by convention, but since we're editing the activation in-place, we don't strictly *need* to).\n",
        "\n",
        "We then use the `run_with_hooks` helper function to run the model and *temporarily* add in the hook for just this run. We enter in the hook as a tuple of the activation name (also the hook point name - found with `utils.get_act_name`) and the hook function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "eKDDEFwO_xXV",
        "outputId": "07fe9fb0-824d-4166-97f8-caa9cadb29d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of the value tensor: torch.Size([1, 33, 12, 64])\n",
            "Original Loss: 3.999\n",
            "Ablated Loss: 4.117\n"
          ]
        }
      ],
      "source": [
        "layer_to_ablate = 0\n",
        "head_index_to_ablate = 8\n",
        "\n",
        "# We define a head ablation hook\n",
        "# The type annotations are NOT necessary, they're just a useful guide to the reader\n",
        "# \n",
        "def head_ablation_hook(\n",
        "    value: TT[\"batch\", \"pos\", \"head_index\", \"d_head\"],\n",
        "    hook: HookPoint\n",
        ") -> TT[\"batch\", \"pos\", \"head_index\", \"d_head\"]:\n",
        "    print(f\"Shape of the value tensor: {value.shape}\")\n",
        "    value[:, :, head_index_to_ablate, :] = 0.\n",
        "    return value\n",
        "\n",
        "original_loss = model(gpt2_tokens, return_type=\"loss\")\n",
        "ablated_loss = model.run_with_hooks(\n",
        "    gpt2_tokens, \n",
        "    return_type=\"loss\", \n",
        "    fwd_hooks=[(\n",
        "        utils.get_act_name(\"v\", layer_to_ablate), \n",
        "        head_ablation_hook\n",
        "        )]\n",
        "    )\n",
        "print(f\"Original Loss: {original_loss.item():.3f}\")\n",
        "print(f\"Ablated Loss: {ablated_loss.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAjAd2Pp_xXW"
      },
      "source": [
        "**Gotcha:** Hooks are global state - they're added in as part of the model, and stay there until removed. `run_with_hooks` tries to create an abstraction where these are local state, by removing all hooks at the end of the function. But you can easily shoot yourself in the foot if there's, eg, an error in one of your hooks so the function never finishes. If you start getting bugs, try `model.reset_hooks()` to clean things up. Further, if you *do* add hooks of your own that you want to keep, which you can do with `add_hook` on the relevant "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7rcOaue_xXW"
      },
      "source": [
        "### Activation Patching on the Indirect Object Identification Task"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5XslKbt_xXW"
      },
      "source": [
        "For a somewhat more involved example, let's use hooks to apply **activation patching** on the **Indirect Object Identification** (IOI) task. \n",
        "\n",
        "The IOI task is the task of identifying that a sentence like \"After John and Mary went to the store, Mary gave a bottle of milk to\" with \" John\" rather than \" Mary\" (ie, finding the indirect object), and Redwood Research have [an excellent paper studying the underlying circuit in GPT-2 Small](https://arxiv.org/abs/2211.00593).\n",
        "\n",
        "**Activation patching** is a technique from [Kevin Meng and David Bau's excellent ROME paper](https://rome.baulab.info/). The goal is to identify which model activations are important for completing a task. We do this by setting up a **clean prompt** and a **corrupted prompt** and a **metric** for performance on the task. We then pick a specific model activation, run the model on the corrupted prompt, but then *intervene* on that activation and patch in its value when run on the clean prompt. We then apply the metric, and see how much this patch has recovered the clean performance. \n",
        "(See [a more detailed explanation of activation patching here](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb#scrollTo=5nUXG6zqmd0f))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NOTKCZ_w_xXW"
      },
      "source": [
        "Here, our clean prompt is \"After John and Mary went to the store, **Mary** gave a bottle of milk to\", our corrupted prompt is \"After John and Mary went to the store, **John** gave a bottle of milk to\", and our metric is the difference between the correct logit ( John) and the incorrect logit ( Mary) on the final token. \n",
        "\n",
        "We see that the logit difference is significantly positive on the clean prompt, and significantly negative on the corrupted prompt, showing that the model is capable of doing the task!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "2fDSsCSb_xXW",
        "outputId": "2cf1d000-fb20-4190-e55f-e5025989e097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Clean logit difference: 4.276\n",
            "Corrupted logit difference: -2.738\n"
          ]
        }
      ],
      "source": [
        "clean_prompt = \"After John and Mary went to the store, Mary gave a bottle of milk to\"\n",
        "corrupted_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
        "\n",
        "clean_tokens = model.to_tokens(clean_prompt)\n",
        "corrupted_tokens = model.to_tokens(corrupted_prompt)\n",
        "\n",
        "def logits_to_logit_diff(logits, correct_answer=\" John\", incorrect_answer=\" Mary\"):\n",
        "    # model.to_single_token maps a string value of a single token to the token index for that token\n",
        "    # If the string is not a single token, it raises an error.\n",
        "    correct_index = model.to_single_token(correct_answer)\n",
        "    incorrect_index = model.to_single_token(incorrect_answer)\n",
        "    return logits[0, -1, correct_index] - logits[0, -1, incorrect_index]\n",
        "\n",
        "# We run on the clean prompt with the cache so we store activations to patch in later.\n",
        "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
        "clean_logit_diff = logits_to_logit_diff(clean_logits)\n",
        "print(f\"Clean logit difference: {clean_logit_diff.item():.3f}\")\n",
        "\n",
        "# We don't need to cache on the corrupted prompt.\n",
        "corrupted_logits = model(corrupted_tokens)\n",
        "corrupted_logit_diff = logits_to_logit_diff(corrupted_logits)\n",
        "print(f\"Corrupted logit difference: {corrupted_logit_diff.item():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4IfOmP8K_xXW"
      },
      "source": [
        "We now setup the hook function to do activation patching. Here, we'll patch in the residual stream at the start of a specific layer and at a specific position. This will let us see how much the model is using the residual stream at that layer and position to represent the key information for the task. \n",
        "\n",
        "We want to iterate over all layers and positions, so we write the hook to take in an position parameter. Hook functions must have the input signature (activation, hook), but we can use `functools.partial` to set the position parameter before passing it to `run_with_hooks`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "Au_L4Dos_xXW",
        "outputId": "d72545ce-1a89-480f-d861-0f4fdb6e9fe4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5f657d60e95041ddad2f316cc145481b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/12 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We define a residual stream patching hook\n",
        "# We choose to act on the residual stream at the start of the layer, so we call it resid_pre\n",
        "# The type annotations are a guide to the reader and are not necessary\n",
        "def residual_stream_patching_hook(\n",
        "    resid_pre: TT[\"batch\", \"pos\", \"d_model\"],\n",
        "    hook: HookPoint,\n",
        "    position: int\n",
        ") -> TT[\"batch\", \"pos\", \"d_model\"]:\n",
        "    # Each HookPoint has a name attribute giving the name of the hook.\n",
        "    clean_resid_pre = clean_cache[hook.name]\n",
        "    resid_pre[:, position, :] = clean_resid_pre[:, position, :]\n",
        "    return resid_pre\n",
        "\n",
        "# We make a tensor to store the results for each patching run. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
        "num_positions = len(clean_tokens[0])\n",
        "ioi_patching_result = torch.zeros((model.cfg.n_layers, num_positions), device=model.cfg.device)\n",
        "\n",
        "for layer in tqdm.tqdm(range(model.cfg.n_layers)):\n",
        "    for position in range(num_positions):\n",
        "        # Use functools.partial to create a temporary hook function with the position fixed\n",
        "        temp_hook_fn = partial(residual_stream_patching_hook, position=position)\n",
        "        # Run the model with the patching hook\n",
        "        patched_logits = model.run_with_hooks(corrupted_tokens, fwd_hooks=[\n",
        "            (utils.get_act_name(\"resid_pre\", layer), temp_hook_fn)\n",
        "        ])\n",
        "        # Calculate the logit difference\n",
        "        patched_logit_diff = logits_to_logit_diff(patched_logits).detach()\n",
        "        # Store the result, normalizing by the clean and corrupted logit difference so it's between 0 and 1 (ish)\n",
        "        ioi_patching_result[layer, position] = (patched_logit_diff - corrupted_logit_diff)/(clean_logit_diff - corrupted_logit_diff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGZio8VT_xXW"
      },
      "source": [
        "We can now visualize the results, and see that this computation is extremely localised within the model. Initially, the second subject (Mary) token is all that matters (naturally, as it's the only different token), and all relevant information remains here until heads in layer 7 and 8 move this to the final token where it's used to predict the indirect object.\n",
        "(Note - the heads are in layer 7 and 8, not 8 and 9, because we patched in the residual stream at the *start* of each layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "xyBS23ah_xXW",
        "outputId": "796dba6c-8076-426c-fd95-cbd92b54f1bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"863d335b-d7b6-4a3a-a61c-0e521b1330d4\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"863d335b-d7b6-4a3a-a61c-0e521b1330d4\")) {                    Plotly.newPlot(                        \"863d335b-d7b6-4a3a-a61c-0e521b1330d4\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"x\":[\"<|endoftext|>_0\",\"After_1\",\" John_2\",\" and_3\",\" Mary_4\",\" went_5\",\" to_6\",\" the_7\",\" store_8\",\",_9\",\" Mary_10\",\" gave_11\",\" a_12\",\" bottle_13\",\" of_14\",\" milk_15\",\" to_16\"],\"z\":[[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.0,0.0,0.0,0.0,0.0,0.0],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9981498718261719,0.0016003302298486233,0.0001506513071944937,-0.0003722773399204016,-2.1754702174803242e-05,-0.0006297986255958676,-0.0005136828985996544],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.998055100440979,0.002283971756696701,0.00018029208877123892,-0.0005049810279160738,-0.00026975831133313477,-5.2483217586996034e-05,-0.0012813518987968564],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9967373609542847,0.004081454128026962,0.0009705316042527556,4.4325202907202765e-05,-0.00015880931459832937,-0.00033855755464173853,-0.001943238778039813],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9905902743339539,0.019987404346466064,0.0018948344513773918,0.0010132251773029566,-6.852731166873127e-05,0.0009123377967625856,-0.0018994574202224612],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9616498351097107,0.08534641563892365,0.005200733430683613,0.0030521845910698175,0.00019552037701942027,0.0011089459294453263,-0.0022845156490802765],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.9630984663963318,0.0843726173043251,0.004123060032725334,0.0007160016102716327,0.000103606769698672,0.0010004442883655429,-0.0042157890275120735],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.935918390750885,0.11111757904291153,0.007704971358180046,0.0003749966563191265,0.0003643912496045232,0.0013281245483085513,0.01874493807554245],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.770156979560852,0.037418629974126816,0.002068872097879648,-8.51152726681903e-05,0.00013406334619503468,0.001727323280647397,0.4499054551124573],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.09650412946939468,0.025925077497959137,0.0019712478388100863,0.00032849598210304976,0.0004214973305352032,0.0018834132933989167,0.8994735479354858],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.02332158386707306,0.01853826828300953,0.0015842861030250788,0.0005242883344180882,0.00025072292191907763,0.0008712757844477892,0.9612751603126526],[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,-0.008558299392461777,0.006338504143059254,0.000581666361540556,-0.00034073300776071846,0.00010768577340058982,0.0006477462593466043,0.9495835900306702]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Position: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Position\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Normalized Logit Difference After Patching Residual Stream on the IOI Task\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('863d335b-d7b6-4a3a-a61c-0e521b1330d4');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Add the index to the end of the label, because plotly doesn't like duplicate labels\n",
        "token_labels = [f\"{token}_{index}\" for index, token in enumerate(model.to_str_tokens(clean_tokens))]\n",
        "imshow(ioi_patching_result, x=token_labels, xaxis=\"Position\", yaxis=\"Layer\", title=\"Normalized Logit Difference After Patching Residual Stream on the IOI Task\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5ktxigr_xXW"
      },
      "source": [
        "## Hooks: Accessing Activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fBb89qD_xXW"
      },
      "source": [
        "Hooks can also be used to just **access** an activation - to run some function using that activation value, *without* changing the activation value. This can be achieved by just having the hook return nothing, and not editing the activation in place. \n",
        "\n",
        "This is useful for eg extracting activations for a specific task, or for doing some long-running calculation across many inputs, eg finding the text that most activates a specific neuron. (Note - everything this can do *could* be done with `run_with_cache` and post-processing, but this workflow can be more intuitive and memory efficient.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qeZnp4og_xXW"
      },
      "source": [
        "To demonstrate this, let's look for **[induction heads](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html)** in GPT-2 Small. \n",
        "\n",
        "Induction circuits are a very important circuit in generative language models, which are used to detect and continue repeated subsequences. They consist of two heads in separate layers that compose together, a **previous token head** which always attends to the previous token, and an **induction head** which attends to the token *after* an earlier copy of the current token. \n",
        "\n",
        "To see why this is important, let's say that the model is trying to predict the next token in a news article about Michael Jordan. The token \" Michael\", in general, could be followed by many surnames. But an induction head will look from that occurence of \" Michael\" to the token after previous occurences of \" Michael\", ie \" Jordan\" and can confidently predict that that will come next."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Al9eoqIp_xXW"
      },
      "source": [
        "An interesting fact about induction heads is that they generalise to arbitrary sequences of repeated tokens. We can see this by generating sequences of 50 random tokens, repeated twice, and plotting the average loss at predicting the next token, by position. We see that the model goes from terrible to very good at the halfway point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "Sb4qCgd8_xXW",
        "outputId": "8dd54eee-f854-4c6e-efc0-bc81a74f5fa7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"a637e30f-67fe-489f-96e9-1b1ff73b6f85\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"a637e30f-67fe-489f-96e9-1b1ff73b6f85\")) {                    Plotly.newPlot(                        \"a637e30f-67fe-489f-96e9-1b1ff73b6f85\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98],\"xaxis\":\"x\",\"y\":[11.041927337646484,14.925912857055664,13.329785346984863,12.08727741241455,16.01771354675293,12.754076957702637,12.951601028442383,12.83857536315918,11.454911231994629,13.02479362487793,13.142663955688477,13.07066535949707,11.819684982299805,11.908781051635742,11.747172355651855,12.418828010559082,11.571779251098633,12.940183639526367,12.483393669128418,11.945897102355957,11.116510391235352,11.867513656616211,11.926597595214844,12.45372200012207,10.79355525970459,12.415575981140137,11.334389686584473,11.18908405303955,11.694379806518555,11.931695938110352,10.530255317687988,12.145119667053223,12.687650680541992,11.13563346862793,12.334027290344238,10.976140022277832,11.497519493103027,11.655136108398438,11.156564712524414,12.092255592346191,11.28429889678955,11.584871292114258,11.681800842285156,11.238826751708984,11.494606971740723,11.02489185333252,10.869157791137695,12.596515655517578,11.68422794342041,10.098185539245605,2.889242649078369,1.4992916584014893,0.49662017822265625,0.812248706817627,0.7675517797470093,0.1988450586795807,0.6786874532699585,0.29768651723861694,0.09365750104188919,0.16393479704856873,0.11757178604602814,0.31885814666748047,0.07097567617893219,0.08853359520435333,0.2501283884048462,0.39061570167541504,0.15324929356575012,0.09070645272731781,0.21464328467845917,0.5098739862442017,0.15635743737220764,0.06190872937440872,0.264312744140625,0.15014079213142395,0.03847258910536766,0.3430064022541046,0.16822689771652222,0.16603270173072815,0.03944282978773117,0.012070822529494762,0.0491936132311821,0.013411121442914009,0.05257860943675041,0.008781904354691505,0.11761561781167984,0.1945173591375351,0.04533224552869797,0.058078981935977936,0.040719352662563324,0.026615699753165245,0.15778708457946777,0.10779577493667603,0.027649348601698875,0.6803348660469055,0.037671755999326706,0.16473500430583954,0.17429731786251068,0.27955862879753113,0.10633353143930435],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Loss by position on random repeated tokens\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('a637e30f-67fe-489f-96e9-1b1ff73b6f85');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "batch_size = 10\n",
        "seq_len = 50\n",
        "random_tokens = torch.randint(1000, 10000, (batch_size, seq_len)).to(model.cfg.device)\n",
        "repeated_tokens = einops.repeat(random_tokens, \"batch seq_len -> batch (2 seq_len)\")\n",
        "repeated_logits = model(repeated_tokens)\n",
        "correct_log_probs = model.loss_fn(repeated_logits, repeated_tokens, per_token=True)\n",
        "loss_by_position = einops.reduce(correct_log_probs, \"batch position -> position\", \"mean\")\n",
        "line(loss_by_position, xaxis=\"Position\", yaxis=\"Loss\", title=\"Loss by position on random repeated tokens\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EIe36w2Z_xXW"
      },
      "source": [
        "The induction heads will be attending from the second occurence of each token to the token *after* its first occurence, ie the token `50-1==49` places back. So by looking at the average attention paid 49 tokens back, we can identify induction heads! Let's define a hook to do this!\n",
        "\n",
        "<details><summary>Technical details</summary>\n",
        "\n",
        "* We attach the hook to the attention pattern activation. There's one big pattern activation per layer, stacked across all heads, so we need to do some tensor manipulation to get a per-head score. \n",
        "* Hook functions can access global state, so we make a big tensor to store the induction head score for each head, and then we just add the score for each head to the appropriate position in the tensor. \n",
        "* To get a single hook function that works for each layer, we use the `hook.layer()` method to get the layer index (internally this is just inferred from the hook names).\n",
        "* As we want to add this to *every* activation pattern hook point, rather than giving the string for an activation name, this time we give a **name filter**. This is a Boolean function on hook point names, and it adds the hook function to every hook point where the function evaluates as true. \n",
        "    * `run_with_hooks` allows us to enter a list of (act_name, hook_function) pairs to all be added at once, so we could also have done this by inputting a list with a hook for each layer.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "KtNIq5p4_xXW",
        "outputId": "d6c0a395-c83d-42cb-8979-cc0a176dbdb2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"8ee72de1-bffc-439f-8a22-6941a87ffe05\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"8ee72de1-bffc-439f-8a22-6941a87ffe05\")) {                    Plotly.newPlot(                        \"8ee72de1-bffc-439f-8a22-6941a87ffe05\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.009409443475306034,8.177491690730676e-05,0.01129238773137331,1.3719411526835756e-06,0.00019874215649906546,0.00012880245049018413,0.009763447567820549,0.0006882605375722051,0.008568796329200268,0.00936545804142952,0.0068737054243683815,0.015072434209287167],[0.0007193253841251135,0.0003980577748734504,0.0021190806291997433,0.014338715001940727,0.005091056227684021,0.010803021490573883,0.014898288995027542,0.013293915428221226,0.012812520377337933,0.016126051545143127,0.006496291141957045,0.0005970040801912546],[0.004599516745656729,0.01732069067656994,0.0024832356721162796,0.0011265965877100825,0.013685440644621849,0.0018735654884949327,0.0037168909329921007,0.008512289263308048,0.003705785609781742,0.00128368369769305,0.0007011652342043817,0.010433654300868511],[0.013557948172092438,0.0072731818072497845,0.003199916798621416,0.011724402196705341,0.02125765196979046,0.013299196027219296,0.002689731540158391,0.0014383868547156453,0.006328609772026539,0.013265210203826427,0.008448063395917416,0.007292576599866152],[0.014431442134082317,0.013392835855484009,0.014665938913822174,0.009900620207190514,0.018412664532661438,0.013118497096002102,0.00875379890203476,0.002114014932885766,0.015763523057103157,0.014504674822092056,0.01779916323721409,1.0363887525954851e-08],[0.457322895526886,0.9010140895843506,0.01568819023668766,0.008857768960297108,0.011035887524485588,0.9287804961204529,0.013216698542237282,0.017476815730333328,0.028779441490769386,0.027788640931248665,0.01731295697391033,0.0182878989726305],[0.01176219992339611,0.017219368368387222,0.01916210539638996,0.0157888513058424,0.02113356813788414,0.012084968388080597,0.030516283586621284,0.01213548518717289,0.011485890485346317,0.9041900038719177,0.0364539735019207,0.013082790188491344],[0.01231278758496046,0.17269685864448547,0.8503395318984985,0.0186309777200222,0.018977094441652298,0.01801256090402603,0.047203123569488525,0.0868878960609436,0.017176618799567223,0.019050342962145805,0.9185282588005066,0.06652513891458511],[0.016583852469921112,0.3889826834201813,0.017009299248456955,0.042924508452415466,0.017407599836587906,0.01396039966493845,0.13413608074188232,0.012729444541037083,0.03463652357459068,0.0317145437002182,0.0636509582400322,0.026576030999422073],[0.241459921002388,0.19432218372821808,0.10764322429895401,0.012330232188105583,0.08735557645559311,0.0264559518545866,0.46035876870155334,0.02957436814904213,0.049177590757608414,0.46946582198143005,0.01703895814716816,0.048766959458589554],[0.3538041114807129,0.49287182092666626,0.040931832045316696,0.14546363055706024,0.061491020023822784,0.015825962647795677,0.2918548285961151,0.4747125506401062,0.05102735012769699,0.016532793641090393,0.15862210094928741,0.25594955682754517],[0.018020696938037872,0.05963464081287384,0.035775963217020035,0.010819154791533947,0.036517184227705,0.10116837918758392,0.05298011004924774,0.0695652887225151,0.009171897545456886,0.3105785548686981,0.380550354719162,0.024707259610295296]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Induction Score by Head\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('8ee72de1-bffc-439f-8a22-6941a87ffe05');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# We make a tensor to store the induction score for each head. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
        "induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
        "def induction_score_hook(\n",
        "    pattern: TT[\"batch\", \"head_index\", \"dest_pos\", \"source_pos\"],\n",
        "    hook: HookPoint,\n",
        "):\n",
        "    # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
        "    # (This only has entries for tokens with index>=seq_len)\n",
        "    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)\n",
        "    # Get an average score per head\n",
        "    induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
        "    # Store the result.\n",
        "    induction_score_store[hook.layer(), :] = induction_score\n",
        "\n",
        "# We make a boolean filter on activation names, that's true only on attention pattern names.\n",
        "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
        "\n",
        "model.run_with_hooks(\n",
        "    repeated_tokens, \n",
        "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
        "    fwd_hooks=[(\n",
        "        pattern_hook_names_filter,\n",
        "        induction_score_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "imshow(induction_score_store, xaxis=\"Head\", yaxis=\"Layer\", title=\"Induction Score by Head\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0pG88y9_xXW"
      },
      "source": [
        "Head 5 in Layer 5 scores extremely highly on this score, and we can feed in a shorter repeated random sequence, visualize the attention pattern for it and see this directly - including the \"induction stripe\" at `seq_len-1` tokens back.\n",
        "\n",
        "This time we put in a hook on the attention pattern activation to visualize the pattern of the relevant head."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "CAm_CCV8_xXX",
        "outputId": "5a95138c-5cb9-48f2-ab28-0d8f92c501bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div id=\"circuits-vis-1169fd3a-ab7d\" style=\"margin: 15px 0;\"/>\n",
              "    <script crossorigin type=\"module\">\n",
              "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.34.0/dist/cdn/esm.js\";\n",
              "    render(\n",
              "      \"circuits-vis-1169fd3a-ab7d\",\n",
              "      AttentionPatterns,\n",
              "      {\"tokens\": [\" damage\", \" purs\", \" seemingly\", \" WH\", \"ogle\", \"aturday\", \"type\", \"ondon\", \" horror\", \" complete\", \" Japanese\", \" Indian\", \" frag\", \"////\", \" dozen\", \"MP\", \"Before\", \" foundation\", \"ups\", \" Free\", \" damage\", \" purs\", \" seemingly\", \" WH\", \"ogle\", \"aturday\", \"type\", \"ondon\", \" horror\", \" complete\", \" Japanese\", \" Indian\", \" frag\", \"////\", \" dozen\", \"MP\", \"Before\", \" foundation\", \"ups\", \" Free\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9984912872314453, 0.0015087014762684703, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.995624840259552, 0.0013407168444246054, 0.0030345299746841192, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9960297346115112, 0.0026835475582629442, 0.000366547959856689, 0.0009201789507642388, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.988231360912323, 0.0006289159646257758, 0.005506334360688925, 0.003647863632068038, 0.0019853527192026377, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9812600612640381, 0.0003074259730055928, 0.00021124766499269754, 0.006538521032780409, 0.0004286302428226918, 0.011254064738750458, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9045183658599854, 0.0009198429179377854, 0.0007341884775087237, 0.06123827025294304, 0.0006113594863563776, 0.030379949137568474, 0.0015979802701622248, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7613058090209961, 0.00020324616343714297, 0.005381282418966293, 0.028916597366333008, 3.4300406696274877e-05, 0.01034645363688469, 0.19311806559562683, 0.000694051559548825, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9151209592819214, 0.05309148505330086, 0.00022251087648328394, 0.01338532380759716, 0.00019093048467766494, 0.0017834787722676992, 0.013396184891462326, 0.0001464214874431491, 0.0026626584585756063, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9171945452690125, 0.0017759815091267228, 0.0030558821745216846, 0.061086881905794144, 5.448474894365063e-06, 0.00023117098317015916, 0.0026024850085377693, 0.0014175201067700982, 0.012407450936734676, 0.00022262790298555046, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9514854550361633, 0.00022867912775836885, 7.844818901503459e-05, 0.010118203237652779, 0.0013402587501332164, 0.0018051000079140067, 0.0013906281674280763, 0.0036816420033574104, 0.027245165780186653, 0.002472338732331991, 0.00015424349112436175, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9210866689682007, 0.0001211214330396615, 1.838577645685291e-06, 0.0008932216442190111, 1.2553820738503418e-07, 1.2203059668536298e-05, 2.8988567919441266e-06, 3.8173297070898116e-05, 0.0011309193214401603, 0.0023104352876544, 1.2921096640639007e-05, 0.07438952475786209, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8780173659324646, 0.08292955160140991, 0.01976747065782547, 0.0038603406865149736, 5.520735157915624e-06, 0.0010451357811689377, 1.2662168956012465e-05, 0.0009086257196031511, 0.0006046267226338387, 0.0024346644058823586, 0.001459232997149229, 0.0011537825921550393, 0.007800959516316652, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9554322361946106, 0.016554154455661774, 0.007325227838009596, 0.0056072925217449665, 0.0002416393836028874, 0.0006913119577802718, 0.0005860626115463674, 2.1530695448745973e-05, 0.0006733006448484957, 0.00045193094410933554, 0.0012404065346345305, 3.3462529245298356e-05, 3.893702341883909e-06, 0.011137541383504868, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9150317907333374, 0.010156702250242233, 0.0026018358767032623, 0.009519870392978191, 0.02947431616485119, 0.0069865938276052475, 0.00400221161544323, 0.0013217938831076026, 0.0008355348836630583, 0.0004785328346770257, 0.0015846762107685208, 0.0011414664331823587, 0.00036795769119635224, 0.0014831305015832186, 0.015013497322797775, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9648393988609314, 0.004967964719980955, 0.003008908359333873, 0.00039300904609262943, 0.0014007356949150562, 0.0004991136374883354, 0.0013642751146107912, 0.0003786266315728426, 0.0005588054191321135, 6.561022746609524e-05, 9.336376388091594e-05, 2.6935422283713706e-05, 3.2788063890620833e-06, 0.0002104984305333346, 0.020210115239024162, 0.0019794178660959005, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9171012043952942, 0.0005994010716676712, 0.0008642454049549997, 0.006190742831677198, 0.006739488802850246, 0.004912836942821741, 0.019813362509012222, 0.00504511222243309, 0.010019869543612003, 0.00012490880908444524, 0.019002055749297142, 0.0002784296520985663, 3.89208180422429e-05, 0.0006278998334892094, 0.004537391476333141, 0.0006303242989815772, 0.0034739391412585974, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9153411984443665, 0.0007665643352083862, 0.0011231760727241635, 0.0005882779369130731, 4.7521243118353595e-07, 0.00016641254478599876, 0.0008064314024522901, 0.0038978795055299997, 0.0177287720143795, 4.063393862452358e-05, 0.026840809732675552, 0.0001764310582075268, 0.00023998603865038604, 0.0025043038185685873, 0.0002298368635820225, 0.0006082800682634115, 0.0002770244609564543, 0.028663329780101776, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7923771739006042, 0.007601678371429443, 0.02896992489695549, 0.003925200551748276, 5.343564407667145e-05, 8.950288611231372e-05, 0.002631203271448612, 0.0012726816348731518, 0.0062936944887042046, 0.0002184416662203148, 0.019347673282027245, 0.0016345203621312976, 7.011080015217885e-05, 0.0025341815780848265, 0.012410754337906837, 0.013074958696961403, 0.06773890554904938, 0.006337034050375223, 0.033418890088796616, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9767994284629822, 0.00035928800934925675, 0.0005511766066774726, 0.00020199353457428515, 2.6019564757007174e-05, 3.797685349127278e-05, 1.5140603863983415e-05, 6.358391692629084e-05, 0.00016118634084705263, 4.677484412241029e-06, 0.0002613130782265216, 3.9459373510908335e-05, 1.0277446563122794e-05, 0.00040216330671682954, 0.0032755862921476364, 0.002932487754151225, 0.003315272741019726, 0.006706069223582745, 0.003845719387754798, 0.0009911711094900966, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.018083278089761734, 0.973302960395813, 0.003956995438784361, 0.0002845396811608225, 3.5358550576347625e-06, 0.00010839788592420518, 5.9599115047603846e-06, 1.2872824299847707e-06, 1.4685554560855962e-05, 0.0004287857736926526, 0.00033992266980931163, 8.113173862511758e-06, 1.153404241449607e-06, 0.0011511433403939009, 0.0017699680756777525, 0.00023713606060482562, 8.312312274938449e-05, 5.5037780839484185e-05, 2.5966315661207773e-05, 0.00010040713823400438, 3.7516474549192935e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.011865666136145592, 0.0003379161062184721, 0.9641554951667786, 0.0022403232287615538, 8.62025535752764e-06, 0.008872082456946373, 4.310333315515891e-05, 5.055396104580723e-05, 0.002202258910983801, 3.996693976660026e-06, 0.0002966761530842632, 3.614663910411764e-06, 4.729608704678867e-08, 0.0004677930264733732, 0.0019146162085235119, 0.0007366389618255198, 0.0006387679604813457, 5.635073102894239e-05, 0.002374078147113323, 0.0029988116584718227, 0.0006224981043487787, 0.00011015179916284978, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04186353459954262, 0.0012501563178375363, 0.0005241777398623526, 0.8923978209495544, 5.470903852256015e-05, 0.016343891620635986, 0.0011088093742728233, 0.02031615749001503, 0.0058289398439228535, 0.0006560759502463043, 0.0059747654013335705, 0.0038701877929270267, 2.076750752166845e-05, 0.0011986801400780678, 0.005917553789913654, 0.001049560378305614, 0.0005639626760967076, 0.0002731570275500417, 0.00027517476701177657, 0.0002744859375525266, 8.880063433025498e-06, 8.207147038774565e-05, 0.00014655021368525922, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04454744607210159, 0.0005877232179045677, 3.897331771440804e-05, 0.00015955349954310805, 0.9017705917358398, 0.0466463565826416, 0.00044674286618828773, 0.0002805035328492522, 3.178143742843531e-05, 0.00011469367746030912, 2.0798049717996037e-06, 0.0006339531391859055, 1.649806108616758e-05, 0.000447437894763425, 0.0008181640296243131, 0.0006556370062753558, 0.0008783296216279268, 0.0013129225699231029, 6.577324711543042e-06, 9.863474588200916e-06, 2.2420375898946077e-05, 3.466930502327159e-05, 6.214497989276424e-05, 0.0004749053914565593, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010001556016504765, 3.986771480413154e-05, 0.00011763747170334682, 0.00027123672771267593, 4.998167059966363e-05, 0.9865765571594238, 0.0010149009758606553, 0.0007056677131913602, 0.0002493101346772164, 4.427643034432549e-06, 2.029120651059202e-06, 1.8546371620686841e-06, 1.9756042490826076e-07, 1.2473998140194453e-05, 0.00022461218759417534, 7.55304135964252e-05, 8.161988080246374e-05, 0.00011868059664266184, 6.678384579572594e-06, 2.0509942260105163e-06, 5.157497071195394e-06, 8.603554420005821e-07, 6.969270907575265e-05, 0.0001362117036478594, 0.000231205063755624, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.022725407034158707, 1.636564775253646e-05, 1.5090506167325657e-05, 0.001185493660159409, 1.2262471500434913e-05, 0.003120026784017682, 0.9583310484886169, 5.044552017352544e-05, 0.005589596461504698, 0.0005927315214648843, 0.00010411605762783438, 8.068320312304422e-06, 2.50193039619262e-07, 6.691137059533503e-06, 0.0003336091758683324, 4.234675725456327e-05, 0.00010975944314850494, 0.006254167295992374, 2.957484139187727e-05, 4.947132765664719e-05, 4.919294838146016e-07, 3.4401969628561346e-07, 1.374447265334311e-06, 0.00043727067532017827, 1.0241633390251081e-05, 0.0009737213840708137, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.013361349701881409, 1.1243156222917605e-05, 9.080774361791555e-06, 0.005184991285204887, 5.398532039180282e-07, 0.0012281285598874092, 3.622982694651e-05, 0.9672775268554688, 0.002299741143360734, 8.312215868500061e-06, 0.0002515222586225718, 5.7856021157931536e-05, 1.608835304978129e-06, 3.814884985331446e-05, 4.845572038902901e-05, 8.922511369746644e-06, 0.00013405426579993218, 9.858868725132197e-05, 0.0002014678029809147, 0.00013711566862184554, 1.367501454296871e-06, 6.48434536287823e-07, 2.315962774446234e-06, 0.00896262563765049, 1.5506109320995165e-06, 0.0006000497960485518, 3.6615314456867054e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06638795137405396, 7.165834176703356e-06, 0.00020394347666297108, 0.013389303348958492, 8.84142359325324e-09, 0.00025357658159919083, 0.002783025149255991, 0.00018392612400930375, 0.89545738697052, 0.00047527457354590297, 0.0018414403311908245, 0.001342273666523397, 9.752743790159002e-05, 0.00023707884247414768, 0.0002260188339278102, 3.216247023374308e-06, 3.3373598853359e-05, 0.0009750201134011149, 0.0031993056181818247, 0.0016776861157268286, 2.6594612791086547e-05, 6.311188940344437e-07, 3.021420525328722e-05, 0.006435129791498184, 2.0838834657865846e-08, 5.309681000653654e-05, 0.004466582089662552, 0.00021309549629222602, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04541865363717079, 0.004564700182527304, 1.3155377928342205e-05, 0.006978414952754974, 4.5234270373839536e-07, 0.00014590680075343698, 0.0006903269677422941, 1.0858948371605948e-05, 0.0004861913912463933, 0.9345020651817322, 5.9240795962978154e-05, 0.00043829853530041873, 0.0002605041954666376, 0.0010175199713557959, 0.0009717009961605072, 1.8869635823648423e-05, 2.414886148471851e-05, 1.2354209502518643e-05, 2.0286829567339737e-06, 1.6135192709043622e-05, 7.261369461275535e-08, 0.00014578024274669588, 5.75867204588576e-07, 0.0038107011932879686, 1.2783179954567458e-06, 3.281824683654122e-05, 0.0002648893278092146, 9.165196956928412e-07, 0.00011142343282699585, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.04995863884687424, 0.0004168839077465236, 7.646984158782288e-05, 0.025538351386785507, 5.689905435701803e-08, 2.6068468287121505e-05, 0.0006773328059352934, 0.00021174899302423, 0.0017350511625409126, 0.00012224531383253634, 0.8870336413383484, 2.1922938685747795e-05, 9.979721653508022e-07, 0.0007515986217185855, 0.0028933079447597265, 7.540974911535159e-05, 3.6095727409701794e-05, 0.0032502594403922558, 0.0007817845325917006, 0.00037036684807389975, 3.754296267288737e-05, 6.817109533585608e-05, 1.122857975133229e-05, 0.02485489659011364, 5.866683139288398e-08, 4.263607934262836e-06, 0.00017715166904963553, 2.6791572963702492e-05, 0.0008346481481567025, 7.327545517910039e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.09347772598266602, 3.0182962291291915e-05, 5.616381258732872e-06, 0.003392311278730631, 3.774471770157106e-05, 0.0002807966957334429, 0.00011124503362225369, 0.00014695775462314487, 0.0030225305818021297, 0.0003958618326578289, 2.761833820841275e-05, 0.8425484895706177, 0.04550248011946678, 0.0033454466611146927, 0.0002539437264204025, 1.5170130609476473e-05, 0.00017716670117806643, 0.0003479468869045377, 2.8961911084479652e-05, 0.00016510083514731377, 1.554984919494018e-06, 6.862390478090674e-07, 1.459987629459647e-06, 0.0055907731875777245, 0.00014059605018701404, 8.906037692213431e-05, 2.312820106453728e-05, 1.6628302546450868e-05, 0.0007198434905149043, 9.541598410578445e-05, 7.4593949648260605e-06, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.06907626241445541, 1.3497216059477068e-05, 6.102096961058123e-08, 0.00014412307064048946, 2.3661663739460437e-09, 1.5476073258469114e-06, 1.5692472743467079e-07, 9.990208127419464e-07, 0.00011348764383001253, 0.0002116446557920426, 1.1491741815916612e-06, 0.04463915899395943, 0.8593131303787231, 0.01880493015050888, 4.374729178380221e-05, 3.792396000790177e-06, 1.4647813486590167e-06, 2.162780219805427e-06, 2.6093605356436456e-06, 1.7374846947859623e-06, 2.5230130873410417e-08, 6.62315713384487e-08, 2.741891602653368e-09, 5.1366172556299716e-05, 8.925424843653218e-09, 2.823543638896808e-07, 4.3189523069031566e-09, 1.1348145534384457e-08, 3.562144001989509e-06, 5.967391643935116e-06, 7.472653607010216e-08, 0.007562943268567324, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.08346474170684814, 0.019865598529577255, 0.0004346933274064213, 0.0007543122628703713, 4.400999031872743e-08, 5.176935519557446e-05, 1.5855711410495132e-07, 2.5019966415129602e-05, 4.092602830496617e-05, 0.00020243582548573613, 5.472957491292618e-05, 0.0009043713798746467, 0.0033885736484080553, 0.8823279738426208, 0.004722364712506533, 0.00012971964315511286, 0.00017458999354857951, 2.926027491412242e-06, 0.00035197200486436486, 0.0001282381999772042, 5.196401616558433e-06, 0.0018148558447137475, 0.0001575847272761166, 0.0003028003848157823, 5.258655733086925e-07, 3.010002001246903e-05, 8.49462278296187e-09, 2.6226489353575744e-06, 4.094829819223378e-06, 1.5532983525190502e-05, 6.127589585958049e-06, 0.00012612946738954633, 0.0005093810032121837, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.010984113439917564, 0.00032101888791657984, 7.489464042009786e-05, 0.0001540315424790606, 3.6447755746849e-07, 7.819242455298081e-06, 2.1644775642926106e-06, 7.893814313320036e-08, 5.0560274758026935e-06, 4.302402430766961e-06, 6.315545306279091e-06, 1.239678113051923e-06, 6.504355809511253e-08, 0.0008479579701088369, 0.9843552112579346, 4.367695146356709e-05, 0.0023264028131961823, 4.1367948142578825e-06, 2.6946750608658476e-07, 8.416556738666259e-06, 1.890642124635633e-06, 5.048412276664749e-05, 0.00041125190909951925, 0.0002783992968033999, 2.1508740246645175e-06, 1.4072769772610627e-05, 9.161548746305925e-07, 1.740807675787437e-08, 1.1089175586675992e-06, 5.935166882409249e-07, 1.4545563544743345e-06, 4.774319108946656e-07, 8.41613356783455e-09, 8.973773947218433e-05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.026594357565045357, 0.0009394707158207893, 4.878790059592575e-05, 0.00029646369512192905, 0.00026110088219866157, 0.00013565643166657537, 2.2518981495522894e-05, 7.968402314872947e-06, 9.103378033614717e-06, 7.034317150100833e-06, 1.4274226487032138e-05, 3.862121229758486e-05, 3.858842774207005e-06, 0.00021526888303924352, 0.004720638506114483, 0.9371508955955505, 0.02485170029103756, 3.561483390512876e-05, 3.753430428332649e-05, 0.00023114413488656282, 0.00023854059691075236, 1.4973934412410017e-05, 3.481640305835754e-05, 0.00016821852477733046, 0.0006630293210037053, 0.00025657896185293794, 8.06116258900147e-06, 1.6583479691689718e-06, 9.156905207419186e-07, 3.6338335007712885e-07, 1.1591928341658786e-06, 4.970763711753534e-06, 2.5123560476458806e-07, 2.600761035864707e-05, 0.0029585871379822493, 0.0, 0.0, 0.0, 0.0, 0.0], [0.01749996468424797, 9.695961489342153e-05, 2.9236003683763556e-05, 1.4204653780325316e-05, 3.1398139981320128e-06, 6.604164809687063e-06, 4.191365860606311e-06, 2.412453568467754e-06, 3.1633990147383884e-06, 4.2013971324195154e-07, 4.809328970623028e-07, 1.9330452971644263e-07, 8.063462786367381e-09, 6.243437837838428e-06, 0.0018595467554405332, 0.00028950051637366414, 0.9789040684700012, 1.6053501894930378e-05, 1.7655560213825083e-06, 4.7804885980440304e-05, 8.604374670539983e-06, 4.5112642510503065e-07, 2.0349052647361532e-05, 1.0409377864561975e-05, 8.284315299533773e-06, 1.3304264939506538e-05, 7.836752047296613e-07, 7.027110768831335e-07, 2.546077553233772e-07, 1.4355770794338696e-08, 4.020105848212552e-08, 3.063548703607921e-08, 1.5812973153117582e-10, 2.985080982398358e-07, 0.0006051847012713552, 0.0005451763281598687, 0.0, 0.0, 0.0, 0.0], [0.04973329231142998, 5.283224527374841e-05, 3.8761496398365125e-05, 0.0006397291435860097, 0.00021509340149350464, 0.0007118131034076214, 0.0009943991899490356, 0.0002278007159475237, 0.000617224897723645, 3.957070930482587e-06, 0.00042181616299785674, 1.3417721675068606e-05, 6.065986895009701e-07, 5.960563066764735e-05, 0.0010314971441403031, 8.374607568839565e-05, 0.0009692514431662858, 0.9375083446502686, 0.0010364694753661752, 4.2678493628045544e-05, 0.0025724058505147696, 8.853505278239027e-06, 6.152176501927897e-05, 0.0014279024908319116, 0.00019087140390183777, 0.0005590522778220475, 0.00027935599791817367, 3.8721704186173156e-05, 0.00022383651230484247, 1.1781104802821574e-07, 0.00010944433597614989, 4.135150902584428e-06, 8.537057283319882e-08, 3.3134786008304218e-06, 2.886961738113314e-05, 1.4137744983599987e-05, 7.511390140280128e-05, 0.0, 0.0, 0.0], [0.013591397553682327, 1.559198608447332e-05, 2.4708966520847753e-05, 3.093709165113978e-05, 1.6213809184151273e-09, 3.487606136332033e-06, 1.1282852028671186e-05, 2.087652319460176e-05, 0.0005035183276049793, 5.189611442801834e-07, 0.0004640358092729002, 5.369312020775396e-06, 2.1478508642758243e-06, 0.00014765399100724608, 1.2562850315589458e-05, 1.1558351616258733e-05, 1.1413586435082834e-05, 0.0010515621397644281, 0.9811865091323853, 0.0006057844148017466, 0.0006025095353834331, 1.5230586996040074e-06, 1.1110523701063357e-05, 1.6395642887800932e-05, 2.9034250559334396e-09, 2.503977611922892e-06, 1.6873375443537952e-06, 5.332580713002244e-06, 0.001000255811959505, 4.2348897721922185e-09, 0.00020410868455655873, 1.672129087637586e-06, 2.2198901206138544e-07, 1.4644158000010066e-05, 2.009857524853942e-07, 9.254677024728153e-07, 9.160103786598484e-08, 0.00043574091978371143, 0.0, 0.0], [0.07567703723907471, 0.0004366639768704772, 0.006016978994011879, 0.0008312161662615836, 4.142863190281787e-07, 1.3458941793942358e-05, 0.0001334310945821926, 0.00012418786354828626, 0.002190695144236088, 8.931085176300257e-06, 0.0007114218315109611, 9.620126365916803e-05, 8.427198849858542e-07, 0.0003388846234884113, 0.0033415164798498154, 0.0015199462650343776, 0.022509703412652016, 0.00042476478847675025, 0.0030570540111511946, 0.8697013854980469, 0.0024330681189894676, 7.798107981216162e-05, 0.0032701410818845034, 0.00034454051638022065, 4.791453989128058e-07, 6.196485173859401e-06, 1.3748945093539078e-05, 2.84065718005877e-05, 0.001966469921171665, 2.1033287112004473e-07, 0.00010553174797678366, 1.976671046577394e-05, 2.1773228553456647e-08, 2.573834717622958e-05, 0.00020047291764058173, 0.00020703890186268836, 0.0014593189116567373, 0.0004615795915015042, 0.0022446142975240946, 0.0], [0.024211131036281586, 6.481274795078207e-06, 2.169923209294211e-05, 6.860340818093391e-06, 1.7559172249548283e-07, 7.476812697859714e-07, 1.6459659946121974e-07, 8.167218084054184e-07, 3.2658936106599867e-06, 4.433819000837502e-08, 4.180863470537588e-06, 1.1945645610467182e-06, 5.290022997428423e-08, 1.3597278666566126e-05, 8.578776032663882e-05, 6.443652091547847e-05, 0.00014210992958396673, 0.00011602412996580824, 7.950893632369116e-05, 3.282578836660832e-05, 0.9748061895370483, 9.925834092427976e-06, 0.0001910763094201684, 1.2816779417335056e-05, 3.11582766698848e-07, 1.4153981737763388e-06, 3.4058076359144707e-09, 1.6056635132599695e-08, 4.780397944159631e-07, 6.947906139309623e-11, 8.898665271317441e-08, 1.3514681995729916e-07, 7.354765685363418e-10, 1.4603227782572503e-07, 1.0785911399580073e-06, 2.399856157353497e-06, 2.0492302610364277e-06, 9.499091538600624e-05, 4.574134072754532e-05, 4.0050548705039546e-05]]]}\n",
              "    )\n",
              "    </script>"
            ],
            "text/plain": [
              "<circuitsvis.utils.render.RenderedHTML at 0x29642bee0>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "induction_head_layer = 5\n",
        "induction_head_index = 5\n",
        "single_random_sequence = torch.randint(1000, 10000, (1, 20)).to(model.cfg.device)\n",
        "repeated_random_sequence = einops.repeat(single_random_sequence, \"batch seq_len -> batch (2 seq_len)\")\n",
        "def visualize_pattern_hook(\n",
        "    pattern: TT[\"batch\", \"head_index\", \"dest_pos\", \"source_pos\"],\n",
        "    hook: HookPoint,\n",
        "):\n",
        "    display(\n",
        "        cv.attention.attention_patterns(\n",
        "            tokens=model.to_str_tokens(repeated_random_sequence), \n",
        "            attention=pattern[0, induction_head_index, :, :][None, :, :] # Add a dummy axis, as CircuitsVis expects 3D patterns.\n",
        "        )\n",
        "    )\n",
        "\n",
        "model.run_with_hooks(\n",
        "    repeated_random_sequence, \n",
        "    return_type=None, \n",
        "    fwd_hooks=[(\n",
        "        utils.get_act_name(\"pattern\", induction_head_layer), \n",
        "        visualize_pattern_hook\n",
        "    )]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-SsMnuSt_xXX"
      },
      "source": [
        "## Available Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZciK_Hyv_xXX"
      },
      "source": [
        "TransformerLens comes with over 40 open source models available, all of which can be loaded into a consistent(-ish) architecture by just changing the name in `from_pretrained`. You can see [a table of the available models here](https://github.com/neelnanda-io/TransformerLens/blob/main/easy_transformer/model_properties_table.md), including the hyper-parameters and the alias used to load the model in TransformerLens.\n",
        "\n",
        "**Note:** Though TransformerLens can load in some large transformers (eg OPT-66B), it does not currently support loading a single model across multiple GPUs, so in practice loading a model (eg >7B parameters) will be impractical, depending on your GPU memory. Feel free to reach out if you need to use larger models for your research."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GPh5QZjV_xXX"
      },
      "source": [
        "\n",
        "Notably, this means that analysis can be near immediately re-run on a different model by just changing the name - to see this, let's load in DistilGPT-2 (a distilled version of GPT-2, with half as many layers) and copy the code from above to see the induction heads in that model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "09n_r0-j_xXX",
        "outputId": "21c7970d-9544-4456-dbe8-e86cfea3e95b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using pad_token, but it is not set yet.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded pretrained model distilgpt2 into HookedTransformer\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"6db30b86-bf16-4323-a6ef-625bf9a232bf\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6db30b86-bf16-4323-a6ef-625bf9a232bf\")) {                    Plotly.newPlot(                        \"6db30b86-bf16-4323-a6ef-625bf9a232bf\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.009252021089196205,0.00010355856647947803,0.011944303289055824,7.619292318850057e-06,0.0008440371020697057,6.9650145633204374e-06,0.008907302282750607,0.0014263407792896032,0.007963543757796288,0.0096383485943079,0.009068964049220085,0.015795370563864708],[0.0037399674765765667,0.017266293987631798,0.003409489756450057,0.0001793129777070135,0.013359448872506618,0.0023515443317592144,0.004183657933026552,0.01567782834172249,0.0019916375167667866,0.00013146440323907882,0.003953904379159212,0.014756210148334503],[0.007096054032444954,0.005524151958525181,0.01252596266567707,0.002623114036396146,0.019214419648051262,0.005478186998516321,0.008867339231073856,0.0012350687757134438,0.01723836548626423,0.012223217636346817,0.01990903727710247,1.5190675106691032e-11],[0.00960894301533699,0.22308360040187836,0.8548814654350281,0.016216255724430084,0.0183235052973032,0.0127740278840065,0.02021724358201027,0.2014797180891037,0.017071271315217018,0.017435332760214806,0.9370417594909668,0.5051765441894531],[0.2685699760913849,0.24233058094978333,0.08928290009498596,0.01064778957515955,0.08298038691282272,0.023900268599390984,0.6408357620239258,0.02561921626329422,0.062032025307416916,0.6530748009681702,0.016804581508040428,0.06585804373025894],[0.021970786154270172,0.08399306982755661,0.05955241620540619,0.013151219114661217,0.034707244485616684,0.1837030053138733,0.0791824609041214,0.1049342006444931,0.007668183650821447,0.4606159031391144,0.1815800666809082,0.02960473671555519]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0},\"title\":{\"text\":\"Induction Score by Head in Distil GPT-2\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6db30b86-bf16-4323-a6ef-625bf9a232bf');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "distilgpt2 = HookedTransformer.from_pretrained(\"distilgpt2\")\n",
        "# We make a tensor to store the induction score for each head. We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
        "distilgpt2_induction_score_store = torch.zeros((distilgpt2.cfg.n_layers, distilgpt2.cfg.n_heads), device=distilgpt2.cfg.device)\n",
        "def induction_score_hook(\n",
        "    pattern: TT[\"batch\", \"head_index\", \"dest_pos\", \"source_pos\"],\n",
        "    hook: HookPoint,\n",
        "):\n",
        "    # We take the diagonal of attention paid from each destination position to source positions seq_len-1 tokens back\n",
        "    # (This only has entries for tokens with index>=seq_len)\n",
        "    induction_stripe = pattern.diagonal(dim1=-2, dim2=-1, offset=1-seq_len)\n",
        "    # Get an average score per head\n",
        "    induction_score = einops.reduce(induction_stripe, \"batch head_index position -> head_index\", \"mean\")\n",
        "    # Store the result.\n",
        "    distilgpt2_induction_score_store[hook.layer(), :] = induction_score\n",
        "\n",
        "# We make a boolean filter on activation names, that's true only on attention pattern names.\n",
        "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
        "\n",
        "distilgpt2.run_with_hooks(\n",
        "    repeated_tokens, \n",
        "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
        "    fwd_hooks=[(\n",
        "        pattern_hook_names_filter,\n",
        "        induction_score_hook\n",
        "    )]\n",
        ")\n",
        "\n",
        "imshow(distilgpt2_induction_score_store, xaxis=\"Head\", yaxis=\"Layer\", title=\"Induction Score by Head in Distil GPT-2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m8xt1ic8_xXX"
      },
      "source": [
        "\n",
        "### An overview of the important open source models in the library\n",
        "\n",
        "* **GPT-2** - the classic generative pre-trained models from OpenAI\n",
        "    * Sizes Small (85M), Medium (300M), Large (700M) and XL (1.5B).\n",
        "    * Trained on ~22B tokens of internet text. ([Open source replication](https://huggingface.co/datasets/openwebtext))\n",
        "* **GPT-Neo** - Eleuther's replication of GPT-2\n",
        "    * Sizes 125M, 1.3B, 2.7B\n",
        "    * Trained on 300B(ish?) tokens of [the Pile](https://pile.eleuther.ai/) a large and diverse dataset including a bunch of code (and weird stuff)\n",
        "* **[OPT](https://ai.facebook.com/blog/democratizing-access-to-large-scale-language-models-with-opt-175b/)** - Meta AI's series of open source models\n",
        "    * Trained on 180B tokens of diverse text.\n",
        "    * 125M, 1.3B, 2.7B, 6.7B, 13B, 30B, 66B\n",
        "* **GPT-J** - Eleuther's 6B parameter model, trained on the Pile\n",
        "* **GPT-NeoX** - Eleuther's 20B parameter model, trained on the Pile\n",
        "* **Stanford CRFM models** - a replication of GPT-2 Small and GPT-2 Medium, trained on 5 different random seeds.\n",
        "    * Notably, 600 checkpoints were taken during training per model, and these are available in the library with eg `HookedTransformer.from_pretrained(\"stanford-gpt2-small-a\", checkpoint_index=265)`.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD0gcu1v_xXX"
      },
      "source": [
        "\n",
        "### An overview of some interpretability-friendly models I've trained and included\n",
        "\n",
        "(Feel free to [reach out](mailto:neelnanda27@gmail.com) if you want more details on any of these models)\n",
        "\n",
        "Each of these models has about ~200 checkpoints taken during training that can also be loaded from TransformerLens, with the `checkpoint_index` argument to `from_pretrained`.\n",
        "\n",
        "Note that all models are trained with a Beginning of Sequence token, and will likely break if given inputs without that! \n",
        "\n",
        "* **Toy Models**: Inspired by [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), I've trained 12 tiny language models, of 1-4L and each of width 512. I think that interpreting these is likely to be far more tractable than larger models, and both serve as good practice and will likely contain motifs and circuits that generalise to far larger models (like induction heads):\n",
        "    * Attention-Only models (ie without MLPs): attn-only-1l, attn-only-2l, attn-only-3l, attn-only-4l\n",
        "    * GELU models (ie with MLP, and the standard GELU activations): gelu-1l, gelu-2l, gelu-3l, gelu-4l\n",
        "    * SoLU models (ie with MLP, and [Anthropic's SoLU activation](https://transformer-circuits.pub/2022/solu/index.html), designed to make MLP neurons more interpretable): solu-1l, solu-2l, solu-3l, solu-4l\n",
        "    * All models are trained on 22B tokens of data, 80% from C4 (web text) and 20% from Python Code\n",
        "    * Models of the same layer size were trained with the same weight initialization and data shuffle, to more directly compare the effect of different activation functions.\n",
        "* **SoLU** models: A larger scan of models trained with [Anthropic's SoLU activation](https://transformer-circuits.pub/2022/solu/index.html), in the hopes that it makes the MLP neuron interpretability easier. \n",
        "    * A scan up to GPT-2 Medium size, trained on 30B tokens of the same data as toy models, 80% from C4 and 20% from Python code. \n",
        "        * solu-6l (40M), solu-8l (100M), solu-10l (200M), solu-12l (340M)\n",
        "    * An older scan up to GPT-2 Medium size, trained on 15B tokens of [the Pile](https://pile.eleuther.ai/)\n",
        "        * solu-1l-pile (13M), solu-2l-pile (13M), solu-4l-pile (13M), solu-6l-pile (40M), solu-8l-pile (100M), solu-10l-pile (200M), solu-12l-pile (340M)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIwo3zBf_xXX"
      },
      "source": [
        "## Other Resources:\n",
        "\n",
        "* [Concrete Open Problems in Mechanistic Interpretability](https://docs.google.com/document/d/1WONBzNqfKIxERejrrPlQMyKqg7jSFW92x5UMXNrMdPo/edit), a doc I wrote giving a long list of open problems in mechanistic interpretability, and thoughts on how to get started on trying to work on them. \n",
        "    * There's a lot of low-hanging fruit in the field, and I expect that many people reading this could use TransformerLens to usefully make progress on some of these!\n",
        "* Other demos:\n",
        "    * **[Exploratory Analysis Demo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/Exploratory_Analysis_Demo.ipynb)**, a demonstration of my standard toolkit for how to use TransformerLens to explore a mysterious behaviour in a language model.\n",
        "    * [Interpretability in the Wild](https://github.com/redwoodresearch/Easy-Transformer) a codebase from Arthur Conmy and Alex Variengien at Redwood research using this library to do a detailed and rigorous reverse engineering of the Indirect Object Identification circuit, to accompany their paper\n",
        "        * Note - this was based on an earlier version of this library, called EasyTransformer. It's pretty similar, but several breaking changes have been made since. \n",
        "    * A [recorded walkthrough](https://www.youtube.com/watch?v=yo4QvDn-vsU) of me doing research with TransformerLens on whether a tiny model can re-derive positional information, with [an accompanying Colab](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/main/No_Position_Experiment.ipynb)\n",
        "* [Neuroscope](https://neuroscope.io), a website showing the text in the dataset that most activates each neuron in some selected models. Good to explore to get a sense for what kind of features the model tends to represent, and as a \"wiki\" to get some info\n",
        "    * A tutorial on how to make an [Interactive Neuroscope](https://github.com/neelnanda-io/TransformerLens/blob/main/Hacky-Interactive-Lexoscope.ipynb), where you type in text and see the neuron activations over the text update live."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRINWKlB_xXX"
      },
      "source": [
        "## Transformer architecture\n",
        "\n",
        "HookedTransformer is a somewhat adapted GPT-2 architecture, but is computationally identical. The most significant changes are to the internal structure of the attention heads: \n",
        "* The weights (W_K, W_Q, W_V) mapping the residual stream to queries, keys and values are 3 separate matrices, rather than big concatenated one.\n",
        "* The weight matrices (W_K, W_Q, W_V, W_O) and activations (keys, queries, values, z (values mixed by attention pattern)) have separate head_index and d_head axes, rather than flattening them into one big axis.\n",
        "    * The activations all have shape `[batch, position, head_index, d_head]`\n",
        "    * W_K, W_Q, W_V have shape `[head_index, d_head, d_model]` and W_O has shape `[head_index, d_model, d_head]`\n",
        "\n",
        "The actual code is a bit of a mess, as there's a variety of Boolean flags to make it consistent with the various different model families in TransformerLens - to understand it and the internal structure, I instead recommend reading the code in [CleanTransformerDemo](https://colab.research.google.com/github/neelnanda-io/TransformerLens/blob/clean-transformer-demo/Clean_Transformer_Demo.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3k5y6BII_xXX"
      },
      "source": [
        "### Parameter Names\n",
        "\n",
        "Here is a list of the parameters and shapes in the model. By convention, all weight matrices multiply on the right (ie `new_activation = old_activation @ weights + bias`). \n",
        "\n",
        "Reminder of the key hyper-params:\n",
        "* `n_layers`: 12. The number of transformer blocks in the model (a block contains an attention layer and an MLP layer)\n",
        "* `n_heads`: 12. The number of attention heads per attention layer\n",
        "* `d_model`: 768. The residual stream width.\n",
        "* `d_head`: 64. The internal dimension of an attention head activation.\n",
        "* `d_mlp`: 3072. The internal dimension of the MLP layers (ie the number of neurons).\n",
        "* `d_vocab`: 50267. The number of tokens in the vocabulary.\n",
        "* `n_ctx`: 1024. The maximum number of tokens in an input prompt.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-5-8U5B2_xXX"
      },
      "source": [
        "**Transformer Block parameters:** \n",
        "Replace 0 with the relevant layer index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "7aMyoztl_xXX",
        "outputId": "ad54218b-53e7-4379-ee5d-e1c8f29faa20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "blocks.0.attn.W_Q torch.Size([12, 768, 64])\n",
            "blocks.0.attn.W_K torch.Size([12, 768, 64])\n",
            "blocks.0.attn.W_V torch.Size([12, 768, 64])\n",
            "blocks.0.attn.W_O torch.Size([12, 64, 768])\n",
            "blocks.0.attn.b_Q torch.Size([12, 64])\n",
            "blocks.0.attn.b_K torch.Size([12, 64])\n",
            "blocks.0.attn.b_V torch.Size([12, 64])\n",
            "blocks.0.attn.b_O torch.Size([768])\n",
            "blocks.0.mlp.W_in torch.Size([768, 3072])\n",
            "blocks.0.mlp.b_in torch.Size([3072])\n",
            "blocks.0.mlp.W_out torch.Size([3072, 768])\n",
            "blocks.0.mlp.b_out torch.Size([768])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if name.startswith(\"blocks.0.\"):\n",
        "        print(name, param.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ch6oejM_xXX"
      },
      "source": [
        "**Embedding & Unembedding parameters:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "IpJXpCjP_xXX",
        "outputId": "13101a01-6516-4256-feda-e9c8cdd21f42"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "embed.W_E torch.Size([50257, 768])\n",
            "pos_embed.W_pos torch.Size([1024, 768])\n",
            "unembed.W_U torch.Size([768, 50257])\n",
            "unembed.b_U torch.Size([50257])\n"
          ]
        }
      ],
      "source": [
        "for name, param in model.named_parameters():\n",
        "    if not name.startswith(\"blocks\"):\n",
        "        print(name, param.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PP8oR-fB_xXX"
      },
      "source": [
        "### Activation + Hook Names\n",
        "\n",
        "Lets get out a list of the activation/hook names in the model and their shapes. In practice, I recommend using the `utils.get_act_name` function to get the names, but this is a useful fallback, and necessary to eg write a name filter function.\n",
        "\n",
        "Let's do this by entering in a short, 10 token prompt, and add a hook function to each activations to print its name and shape. To avoid spam, let's just add this to activations in the first block or not in a block.\n",
        "\n",
        "Note 1: Each LayerNorm has a hook for the scale factor (ie the standard deviation of the input activations for each token position & batch element) and for the normalized output (ie the input activation with mean 0 and standard deviation 1, but *before* applying scaling or translating with learned weights). LayerNorm is applied every time a layer reads from the residual stream: `ln1` is the LayerNorm before the attention layer in a block, `ln2` the one before the MLP layer, and `ln_final` is the LayerNorm before the unembed. \n",
        "\n",
        "Note 2: *Every* activation apart from the attention pattern and attention scores has shape beginning with `[batch, position]`. The attention pattern and scores have shape `[batch, head_index, dest_position, source_position]` (the numbers are the same, unless we're using caching)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "KL4X4hfn_xXX",
        "outputId": "fbe41c4f-ef0e-47f6-f57b-5d42b3f0859e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num tokens: 1\n",
            "hook_embed torch.Size([1, 10, 768])\n",
            "hook_pos_embed torch.Size([1, 10, 768])\n",
            "blocks.0.hook_resid_pre torch.Size([1, 10, 768])\n",
            "blocks.0.ln1.hook_scale torch.Size([1, 10, 1])\n",
            "blocks.0.ln1.hook_normalized torch.Size([1, 10, 768])\n",
            "blocks.0.attn.hook_q torch.Size([1, 10, 12, 64])\n",
            "blocks.0.attn.hook_k torch.Size([1, 10, 12, 64])\n",
            "blocks.0.attn.hook_v torch.Size([1, 10, 12, 64])\n",
            "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 10, 10])\n",
            "blocks.0.attn.hook_pattern torch.Size([1, 12, 10, 10])\n",
            "blocks.0.attn.hook_z torch.Size([1, 10, 12, 64])\n",
            "blocks.0.hook_attn_out torch.Size([1, 10, 768])\n",
            "blocks.0.hook_resid_mid torch.Size([1, 10, 768])\n",
            "blocks.0.ln2.hook_scale torch.Size([1, 10, 1])\n",
            "blocks.0.ln2.hook_normalized torch.Size([1, 10, 768])\n",
            "blocks.0.mlp.hook_pre torch.Size([1, 10, 3072])\n",
            "blocks.0.mlp.hook_post torch.Size([1, 10, 3072])\n",
            "blocks.0.hook_mlp_out torch.Size([1, 10, 768])\n",
            "blocks.0.hook_resid_post torch.Size([1, 10, 768])\n",
            "ln_final.hook_scale torch.Size([1, 10, 1])\n",
            "ln_final.hook_normalized torch.Size([1, 10, 768])\n"
          ]
        }
      ],
      "source": [
        "test_prompt = \"The quick brown fox jumped over the lazy dog\"\n",
        "print(\"Num tokens:\", len(model.to_tokens(test_prompt)))\n",
        "\n",
        "def print_name_shape_hook_function(activation, hook):\n",
        "    print(hook.name, activation.shape)\n",
        "\n",
        "not_in_late_block_filter = lambda name: name.startswith(\"blocks.0.\") or not name.startswith(\"blocks\")\n",
        "\n",
        "model.run_with_hooks(\n",
        "    test_prompt,\n",
        "    return_type=None,\n",
        "    fwd_hooks=[(not_in_late_block_filter, print_name_shape_hook_function)],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "avspZ2Mi_xXX"
      },
      "source": [
        "### Folding LayerNorm (For the Curious)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4VxnaqEn_xXX"
      },
      "source": [
        "(For the curious - this is an important technical detail that's worth understanding, especially if you have preconceptions about how transformers work, but not necessary to use TransformerLens)\n",
        "\n",
        "LayerNorm is a normalization technique used by transformers, analogous to BatchNorm but more friendly to massive parallelisation. No one *really* knows why it works, but it seems to improve model numerical stability. Unlike BatchNorm, LayerNorm actually changes the functional form of the model, which makes it a massive pain for interpretability! \n",
        "\n",
        "Folding LayerNorm is a technique to make it lower overhead to deal with, and the flags `center_writing_weights` and `fold_ln` in `HookedTransformer.from_pretrained` apply this automatically (they default to True). These simplify the internal structure without changing the weights.\n",
        "\n",
        "Intuitively, LayerNorm acts on each residual stream vector (ie for each batch element and token position) independently, sets their mean to 0 (centering) and standard deviation to 1 (normalizing) (*across* the residual stream dimension - very weird!), and then applies a learned elementwise scaling and translation to each vector.\n",
        "\n",
        "Mathematically, centering is a linear map, normalizing is *not* a linear map, and scaling and translation are linear maps. \n",
        "* **Centering:** LayerNorm is applied every time a layer reads from the residual stream, so the mean of any residual stream vector can never matter - `center_writing_weights` set every weight matrix writing to the residual to have zero mean. \n",
        "* **Normalizing:** Normalizing is not a linear map, and cannot be factored out. The `hook_scale` hook point lets you access and control for this.\n",
        "* **Scaling and Translation:** Scaling and translation are linear maps, and are always followed by another linear map. The composition of two linear maps is another linear map, so we can *fold* the scaling and translation weights into the weights of the subsequent layer, and simplify things without changing the underlying computation. \n",
        "\n",
        "[See the docs for more details](https://github.com/neelnanda-io/TransformerLens/blob/main/further_comments.md#what-is-layernorm-folding-fold_ln)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bynrUOLq_xXY"
      },
      "source": [
        "A fun consequence of LayerNorm folding is that it creates a bias across the unembed, a `d_vocab` length vector that is added to the output logits - GPT-2 is not trained with this, but it *is* trained with a final LayerNorm that contains a bias. \n",
        "\n",
        "Turns out, this LayerNorm bias learns structure of the data that we can only see after folding! In particular, it essentially learns **unigram statistics** - rare tokens get suppressed, common tokens get boosted, by pretty dramatic degrees! Let's list the top and bottom 20 - at the top we see common punctuation and words like \" the\" and \" and\", at the bottom we see weird-ass tokens like \" RandomRedditor\":"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "ZxwLxVG5_xXY"
      },
      "outputs": [],
      "source": [
        "unembed_bias = model.unembed.b_U\n",
        "bias_values, bias_indices = unembed_bias.sort(descending=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "cECELGLn_xXY",
        "outputId": "cc086d3f-47a3-4af4-a32d-bd67ebec59c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 20 values\n",
            "7.03 ','\n",
            "6.98 ' the'\n",
            "6.68 ' and'\n",
            "6.49 '.'\n",
            "6.48 '\\n'\n",
            "6.47 ' a'\n",
            "6.41 ' in'\n",
            "6.25 ' to'\n",
            "6.16 ' of'\n",
            "6.04 '-'\n",
            "6.03 ' ('\n",
            "5.88 ' \"'\n",
            "5.80 ' for'\n",
            "5.72 ' that'\n",
            "5.64 ' on'\n",
            "5.59 ' is'\n",
            "5.52 ' as'\n",
            "5.49 ' at'\n",
            "5.45 ' with'\n",
            "5.44 ' or'\n",
            "...\n",
            "Bottom 20 values\n",
            "-3.82 ' '\n",
            "-3.83 '\\x18'\n",
            "-3.83 '\\x14'\n",
            "-3.83 ' RandomRedditor'\n",
            "-3.83 ''\n",
            "-3.83 ''\n",
            "-3.83 '\\x1b'\n",
            "-3.83 ''\n",
            "-3.83 '\\x05'\n",
            "-3.83 '\\x00'\n",
            "-3.83 '\\x06'\n",
            "-3.83 '\\x07'\n",
            "-3.83 '\\x0c'\n",
            "-3.83 '\\x02'\n",
            "-3.83 'oreAndOnline'\n",
            "-3.84 '\\x11'\n",
            "-3.84 ''\n",
            "-3.84 '\\x10'\n",
            "-3.84 ''\n",
            "-3.84 ''\n"
          ]
        }
      ],
      "source": [
        "top_k = 20\n",
        "print(f\"Top {top_k} values\")\n",
        "for i in range(top_k):\n",
        "    print(f\"{bias_values[i].item():.2f} {repr(model.to_string(bias_indices[i]))}\")\n",
        "\n",
        "print(\"...\")\n",
        "print(f\"Bottom {top_k} values\")\n",
        "for i in range(top_k, 0, -1):\n",
        "    print(f\"{bias_values[-i].item():.2f} {repr(model.to_string(bias_indices[-i]))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcJ753nd_xXY"
      },
      "source": [
        "This can have real consequences for interpretability - for example, this bias favours \" John\" over \" Mary\" by about 1.2, about 1/3 of the effect size of the Indirect Object Identification Circuit! All other things being the same, this makes the John token 3.6x times more likely than the Mary token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "p5cDCgRZ_xXY",
        "outputId": "2e16d8d1-2b0f-481b-9fba-736a75244759"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "John bias: 2.8995\n",
            "Mary bias: 1.6034\n",
            "Prob ratio bias: 3.6550x\n"
          ]
        }
      ],
      "source": [
        "john_bias = model.unembed.b_U[model.to_single_token(' John')]\n",
        "mary_bias = model.unembed.b_U[model.to_single_token(' Mary')]\n",
        "\n",
        "print(f\"John bias: {john_bias.item():.4f}\")\n",
        "print(f\"Mary bias: {mary_bias.item():.4f}\")\n",
        "print(f\"Prob ratio bias: {torch.exp(john_bias - mary_bias).item():.4f}x\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XARWckip_xXY"
      },
      "source": [
        "# Features\n",
        "\n",
        "An overview of some other important features of the library. I recommend checking out the [Exploratory Analysis Demo](https://colab.research.google.com/github/neelnanda-io/Easy-Transformer/blob/main/Exploratory_Analysis_Demo.ipynb) for some other important features not mentioned here, and for a demo of what using the library in practice looks like."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiaYmAbd_xXY"
      },
      "source": [
        "## Dealing with tokens\n",
        "\n",
        "**Tokenization** is one of the most annoying features of studying language models. We want language models to be able to take in arbitrary text as input, but the transformer architecture needs the inputs to be elements of a fixed, finite vocabulary. The solution to this is **tokens**, a fixed vocabulary of \"sub-words\", that any natural language can be broken down into with a **tokenizer**. This is invertible, and we can recover the original text, called **de-tokenization**. \n",
        "\n",
        "TransformerLens comes with a range of utility functions to deal with tokenization. Different models can have different tokenizers, so these are all methods on the model.\n",
        "\n",
        "get_token_position, to_tokens, to_string, to_str_tokens, prepend_bos, to_single_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duUiES2Y_xXY"
      },
      "source": [
        "The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\n",
        "\n",
        "Some observations - there are a lot of arbitrary-ish details in here!\n",
        "* The tokenizer splits on spaces, so no token contains two words.\n",
        "* Tokens include the preceding space, and whether the first token is a capital letter. `how` and ` how` are different tokens!\n",
        "* Common words are single tokens, even if fairly long (` paragraph`) while uncommon words are split into multiple tokens (` token|ized`).\n",
        "* Tokens *mostly* split on punctuation characters (eg `*` and `.`), but eg `'s` is a single token."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "DfgHjtK7_xXY",
        "outputId": "6f673065-53a7-48db-8617-c4fd83dc5b05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<|endoftext|>', 'The', ' first', ' thing', ' you', ' need', ' to', ' figure', ' out', ' is', ' *', 'how', '*', ' things', ' are', ' token', 'ized', '.', ' `', 'model', '.', 'to', '_', 'str', '_', 't', 'ok', 'ens', '`', ' splits', ' a', ' string', ' into', ' the', ' tokens', ' *', 'as', ' a', ' list', ' of', ' sub', 'strings', '*,', ' and', ' so', ' lets', ' you', ' explore', ' what', ' the', ' text', ' looks', ' like', '.', ' To', ' demonstrate', ' this', ',', ' let', \"'s\", ' use', ' it', ' on', ' this', ' paragraph', '.']\n"
          ]
        }
      ],
      "source": [
        "example_text = \"The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\"\n",
        "example_text_str_tokens = model.to_str_tokens(example_text)\n",
        "print(example_text_str_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mIDp5bDN_xXY"
      },
      "source": [
        "The transformer needs to take in a sequence of integers, not strings, so we need to convert these tokens into integers. `model.to_tokens` does this, and returns a tensor of integers on the model's device (shape `[batch, position]`). It maps a string to a batch of size 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "_5jzmB8j_xXY",
        "outputId": "48322746-35ff-4ce5-f208-444f1acaccd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[50256,   464,   717,  1517,   345,   761,   284,  3785,   503,   318,\n",
            "          1635,  4919,     9,  1243,   389, 11241,  1143,    13,  4600, 19849,\n",
            "            13,  1462,    62,  2536,    62,    83,   482,   641,    63, 30778,\n",
            "           257,  4731,   656,   262, 16326,  1635,   292,   257,  1351,   286,\n",
            "           850, 37336, 25666,   290,   523,  8781,   345,  7301,   644,   262,\n",
            "          2420,  3073,   588,    13,  1675, 10176,   428,    11,  1309,   338,\n",
            "           779,   340,   319,   428,  7322,    13]])\n"
          ]
        }
      ],
      "source": [
        "example_text_tokens = model.to_tokens(example_text)\n",
        "print(example_text_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMAgyGsF_xXY"
      },
      "source": [
        "`to_tokens` can also take in a list of strings, and return a batch of size `len(strings)`. If the strings are different numbers of tokens, it adds a PAD token to the end of the shorter strings to make them the same length.\n",
        "\n",
        "(Note: In GPT-2, 50256 signifies both the beginning of sequence, end of sequence and padding token - see the `prepend_bos` section for details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "ZNNa5CCv_xXY",
        "outputId": "0c2d518b-55e5-457f-e3cd-530abd0c5d20"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[50256,   464,  3797,  3332,   319,   262,  2603,    13, 50256, 50256],\n",
            "        [50256,   464,  3797,  3332,   319,   262,  2603,  1107,  1327,    13]])\n"
          ]
        }
      ],
      "source": [
        "example_multi_text = [\"The cat sat on the mat.\", \"The cat sat on the mat really hard.\"]\n",
        "example_multi_text_tokens = model.to_tokens(example_multi_text)\n",
        "print(example_multi_text_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dA-Vnawk_xXY"
      },
      "source": [
        "`model.to_single_token` is a convenience function that takes in a string corresponding to a *single* token and returns the corresponding integer. This is useful for eg looking up the logit corresponding to a single token. \n",
        "\n",
        "For example, let's input `The cat sat on the mat.` to GPT-2, and look at the log prob predicting that the next token is ` The`. \n",
        "\n",
        "<details><summary>Technical notes</summary>\n",
        "\n",
        "Note that if we input a string to the model, it's implicitly converted to a string with `to_tokens`. \n",
        "\n",
        "Note further that the log probs have shape `[batch, position, d_vocab]==[1, 8, 50257]`, with a vector of log probs predicting the next token for *every* token position. GPT-2 uses causal attention which means heads can only look backwards (equivalently, information can only move forwards in the model.), so the log probs at position k are only a function of the first k tokens, and it can't just cheat and look at the k+1 th token. This structure lets it generate text more efficiently, and lets it treat every *token* as a training example, rather than every *sequence*.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "S0RkoN_E_xXY",
        "outputId": "aeb7d289-403a-4914-f196-5440b44a957e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Probability tensor shape [batch, position, d_vocab] == torch.Size([1, 8, 50257])\n",
            "| The| probability: 11.98%\n"
          ]
        }
      ],
      "source": [
        "cat_text = \"The cat sat on the mat.\"\n",
        "cat_logits = model(cat_text)\n",
        "cat_probs = cat_logits.softmax(dim=-1)\n",
        "print(f\"Probability tensor shape [batch, position, d_vocab] == {cat_probs.shape}\")\n",
        "\n",
        "capital_the_token_index = model.to_single_token(\" The\")\n",
        "print(f\"| The| probability: {cat_probs[0, -1, capital_the_token_index].item():.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfAuC4Ak_xXY"
      },
      "source": [
        "`model.to_string` is the inverse of `to_tokens` and maps a tensor of integers to a string or list of strings. It also works on integers and lists of integers.\n",
        "\n",
        "For example, let's look up token 256 (due to technical details of tokenization, this will be the most common pair of ASCII characters!), and also verify that our tokens above map back to a string."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "pRJrKp0A_xXY",
        "outputId": "e8091c93-742e-44b8-80f8-3472b5584765"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token 256 - the most common pair of ASCII characters: | t|\n",
            "De-Tokenizing the example tokens: <|endoftext|>The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Token 256 - the most common pair of ASCII characters: |{model.to_string(256)}|\")\n",
        "# Squeeze means to remove dimensions of length 1. \n",
        "# Here, that removes the dummy batch dimension so it's a rank 1 tensor and returns a string\n",
        "# Rank 2 tensors map to a list of strings\n",
        "print(f\"De-Tokenizing the example tokens: {model.to_string(example_text_tokens.squeeze())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBTSnGJc_xXY"
      },
      "source": [
        "A related annoyance of tokenization is that it's hard to figure out how many tokens a string will break into. `model.get_token_position(single_token, tokens)` returns the position of `single_token` in `tokens`. `tokens` can be either a string or a tensor of tokens. \n",
        "\n",
        "Note that position is zero-indexed, it's two (ie third) because there's a beginning of sequence token automatically prepended (see the next section for details)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "ca9imu13_xXZ",
        "outputId": "5f76811d-b3e8-4c92-a686-830871325320"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "With BOS: 2\n",
            "Without BOS: 1\n"
          ]
        }
      ],
      "source": [
        "print(\"With BOS:\", model.get_token_position(\" cat\", \"The cat sat on the mat\"))\n",
        "print(\"Without BOS:\", model.get_token_position(\" cat\", \"The cat sat on the mat\", prepend_bos=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DJm81fB_xXZ"
      },
      "source": [
        "If there are multiple copies of the token, we can set `mode=\"first\"` to find the first occurence's position and `mode=\"last\"` to find the last"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "MTFDV-Yb_xXZ",
        "outputId": "bf5e7100-95e2-4597-fe29-daf0e8f3e3ba"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First occurence 2\n",
            "Final occurence 13\n"
          ]
        }
      ],
      "source": [
        "print(\"First occurence\", model.get_token_position(\n",
        "    \" cat\", \n",
        "    \"The cat sat on the mat. The mat sat on the cat.\", \n",
        "    mode=\"first\"))\n",
        "print(\"Final occurence\", model.get_token_position(\n",
        "    \" cat\", \n",
        "    \"The cat sat on the mat. The mat sat on the cat.\", \n",
        "    mode=\"last\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UG4CH5VD_xXZ"
      },
      "source": [
        "In general, tokenization is a pain, and full of gotchas. I highly recommend just playing around with different inputs and their tokenization and getting a feel for it. As another \"fun\" example, let's look at the tokenization of arithmetic expressions - tokens do *not* contain consistent numbers of digits. (This makes it even more impressive that GPT-3 can do arithmetic!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "id": "yX-sD6DR_xXZ",
        "outputId": "70b8b57e-27fb-4ea2-cf79-7eabdb7abdd1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['<|endoftext|>', '23', '42', '+', '2017', '=', '214', '45']\n",
            "['<|endoftext|>', '1000', '+', '1', '000000', '=', '9999', '99']\n"
          ]
        }
      ],
      "source": [
        "print(model.to_str_tokens(\"2342+2017=21445\"))\n",
        "print(model.to_str_tokens(\"1000+1000000=999999\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idxbg3M7_xXZ"
      },
      "source": [
        "I also *highly* recommend investigating prompts with easy tokenization when starting out - ideally key words should form a single token, be in the same position in different prompts, have the same total length, etc. Eg study Indirect Object Identification with common English names like ` Tim` rather than ` Ne|el`. Transformers need to spend some parameters in early layers converting multi-token words to a single feature, and then de-converting this in the late layers, and unless this is what you're explicitly investigating, this will make the behaviour you're investigating be messier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mz27Od8y_xXZ"
      },
      "source": [
        "### Gotcha: `prepend_bos`\n",
        "\n",
        "Key Takeaway: **If you get weird off-by-one errors, check whether there's an unexpected `prepend_bos`!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwq8Bnxb_xXZ"
      },
      "source": [
        "A weirdness you may have noticed in the above is that `to_tokens` and `to_str_tokens` added a weird `<|endoftext|>` to the start of each prompt. TransformerLens does this by default, and it can easily trip up new users. Notably, **this includes `model.forward`** (which is what's implicitly used when you do eg `model(\"Hello World\")`). This is called a **Beginning of Sequence (BOS)** token, and it's a special token used to mark the beginning of the sequence. Confusingly, in GPT-2, the End of Sequence (EOS), Beginning of Sequence (BOS) and Padding (PAD) tokens are all the same, `<|endoftext|>` with index `50256`.\n",
        "\n",
        "You can disable this behaviour by setting the flag `prepend_bos=False` in `to_tokens`, `to_str_tokens`, `model.forward` and any other function that converts strings to multi-token tensors. \n",
        "\n",
        "**Gotcha:** You only want to do this at the *start* of a prompt. If you, eg, want to input a question followed by an answer, and want to tokenize these separately, you do *not* want to prepend_bos on the answer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 98,
      "metadata": {
        "id": "qk-CWivp_xXZ",
        "outputId": "e00cd9ec-7ff5-4e32-db44-5ecd52efc21e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logits shape by default (with BOS) torch.Size([1, 3, 50257])\n",
            "Logits shape with BOS torch.Size([1, 3, 50257])\n",
            "Logits shape without BOS - only 2 positions! torch.Size([1, 2, 50257])\n"
          ]
        }
      ],
      "source": [
        "print(\"Logits shape by default (with BOS)\", model(\"Hello World\").shape)\n",
        "print(\"Logits shape with BOS\", model(\"Hello World\", prepend_bos=True).shape)\n",
        "print(\"Logits shape without BOS - only 2 positions!\", model(\"Hello World\", prepend_bos=False).shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zI20MIcZ_xXZ"
      },
      "source": [
        "`prepend_bos` is a bit of a hack, and I've gone back and forth on what the correct default here is. The reason I do this is that transformers tend to treat the first token weirdly - this doesn't really matter in training (where all inputs are >1000 tokens), but this can be a big issue when investigating short prompts! The reason for this is that attention patterns are a probability distribution and so need to add up to one, so to simulate being \"off\" they normally look at the first token. Giving them a BOS token lets the heads rest by looking at that, preserving the information in the first \"real\" token.\n",
        "\n",
        "Further, *some* models are trained to need a BOS token (OPT and my interpretability-friendly models are, GPT-2 and GPT-Neo are not). But despite GPT-2 not being trained with this, empirically it seems to make interpretability easier.\n",
        "\n",
        "For example, the model can get much worse at Indirect Object Identification without a BOS (and with a name as the first token):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 99,
      "metadata": {
        "id": "oDbI8Ao4_xXZ",
        "outputId": "1f37991f-3bee-4a28-d483-52283294162c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Logit difference with BOS: 6.754\n",
            "Logit difference without BOS: 2.782\n"
          ]
        }
      ],
      "source": [
        "ioi_logits_with_bos = model(\"Claire and Mary went to the shops, then Mary gave a bottle of milk to\", prepend_bos=True)\n",
        "mary_logit_with_bos = ioi_logits_with_bos[0, -1, model.to_single_token(\" Mary\")].item()\n",
        "claire_logit_with_bos = ioi_logits_with_bos[0, -1, model.to_single_token(\" Claire\")].item()\n",
        "print(f\"Logit difference with BOS: {(claire_logit_with_bos - mary_logit_with_bos):.3f}\")\n",
        "\n",
        "ioi_logits_without_bos = model(\"Claire and Mary went to the shops, then Mary gave a bottle of milk to\", prepend_bos=False)\n",
        "mary_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(\" Mary\")].item()\n",
        "claire_logit_without_bos = ioi_logits_without_bos[0, -1, model.to_single_token(\" Claire\")].item()\n",
        "print(f\"Logit difference without BOS: {(claire_logit_without_bos - mary_logit_without_bos):.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfYqF1Z-_xXZ"
      },
      "source": [
        "Though, note that this also illustrates another gotcha - when `Claire` is at the start of a sentence (no preceding space), it's actually *two* tokens, not one, which probably confuses the relevant circuit. (Note - in this test we put `prepend_bos=False`, because we want to analyse the tokenization of a specific string, not to give an input to the model!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 100,
      "metadata": {
        "id": "RRznvZjV_xXZ",
        "outputId": "a6c411c1-8286-4ba1-cde8-96f22d7969f0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "| Claire| -> [' Claire']\n",
            "|Claire| -> ['Cl', 'aire']\n"
          ]
        }
      ],
      "source": [
        "print(f\"| Claire| -> {model.to_str_tokens(' Claire', prepend_bos=False)}\")\n",
        "print(f\"|Claire| -> {model.to_str_tokens('Claire', prepend_bos=False)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYYYpjX3_xXZ"
      },
      "source": [
        "## Factored Matrix Class\n",
        "\n",
        "In transformer interpretability, we often need to analyse low rank factorized matrices - a matrix $M = AB$, where M is `[large, large]`, but A is `[large, small]` and B is `[small, large]`. This is a common structure in transformers, and the `FactoredMatrix` class is a convenient way to work with these. It implements efficient algorithms for various operations on these, such as computing the trace, eigenvalues, Frobenius norm, singular value decomposition, and products with other matrices. It can (approximately) act as a drop-in replacement for the original matrix, and supports leading batch dimensions to the factored matrix. \n",
        "\n",
        "<details><summary>Why are low-rank factorized matrices useful for transformer interpretability?</summary>\n",
        "\n",
        "As argued in [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html), an unexpected fact about transformer attention heads is that rather than being best understood as keys, queries and values (and the requisite weight matrices), they're actually best understood as two low rank factorized matrices. \n",
        "* **Where to move information from:** $W_QK = W_Q W_K^T$, used for determining the attention pattern - what source positions to move information from and what destination positions to move them to.\n",
        "    * Intuitively, residual stream -> query and residual stream -> key are linear maps, *and* `attention_score = query @ key.T` is a linear map, so the whole thing can be factored into one big bilinear form `residual @ W_QK @ residual.T`\n",
        "* **What information to move:** $W_OV = W_V W_O$, used to determine what information to copy from the source position to the destination position (weighted by the attention pattern weight from that destination to that source). \n",
        "    * Intuitively, the residual stream is a `[position, d_model]` tensor (ignoring batch). The attention pattern acts on the *position* dimension (where to move information from and to) and the value and output weights act on the *d_model* dimension - ie *what* information is contained at that source position. So we can factor it all into `attention_pattern @ residual @ W_V @ W_O`, and so only need to care about `W_OV = W_V @ W_O`\n",
        "* Note - the internal head dimension is smaller than the residual stream dimension, so the factorization is low rank. (here, `d_model=768` and `d_head=64`)\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7ZaV_dw_xXZ"
      },
      "source": [
        "### Basic Examples"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-4PtEkb_xXZ"
      },
      "source": [
        "We can use the basic class directly - let's make a factored matrix directly and look at the basic operations:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 101,
      "metadata": {
        "id": "ZU7OI2_S_xXZ",
        "outputId": "a6d3b702-6d86-49fd-f8da-aa252ff486b7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Norms:\n",
            "tensor(6.5155)\n",
            "tensor(6.5155)\n",
            "Right dimension: 5, Left dimension: 5, Hidden dimension: 2\n"
          ]
        }
      ],
      "source": [
        "A = torch.randn(5, 2)\n",
        "B = torch.randn(2, 5)\n",
        "AB = A @ B\n",
        "AB_factor = FactoredMatrix(A, B)\n",
        "print(\"Norms:\")\n",
        "print(AB.norm())\n",
        "print(AB_factor.norm())\n",
        "\n",
        "print(f\"Right dimension: {AB_factor.rdim}, Left dimension: {AB_factor.ldim}, Hidden dimension: {AB_factor.mdim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MY-NQDgO_xXZ"
      },
      "source": [
        "We can also look at the eigenvalues and singular values of the matrix. Note that, because the matrix is rank 2 but 5 by 5, the final 3 eigenvalues and singular values are zero - the factored class omits the zeros."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {
        "id": "HO4Hr2Im_xXa",
        "outputId": "0b79c1f8-33b2-44cb-dd4d-829259a81717"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Eigenvalues:\n",
            "tensor([-5.3892e+00+0.j, -1.0417e-08+0.j,  6.5417e-01+0.j,  1.6648e-07+0.j,\n",
            "        -1.8781e-08+0.j])\n",
            "tensor([ 0.6542+0.j, -5.3892+0.j])\n",
            "\n",
            "Singular Values:\n",
            "tensor([6.3607e+00, 1.4120e+00, 2.3244e-07, 5.7410e-08, 1.1489e-08])\n",
            "tensor([6.3607, 1.4120])\n"
          ]
        }
      ],
      "source": [
        "print(\"Eigenvalues:\")\n",
        "print(torch.linalg.eig(AB).eigenvalues)\n",
        "print(AB_factor.eigenvalues)\n",
        "print()\n",
        "print(\"Singular Values:\")\n",
        "print(torch.linalg.svd(AB).S)\n",
        "print(AB_factor.S)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rg08kCSj_xXa"
      },
      "source": [
        "We can multiply with other matrices - it automatically chooses the smallest possible dimension to factor along (here it's 2, rather than 5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "1mmTMe-C_xXa",
        "outputId": "4cba392d-3efd-4eeb-a8a4-7b829dacccaf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unfactored: torch.Size([5, 300]) tensor(115.3525)\n",
            "Factored: torch.Size([5, 300]) tensor(115.3525)\n",
            "Right dimension: 300, Left dimension: 5, Hidden dimension: 2\n"
          ]
        }
      ],
      "source": [
        "C = torch.randn(5, 300)\n",
        "ABC = AB @ C\n",
        "ABC_factor = AB_factor @ C\n",
        "print(\"Unfactored:\", ABC.shape, ABC.norm())\n",
        "print(\"Factored:\", ABC_factor.shape, ABC_factor.norm())\n",
        "print(f\"Right dimension: {ABC_factor.rdim}, Left dimension: {ABC_factor.ldim}, Hidden dimension: {ABC_factor.mdim}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bya5YOas_xXa"
      },
      "source": [
        "If we want to collapse this back to an unfactored matrix, we can use the AB property to get the product:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 104,
      "metadata": {
        "id": "gdeLDI6z_xXa",
        "outputId": "20b366e9-0c75-41f6-c949-bb1271ff9a6c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(True)\n"
          ]
        }
      ],
      "source": [
        "AB_unfactored = AB_factor.AB\n",
        "print(torch.isclose(AB_unfactored, AB).all())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zo3mwvb6_xXa"
      },
      "source": [
        "### Medium Example: Eigenvalue Copying Scores\n",
        "\n",
        "(This is a more involved example of how to use the factored matrix class, skip it if you aren't following)\n",
        "\n",
        "For a more involved example, let's look at the eigenvalue copying score from [A Mathematical Framework](https://transformer-circuits.pub/2021/framework/index.html) of the OV circuit for various heads. The OV Circuit for a head (the factorised matrix $W_OV = W_V W_O$) is a linear map that determines what information is moved from the source position to the destination position. Because this is low rank, it can be thought of as *reading in* some low rank subspace of the source residual stream and *writing to* some low rank subspace of the destination residual stream (with maybe some processing happening in the middle).\n",
        "\n",
        "A common operation for this will just be to *copy*, ie to have the same reading and writing subspace, and to do minimal processing in the middle. Empirically, this tends to coincide with the OV Circuit having (approximately) positive real eigenvalues. I mostly assert this as an empirical fact, but intuitively, operations that involve mapping eigenvectors to different directions (eg rotations) tend to have complex eigenvalues. And operations that preserve eigenvector direction but negate it tend to have negative real eigenvalues. And \"what happens to the eigenvectors\" is a decent proxy for what happens to an arbitrary vector.\n",
        "\n",
        "We can get a score for \"how positive real the OV circuit eigenvalues are\" with $\\frac{\\sum \\lambda_i}{\\sum |\\lambda_i|}$, where $\\lambda_i$ are the eigenvalues of the OV circuit. This is a bit of a hack, but it seems to work well in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agk7i2Dl_xXa"
      },
      "source": [
        "Let's use FactoredMatrix to compute this for every head in the model! We use the helper `model.OV` to get the concatenated OV circuits for all heads across all layers in the model. This has the shape `[n_layers, n_heads, d_model, d_model]`, where `n_layers` and `n_heads` are batch dimensions and the final two dimensions are factorised as `[n_layers, n_heads, d_model, d_head]` and `[n_layers, n_heads, d_head, d_model]` matrices.\n",
        "\n",
        "We can then get the eigenvalues for this, where there are separate eigenvalues for each element of the batch (a `[n_layers, n_heads, d_head]` tensor of complex numbers), and calculate the copying score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 105,
      "metadata": {
        "id": "aw_hlVmO_xXa",
        "outputId": "82a88275-6e29-4aac-af53-2822da650182"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FactoredMatrix: Shape(torch.Size([12, 12, 768, 768])), Hidden Dim(64)\n"
          ]
        }
      ],
      "source": [
        "OV_circuit_all_heads = model.OV\n",
        "print(OV_circuit_all_heads)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "O0-ky1bE_xXa",
        "outputId": "c27c4484-c7ce-4f4a-ee5d-e6f70caaf928"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12, 12, 64])\n",
            "torch.complex64\n"
          ]
        }
      ],
      "source": [
        "OV_circuit_all_heads_eigenvalues = OV_circuit_all_heads.eigenvalues \n",
        "print(OV_circuit_all_heads_eigenvalues.shape)\n",
        "print(OV_circuit_all_heads_eigenvalues.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "01l6DNC8_xXa",
        "outputId": "37b1d36c-f116-4c85-ffc9-c1dc4aefb6bc"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"3d2ed137-7532-46f7-b5bf-8b865dfdf45f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3d2ed137-7532-46f7-b5bf-8b865dfdf45f\")) {                    Plotly.newPlot(                        \"3d2ed137-7532-46f7-b5bf-8b865dfdf45f\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.7775009870529175,0.35272666811943054,0.2596185505390167,0.667025625705719,0.8384258151054382,0.5584431290626526,0.8444746136665344,0.4137907922267914,0.2448895424604416,0.028157558292150497,0.35840970277786255,0.1628827601671219],[-0.4541912078857422,-0.6529326438903809,-0.548457145690918,-0.7990368604660034,-0.7736424207687378,-0.8522582054138184,0.9774323105812073,0.662625253200531,-0.7303222417831421,-0.7007020711898804,-0.6946624517440796,-0.9996722340583801],[-0.7837162017822266,0.8967756628990173,0.47509536147117615,-0.6671971678733826,0.7881460785865784,-0.8547748327255249,-0.9054181575775146,-0.574938178062439,-0.32175105810165405,-0.028594080358743668,-0.9247617721557617,-0.9699268341064453],[0.5864037275314331,-0.76143479347229,0.5971695184707642,0.7854392528533936,-0.8788883686065674,0.39087456464767456,0.044738586992025375,0.1102805808186531,-0.8169987201690674,0.2212953269481659,-0.9939578771591187,0.577440083026886],[0.5254793167114258,0.30490124225616455,-0.107291579246521,0.9433152675628662,-0.9314428567886353,0.5273630619049072,-0.42647096514701843,-0.9984429478645325,0.5296756625175476,0.8604294061660767,-0.88950514793396,0.9556970000267029],[0.6629186272621155,0.429569810628891,0.9736858010292053,0.6555481553077698,0.12201868742704391,0.7442768812179565,0.5037953853607178,0.9525357484817505,-0.6507166624069214,-0.9316279292106628,0.9791511297225952,-0.9972584843635559],[0.9613032937049866,0.7501777410507202,-0.3806661069393158,0.6429786682128906,0.9557769298553467,-0.9428837895393372,-0.9948078989982605,0.7852988243103027,0.9657301902770996,0.7073014378547668,0.3687227964401245,0.8128010034561157],[0.9659483432769775,0.9730119705200195,0.3190060257911682,-0.3029051125049591,0.9790951609611511,0.9357923269271851,-0.5550316572189331,-0.005466540809720755,0.9867777228355408,0.8249565958976746,0.5664295554161072,0.10005273669958115],[-0.9464486241340637,-0.2547200918197632,0.6522325873374939,0.14152537286281586,0.9884140491485596,0.9860583543777466,0.6949269771575928,0.9901810884475708,0.9791203141212463,-0.2359551340341568,-0.982071042060852,0.6506689190864563],[0.9895942807197571,-0.2917816638946533,0.9714025259017944,0.9951602220535278,0.18783794343471527,-0.9460936784744263,0.4780190885066986,-0.2489192634820938,0.9437097907066345,0.1186617910861969,0.9941242933273315,-0.3808818757534027],[0.9564487338066101,0.5542723536491394,0.42118075489997864,0.6628789901733398,0.8659594655036926,0.9937118887901306,0.9069077372550964,0.39811062812805176,-0.41342177987098694,0.9971913695335388,0.3459666669368744,0.9938657283782959],[0.589126706123352,0.9313737750053406,0.9268401265144348,0.9993563890457153,0.6227540969848633,0.8463947176933289,0.6584346294403076,0.8423123955726624,0.29784950613975525,0.8728677034378052,0.996314525604248,0.9867525696754456]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0,\"cmin\":-1.0,\"cmax\":1.0},\"title\":{\"text\":\"OV Copying Score for each head in GPT-2 Small\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3d2ed137-7532-46f7-b5bf-8b865dfdf45f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "OV_copying_score = OV_circuit_all_heads_eigenvalues.sum(dim=-1).real / OV_circuit_all_heads_eigenvalues.abs().sum(dim=-1)\n",
        "imshow(utils.to_numpy(OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head in GPT-2 Small\", zmax=1.0, zmin=-1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WwE9Mvy_xXa"
      },
      "source": [
        "Head 11 in Layer 11 (L11H11) has a high copying score, and if we plot the eigenvalues they look approximately as expected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "ot4Ckhim_xXa",
        "outputId": "ac208555-06e6-4049-b66c-438a98bdeb44"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"49d01d3e-f3ab-4669-a4d4-16944acd3829\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"49d01d3e-f3ab-4669-a4d4-16944acd3829\")) {                    Plotly.newPlot(                        \"49d01d3e-f3ab-4669-a4d4-16944acd3829\",                        [{\"hovertemplate\":\"Real=%{x}<br>Imaginary=%{y}<extra></extra>\",\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[-2.1397268772125244,1.41526460647583,3.444453239440918,4.027667999267578,8.882658004760742,8.477514266967773,8.216815948486328,8.216815948486328,4.866767883300781,4.866767883300781,4.843701362609863,4.843701362609863,7.855456352233887,7.855456352233887,7.769120216369629,7.769120216369629,5.078594207763672,7.042275428771973,7.042275428771973,5.365773677825928,5.365773677825928,7.678597450256348,7.678597450256348,5.563434600830078,5.563434600830078,5.421733379364014,7.672958850860596,6.573324203491211,6.573324203491211,7.172203063964844,7.172203063964844,5.675152778625488,5.675152778625488,7.423608779907227,7.423608779907227,7.470805644989014,6.089090824127197,6.089090824127197,6.306833744049072,6.306833744049072,6.511753082275391,6.511753082275391,5.9552459716796875,5.9552459716796875,5.8588080406188965,5.8588080406188965,7.147883415222168,7.147883415222168,7.18571662902832,7.18571662902832,6.670607566833496,6.670607566833496,6.73598575592041,6.73598575592041,6.149767875671387,6.149767875671387,6.288788318634033,6.288788318634033,6.34478759765625,6.625565052032471,6.625565052032471,6.899182319641113,6.899182319641113,6.856420516967773],\"xaxis\":\"x\",\"y\":[0.0,0.0,0.0,0.0,0.0,0.0,0.4086860716342926,-0.4086860716342926,0.41852322220802307,-0.41852322220802307,0.09079320728778839,-0.09079320728778839,0.700724184513092,-0.700724184513092,0.47056105732917786,-0.47056105732917786,0.0,1.0298689603805542,-1.0298689603805542,0.46421313285827637,-0.46421313285827637,0.33565443754196167,-0.33565443754196167,0.5558289885520935,-0.5558289885520935,0.0,0.0,0.9988726377487183,-0.9988726377487183,0.753179669380188,-0.753179669380188,0.48253458738327026,-0.48253458738327026,0.4257570803165436,-0.4257570803165436,0.0,0.6436289548873901,-0.6436289548873901,0.7701691389083862,-0.7701691389083862,0.7558003664016724,-0.7558003664016724,0.2591104805469513,-0.2591104805469513,0.013038340955972672,-0.013038340955972672,0.40167000889778137,-0.40167000889778137,0.2819214463233948,-0.2819214463233948,0.6146222949028015,-0.6146222949028015,0.5391287207603455,-0.5391287207603455,0.28233790397644043,-0.28233790397644043,0.35283640027046204,-0.35283640027046204,0.0,0.24868419766426086,-0.24868419766426086,0.1554555892944336,-0.1554555892944336,0.0],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Real\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Imaginary\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Eigenvalues of Head L11H11 of GPT-2 Small\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('49d01d3e-f3ab-4669-a4d4-16944acd3829');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "scatter(x=OV_circuit_all_heads_eigenvalues[-1, -1, :].real, y=OV_circuit_all_heads_eigenvalues[-1, -1, :].imag, title=\"Eigenvalues of Head L11H11 of GPT-2 Small\", xaxis=\"Real\", yaxis=\"Imaginary\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gV0TM-ai_xXa"
      },
      "source": [
        "We can even look at the full OV circuit, from the input tokens to output tokens: $W_E W_V W_O W_U$. This is a `[d_vocab, d_vocab]==[50257, 50257]` matrix, so absolutely enormous, even for a single head. But with the FactoredMatrix class, we can compute the full eigenvalue copying score of every head in a few seconds."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "66LRDpuo_xXa",
        "outputId": "539f2a2a-4be1-409a-9368-6ce60046cb49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FactoredMatrix: Shape(torch.Size([12, 12, 50257, 50257])), Hidden Dim(64)\n"
          ]
        }
      ],
      "source": [
        "full_OV_circuit = model.embed.W_E @ OV_circuit_all_heads @ model.unembed.W_U\n",
        "print(full_OV_circuit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {
        "id": "E27IvUwx_xXa",
        "outputId": "84ac4434-36c9-4df6-c5b9-ac2e69b09c08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([12, 12, 64])\n",
            "torch.complex64\n"
          ]
        }
      ],
      "source": [
        "full_OV_circuit_eigenvalues = full_OV_circuit.eigenvalues\n",
        "print(full_OV_circuit_eigenvalues.shape)\n",
        "print(full_OV_circuit_eigenvalues.dtype)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "RZljdTEM_xXa",
        "outputId": "50cb2e1e-b42e-43e2-b7f8-423ac839e9be"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"3290e406-9b62-46f9-98c8-8feac3583a2f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"3290e406-9b62-46f9-98c8-8feac3583a2f\")) {                    Plotly.newPlot(                        \"3290e406-9b62-46f9-98c8-8feac3583a2f\",                        [{\"coloraxis\":\"coloraxis\",\"name\":\"0\",\"z\":[[0.8356362581253052,0.5853539109230042,0.5105839967727661,0.7843374609947205,0.8644155263900757,0.7026583552360535,0.8969926238059998,0.5868821740150452,0.4248647093772888,-0.16337519884109497,0.4626849591732025,0.27605414390563965],[-0.05292024090886116,-0.3177311420440674,-0.4810578227043152,-0.7838065028190613,-0.6360207200050354,-0.7758684158325195,0.9681803584098816,0.8119112849235535,-0.7510464191436768,-0.6878446340560913,-0.6429882645606995,-0.9985854625701904],[-0.659832775592804,0.915250301361084,0.5461496710777283,-0.4874395728111267,0.7720562219619751,-0.7541061043739319,-0.8472450375556946,-0.694898247718811,-0.15575101971626282,0.24442270398139954,-0.9106622934341431,-0.9439151287078857],[0.6486897468566895,-0.5592911839485168,0.5935593843460083,0.7843034863471985,-0.8150344491004944,0.6130046248435974,0.16785871982574463,0.3519590497016907,-0.6837261915206909,0.22237664461135864,-0.9929219484329224,0.6535818576812744],[0.5740951895713806,0.36401331424713135,0.09609035402536392,0.9359624981880188,-0.9228775501251221,0.619107723236084,-0.33572664856910706,-0.998464822769165,0.6448630690574646,0.8468658924102783,-0.7557654976844788,0.9527968764305115],[0.7326542139053345,0.532416820526123,0.9732670187950134,0.7239247560501099,0.2553896903991699,0.8158413171768188,0.6655787825584412,0.9287099838256836,-0.5660437345504761,-0.8908745050430298,0.9834232926368713,-0.998117983341217],[0.9698693156242371,0.7439671158790588,-0.35639357566833496,0.6022986173629761,0.9708117246627808,-0.9278276562690735,-0.9962316155433655,0.8345207571983337,0.9714326858520508,0.8158542513847351,0.5902576446533203,0.8199344873428345],[0.9820227026939392,0.9859328269958496,0.5152459144592285,-0.5610515475273132,0.9663665294647217,0.9495158791542053,-0.5204811096191406,0.31047508120536804,0.9859085083007812,0.7797457575798035,0.6738533973693848,0.39197400212287903],[-0.9062040448188782,0.11750978976488113,0.8077874779701233,0.41693049669265747,0.9829015135765076,0.9902303218841553,0.7847104668617249,0.9945629835128784,0.9868025779724121,-0.268044114112854,-0.9908866882324219,0.745792806148529],[0.9906190037727356,-0.1823112666606903,0.9757837057113647,0.9986750483512878,0.254433274269104,-0.9544061422348022,0.5869240760803223,-0.23538005352020264,0.9550504088401794,0.2551196813583374,0.9929869771003723,0.09052585065364838],[0.9707273244857788,0.6956093311309814,0.628002405166626,0.790286660194397,0.9343839883804321,0.9895793795585632,0.9436281323432922,-0.10835029184818268,-0.34311148524284363,0.9986709356307983,0.5086733102798462,0.9949509501457214],[0.828312873840332,0.9432438015937805,0.9491767883300781,0.9995352029800415,0.5712308287620544,0.8055234551429749,0.6781863570213318,0.827256977558136,0.8314761519432068,0.8778651356697083,0.9944959282875061,0.9973866939544678]],\"type\":\"heatmap\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"Head: %{x}<br>Layer: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"scaleanchor\":\"y\",\"constrain\":\"domain\",\"title\":{\"text\":\"Head\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"autorange\":\"reversed\",\"constrain\":\"domain\",\"title\":{\"text\":\"Layer\"}},\"coloraxis\":{\"colorscale\":[[0.0,\"rgb(103,0,31)\"],[0.1,\"rgb(178,24,43)\"],[0.2,\"rgb(214,96,77)\"],[0.3,\"rgb(244,165,130)\"],[0.4,\"rgb(253,219,199)\"],[0.5,\"rgb(247,247,247)\"],[0.6,\"rgb(209,229,240)\"],[0.7,\"rgb(146,197,222)\"],[0.8,\"rgb(67,147,195)\"],[0.9,\"rgb(33,102,172)\"],[1.0,\"rgb(5,48,97)\"]],\"cmid\":0.0,\"cmin\":-1.0,\"cmax\":1.0},\"title\":{\"text\":\"OV Copying Score for each head in GPT-2 Small\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('3290e406-9b62-46f9-98c8-8feac3583a2f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "full_OV_copying_score = full_OV_circuit_eigenvalues.sum(dim=-1).real / full_OV_circuit_eigenvalues.abs().sum(dim=-1)\n",
        "imshow(utils.to_numpy(full_OV_copying_score), xaxis=\"Head\", yaxis=\"Layer\", title=\"OV Copying Score for each head in GPT-2 Small\", zmax=1.0, zmin=-1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRmIqWNw_xXa"
      },
      "source": [
        "Interestingly, these are highly (but not perfectly!) correlated. I'm not sure what to read from this, or what's up with the weird outlier heads!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "w1BwWik0_xXa",
        "outputId": "c6318326-1258-46d8-b340-2277ee386f9e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"fbf561b3-3493-40ac-b396-0f320d1752a8\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"fbf561b3-3493-40ac-b396-0f320d1752a8\")) {                    Plotly.newPlot(                        \"fbf561b3-3493-40ac-b396-0f320d1752a8\",                        [{\"hovertemplate\":\"<b>%{hovertext}</b><br><br>Full OV Copying Score=%{x}<br>OV Copying Score=%{y}<extra></extra>\",\"hovertext\":[\"L0H0\",\"L0H1\",\"L0H2\",\"L0H3\",\"L0H4\",\"L0H5\",\"L0H6\",\"L0H7\",\"L0H8\",\"L0H9\",\"L0H10\",\"L0H11\",\"L1H0\",\"L1H1\",\"L1H2\",\"L1H3\",\"L1H4\",\"L1H5\",\"L1H6\",\"L1H7\",\"L1H8\",\"L1H9\",\"L1H10\",\"L1H11\",\"L2H0\",\"L2H1\",\"L2H2\",\"L2H3\",\"L2H4\",\"L2H5\",\"L2H6\",\"L2H7\",\"L2H8\",\"L2H9\",\"L2H10\",\"L2H11\",\"L3H0\",\"L3H1\",\"L3H2\",\"L3H3\",\"L3H4\",\"L3H5\",\"L3H6\",\"L3H7\",\"L3H8\",\"L3H9\",\"L3H10\",\"L3H11\",\"L4H0\",\"L4H1\",\"L4H2\",\"L4H3\",\"L4H4\",\"L4H5\",\"L4H6\",\"L4H7\",\"L4H8\",\"L4H9\",\"L4H10\",\"L4H11\",\"L5H0\",\"L5H1\",\"L5H2\",\"L5H3\",\"L5H4\",\"L5H5\",\"L5H6\",\"L5H7\",\"L5H8\",\"L5H9\",\"L5H10\",\"L5H11\",\"L6H0\",\"L6H1\",\"L6H2\",\"L6H3\",\"L6H4\",\"L6H5\",\"L6H6\",\"L6H7\",\"L6H8\",\"L6H9\",\"L6H10\",\"L6H11\",\"L7H0\",\"L7H1\",\"L7H2\",\"L7H3\",\"L7H4\",\"L7H5\",\"L7H6\",\"L7H7\",\"L7H8\",\"L7H9\",\"L7H10\",\"L7H11\",\"L8H0\",\"L8H1\",\"L8H2\",\"L8H3\",\"L8H4\",\"L8H5\",\"L8H6\",\"L8H7\",\"L8H8\",\"L8H9\",\"L8H10\",\"L8H11\",\"L9H0\",\"L9H1\",\"L9H2\",\"L9H3\",\"L9H4\",\"L9H5\",\"L9H6\",\"L9H7\",\"L9H8\",\"L9H9\",\"L9H10\",\"L9H11\",\"L10H0\",\"L10H1\",\"L10H2\",\"L10H3\",\"L10H4\",\"L10H5\",\"L10H6\",\"L10H7\",\"L10H8\",\"L10H9\",\"L10H10\",\"L10H11\",\"L11H0\",\"L11H1\",\"L11H2\",\"L11H3\",\"L11H4\",\"L11H5\",\"L11H6\",\"L11H7\",\"L11H8\",\"L11H9\",\"L11H10\",\"L11H11\"],\"legendgroup\":\"\",\"marker\":{\"color\":\"#636efa\",\"symbol\":\"circle\"},\"mode\":\"markers\",\"name\":\"\",\"orientation\":\"v\",\"showlegend\":false,\"x\":[0.8356362581253052,0.5853539109230042,0.5105839967727661,0.7843374609947205,0.8644155263900757,0.7026583552360535,0.8969926238059998,0.5868821740150452,0.4248647093772888,-0.16337519884109497,0.4626849591732025,0.27605414390563965,-0.05292024090886116,-0.3177311420440674,-0.4810578227043152,-0.7838065028190613,-0.6360207200050354,-0.7758684158325195,0.9681803584098816,0.8119112849235535,-0.7510464191436768,-0.6878446340560913,-0.6429882645606995,-0.9985854625701904,-0.659832775592804,0.915250301361084,0.5461496710777283,-0.4874395728111267,0.7720562219619751,-0.7541061043739319,-0.8472450375556946,-0.694898247718811,-0.15575101971626282,0.24442270398139954,-0.9106622934341431,-0.9439151287078857,0.6486897468566895,-0.5592911839485168,0.5935593843460083,0.7843034863471985,-0.8150344491004944,0.6130046248435974,0.16785871982574463,0.3519590497016907,-0.6837261915206909,0.22237664461135864,-0.9929219484329224,0.6535818576812744,0.5740951895713806,0.36401331424713135,0.09609035402536392,0.9359624981880188,-0.9228775501251221,0.619107723236084,-0.33572664856910706,-0.998464822769165,0.6448630690574646,0.8468658924102783,-0.7557654976844788,0.9527968764305115,0.7326542139053345,0.532416820526123,0.9732670187950134,0.7239247560501099,0.2553896903991699,0.8158413171768188,0.6655787825584412,0.9287099838256836,-0.5660437345504761,-0.8908745050430298,0.9834232926368713,-0.998117983341217,0.9698693156242371,0.7439671158790588,-0.35639357566833496,0.6022986173629761,0.9708117246627808,-0.9278276562690735,-0.9962316155433655,0.8345207571983337,0.9714326858520508,0.8158542513847351,0.5902576446533203,0.8199344873428345,0.9820227026939392,0.9859328269958496,0.5152459144592285,-0.5610515475273132,0.9663665294647217,0.9495158791542053,-0.5204811096191406,0.31047508120536804,0.9859085083007812,0.7797457575798035,0.6738533973693848,0.39197400212287903,-0.9062040448188782,0.11750978976488113,0.8077874779701233,0.41693049669265747,0.9829015135765076,0.9902303218841553,0.7847104668617249,0.9945629835128784,0.9868025779724121,-0.268044114112854,-0.9908866882324219,0.745792806148529,0.9906190037727356,-0.1823112666606903,0.9757837057113647,0.9986750483512878,0.254433274269104,-0.9544061422348022,0.5869240760803223,-0.23538005352020264,0.9550504088401794,0.2551196813583374,0.9929869771003723,0.09052585065364838,0.9707273244857788,0.6956093311309814,0.628002405166626,0.790286660194397,0.9343839883804321,0.9895793795585632,0.9436281323432922,-0.10835029184818268,-0.34311148524284363,0.9986709356307983,0.5086733102798462,0.9949509501457214,0.828312873840332,0.9432438015937805,0.9491767883300781,0.9995352029800415,0.5712308287620544,0.8055234551429749,0.6781863570213318,0.827256977558136,0.8314761519432068,0.8778651356697083,0.9944959282875061,0.9973866939544678],\"xaxis\":\"x\",\"y\":[0.7775009870529175,0.35272666811943054,0.2596185505390167,0.667025625705719,0.8384258151054382,0.5584431290626526,0.8444746136665344,0.4137907922267914,0.2448895424604416,0.028157558292150497,0.35840970277786255,0.1628827601671219,-0.4541912078857422,-0.6529326438903809,-0.548457145690918,-0.7990368604660034,-0.7736424207687378,-0.8522582054138184,0.9774323105812073,0.662625253200531,-0.7303222417831421,-0.7007020711898804,-0.6946624517440796,-0.9996722340583801,-0.7837162017822266,0.8967756628990173,0.47509536147117615,-0.6671971678733826,0.7881460785865784,-0.8547748327255249,-0.9054181575775146,-0.574938178062439,-0.32175105810165405,-0.028594080358743668,-0.9247617721557617,-0.9699268341064453,0.5864037275314331,-0.76143479347229,0.5971695184707642,0.7854392528533936,-0.8788883686065674,0.39087456464767456,0.044738586992025375,0.1102805808186531,-0.8169987201690674,0.2212953269481659,-0.9939578771591187,0.577440083026886,0.5254793167114258,0.30490124225616455,-0.107291579246521,0.9433152675628662,-0.9314428567886353,0.5273630619049072,-0.42647096514701843,-0.9984429478645325,0.5296756625175476,0.8604294061660767,-0.88950514793396,0.9556970000267029,0.6629186272621155,0.429569810628891,0.9736858010292053,0.6555481553077698,0.12201868742704391,0.7442768812179565,0.5037953853607178,0.9525357484817505,-0.6507166624069214,-0.9316279292106628,0.9791511297225952,-0.9972584843635559,0.9613032937049866,0.7501777410507202,-0.3806661069393158,0.6429786682128906,0.9557769298553467,-0.9428837895393372,-0.9948078989982605,0.7852988243103027,0.9657301902770996,0.7073014378547668,0.3687227964401245,0.8128010034561157,0.9659483432769775,0.9730119705200195,0.3190060257911682,-0.3029051125049591,0.9790951609611511,0.9357923269271851,-0.5550316572189331,-0.005466540809720755,0.9867777228355408,0.8249565958976746,0.5664295554161072,0.10005273669958115,-0.9464486241340637,-0.2547200918197632,0.6522325873374939,0.14152537286281586,0.9884140491485596,0.9860583543777466,0.6949269771575928,0.9901810884475708,0.9791203141212463,-0.2359551340341568,-0.982071042060852,0.6506689190864563,0.9895942807197571,-0.2917816638946533,0.9714025259017944,0.9951602220535278,0.18783794343471527,-0.9460936784744263,0.4780190885066986,-0.2489192634820938,0.9437097907066345,0.1186617910861969,0.9941242933273315,-0.3808818757534027,0.9564487338066101,0.5542723536491394,0.42118075489997864,0.6628789901733398,0.8659594655036926,0.9937118887901306,0.9069077372550964,0.39811062812805176,-0.41342177987098694,0.9971913695335388,0.3459666669368744,0.9938657283782959,0.589126706123352,0.9313737750053406,0.9268401265144348,0.9993563890457153,0.6227540969848633,0.8463947176933289,0.6584346294403076,0.8423123955726624,0.29784950613975525,0.8728677034378052,0.996314525604248,0.9867525696754456],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Full OV Copying Score\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"OV Copying Score\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"OV Copying Score for each head in GPT-2 Small\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('fbf561b3-3493-40ac-b396-0f320d1752a8');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "scatter(x=full_OV_copying_score.flatten(), y=OV_copying_score.flatten(), hover_name=[f\"L{layer}H{head}\" for layer in range(12) for head in range(12)], title=\"OV Copying Score for each head in GPT-2 Small\", xaxis=\"Full OV Copying Score\", yaxis=\"OV Copying Score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "b9-pwMcv_xXa",
        "outputId": "c550b028-683f-4713-ea7f-250964c5b6d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Token 256 - the most common pair of ASCII characters: | t|\n",
            "De-Tokenizing the example tokens: <|endoftext|>The first thing you need to figure out is *how* things are tokenized. `model.to_str_tokens` splits a string into the tokens *as a list of substrings*, and so lets you explore what the text looks like. To demonstrate this, let's use it on this paragraph.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Token 256 - the most common pair of ASCII characters: |{model.to_string(256)}|\")\n",
        "# Squeeze means to remove dimensions of length 1. \n",
        "# Here, that removes the dummy batch dimension so it's a rank 1 tensor and returns a string\n",
        "# Rank 2 tensors map to a list of strings\n",
        "print(f\"De-Tokenizing the example tokens: {model.to_string(example_text_tokens.squeeze())}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs-5iuea_xXa"
      },
      "source": [
        "## Generating Text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTtFVNWl_xXb"
      },
      "source": [
        "TransformerLens also has basic text generation functionality, which can be useful for generally exploring what the model is capable of (thanks to Ansh Radhakrishnan for adding this!). This is pretty rough functionality, and where possible I recommend using more established libraries like HuggingFace for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "LKViQs5o_xXb",
        "outputId": "780076e6-f0d9-4a29-ff00-1ecb5d1f9531"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f8200abc190479aa112ec53ba155b2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "\"(CNN) President Barack Obama caught in embarrassing new scandal\\n\\nOn CBS's Face the Nation Sunday, the Obama administration officials took questions from the audience about the 2009 Benghazi attack, which killed four Americans.\\n\\nThe administration officials did not learn that the government had been compromised did not they?\\n\\nRead\""
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.generate(\"(CNN) President Barack Obama caught in embarrassing new scandal\\n\", max_new_tokens=50, temperature=0.7, prepend_bos=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjaXm6Ct_xXb"
      },
      "source": [
        "## Hook Points\n",
        "\n",
        "The key part of TransformerLens that lets us access and edit intermediate activations are the HookPoints around every model activation. Importantly, this technique will work for *any* model architecture, not just transformers, so long as you're able to edit the model code to add in HookPoints! This is essentially a lightweight library bundled with TransformerLens that should let you take an arbitrary model and make it easier to study. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3VoG0Ms_xXb"
      },
      "source": [
        "This is implemented by having a HookPoint layer. Each transformer component has a HookPoint for every activation, which wraps around that activation. The HookPoint acts as an identity function, but has a variety of helper functions that allows us to put PyTorch hooks in to edit and access the relevant activation. \n",
        "\n",
        "There is also a `HookedRootModule` class - this is a utility class that the root module should inherit from (root module = the model we run) - it has several utility functions for using hooks well, notably `reset_hooks`, `run_with_cache` and `run_with_hooks`. \n",
        "\n",
        "The default interface is the `run_with_hooks` function on the root module, which lets us run a forwards pass on the model, and pass on a list of hooks paired with layer names to run on that pass. \n",
        "\n",
        "The syntax for a hook is `function(activation, hook)` where `activation` is the activation the hook is wrapped around, and `hook` is the `HookPoint` class the function is attached to. If the function returns a new activation or edits the activation in-place, that replaces the old one, if it returns None then the activation remains as is.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9faMfBqy_xXb"
      },
      "source": [
        "### Toy Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go8ZBKgm_xXb"
      },
      "source": [
        "\n",
        "Here's a simple example of defining a small network with HookPoints:\n",
        "\n",
        "We define a basic network with two layers that each take a scalar input $x$, square it, and add a constant:\n",
        "$x_0=x$, $x_1=x_0^2+3$, $x_2=x_1^2-4$.\n",
        "\n",
        "We wrap the input, each layer's output, and the intermediate value of each layer (the square) in a hook point.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "id": "oj1PkKEM_xXb"
      },
      "outputs": [],
      "source": [
        "\n",
        "from transformer_lens.hook_points import HookedRootModule, HookPoint\n",
        "\n",
        "\n",
        "class SquareThenAdd(nn.Module):\n",
        "    def __init__(self, offset):\n",
        "        super().__init__()\n",
        "        self.offset = nn.Parameter(torch.tensor(offset))\n",
        "        self.hook_square = HookPoint()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # The hook_square doesn't change the value, but lets us access it\n",
        "        square = self.hook_square(x * x)\n",
        "        return self.offset + square\n",
        "\n",
        "\n",
        "class TwoLayerModel(HookedRootModule):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.layer1 = SquareThenAdd(3.0)\n",
        "        self.layer2 = SquareThenAdd(-4.0)\n",
        "        self.hook_in = HookPoint()\n",
        "        self.hook_mid = HookPoint()\n",
        "        self.hook_out = HookPoint()\n",
        "\n",
        "        # We need to call the setup function of HookedRootModule to build an\n",
        "        # internal dictionary of modules and hooks, and to give each hook a name\n",
        "        super().setup()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # We wrap the input and each layer's output in a hook - they leave the\n",
        "        # value unchanged (unless there's a hook added to explicitly change it),\n",
        "        # but allow us to access it.\n",
        "        x_in = self.hook_in(x)\n",
        "        x_mid = self.hook_mid(self.layer1(x_in))\n",
        "        x_out = self.hook_out(self.layer2(x_mid))\n",
        "        return x_out\n",
        "\n",
        "\n",
        "model = TwoLayerModel()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HCb4PJnD_xXb"
      },
      "source": [
        "\n",
        "We can add a cache, to save the activation at each hook point\n",
        "\n",
        "(There's a custom `run_with_cache` function on the root module as a convenience, which is a wrapper around model.forward that return model_out, cache_object - we could also manually add hooks with `run_with_hooks` that store activations in a global caching dictionary. This is often useful if we only want to store, eg, subsets or functions of some activations.)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "VdfGTZv__xXb",
        "outputId": "a579a57d-b745-4e90-ce21-7b2c150a9982"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model output: 780.0\n",
            "Value cached at hook hook_in 5.0\n",
            "Value cached at hook layer1.hook_square 25.0\n",
            "Value cached at hook hook_mid 28.0\n",
            "Value cached at hook layer2.hook_square 784.0\n",
            "Value cached at hook hook_out 780.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "out, cache = model.run_with_cache(torch.tensor(5.0))\n",
        "print(\"Model output:\", out.item())\n",
        "for key in cache:\n",
        "    print(f\"Value cached at hook {key}\", cache[key].item())\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uc9mx_Ut_xXb"
      },
      "source": [
        "\n",
        "We can also use hooks to intervene on activations - eg, we can set the intermediate value in layer 2 to zero to change the output to -5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {
        "id": "BpMqp1MM_xXb",
        "outputId": "390094b4-96a6-4f67-a2b3-bcf92848ae03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "layer2.hook_square\n",
            "Output after intervening on layer2.hook_scaled -4.0\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def set_to_zero_hook(tensor, hook):\n",
        "    print(hook.name)\n",
        "    return torch.tensor(0.0)\n",
        "\n",
        "\n",
        "print(\n",
        "    \"Output after intervening on layer2.hook_scaled\",\n",
        "    model.run_with_hooks(\n",
        "        torch.tensor(5.0), fwd_hooks=[(\"layer2.hook_square\", set_to_zero_hook)]\n",
        "    ).item(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heJfIUp5_xXc"
      },
      "source": [
        "## Loading Pre-Trained Checkpoints\n",
        "\n",
        "There are a lot of interesting questions combining mechanistic interpretability and training dynamics - analysing model capabilities and the underlying circuits that make them possible, and how these change as we train the model. \n",
        "\n",
        "TransformerLens supports these by having several model families with checkpoints throughout training. `HookedTransformer.from_pretrained` can load a checkpoint of a model with the `checkpoint_index` (the label 0 to `num_checkpoints-1`) or `checkpoint_value` (the step or token number, depending on how the checkpoints were labelled)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6eOY3GX_xXc"
      },
      "source": [
        "\n",
        "Available models:\n",
        "* All of my interpretability-friendly models have checkpoints available, including:\n",
        "    * The toy models - `attn-only`, `solu`, `gelu` 1L to 4L\n",
        "        * These have ~200 checkpoints, taken on a piecewise linear schedule (more checkpoints near the start of training), up to 22B tokens. Labelled by number of tokens seen.\n",
        "    * The SoLU models trained on 80% Web Text and 20% Python Code (`solu-6l` to `solu-12l`)\n",
        "        * Same checkpoint schedule as the toy models, this time up to 30B tokens\n",
        "    * The SoLU models trained on the pile (`solu-1l-pile` to `solu-12l-pile`)\n",
        "        * These have ~100 checkpoints, taken on a linear schedule, up to 15B tokens. Labelled by number of steps.\n",
        "        * The 12L training crashed around 11B tokens, so is truncated.\n",
        "* The Stanford Centre for Research of Foundation Models trained 5 GPT-2 Small sized and 5 GPT-2 Medium sized models (`stanford-gpt2-small-a` to `e` and `stanford-gpt2-medium-a` to `e`)\n",
        "    * 600 checkpoints, taken on a piecewise linear schedule, labelled by the number of steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BInOqbNh_xXc"
      },
      "source": [
        "The checkpoint structure and labels is somewhat messy and ad-hoc, so I mostly recommend using the `checkpoint_index` syntax (where you can just count from 0 to the number of checkpoints) rather than `checkpoint_value` syntax (where you need to know the checkpoint schedule, and whether it was labelled with the number of tokens or steps). The helper function `get_checkpoint_labels` tells you the checkpoint schedule for a given model - ie what point was each checkpoint taken at, and what type of label was used.\n",
        "\n",
        "Here are graphs of the schedules for several checkpointed models: (note that the first 3 use a log scale, latter 2 use a linear scale)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "Vbnd03MY_xXc",
        "outputId": "28667ed9-81fc-4411-d1c5-924ebb43627a"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"2ff1a906-cd94-45b4-b10a-a6e47115251f\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"2ff1a906-cd94-45b4-b10a-a6e47115251f\")) {                    Plotly.newPlot(                        \"2ff1a906-cd94-45b4-b10a-a6e47115251f\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162],\"xaxis\":\"x\",\"y\":[262144,2621440,4718592,7077888,9175040,11272192,13631488,15728640,18087936,20185088,22282240,33292288,44302336,55312384,66322432,77332480,88342528,99352576,110362624,121372672,132382720,143392768,154402816,165412864,176422912,187432960,198443008,209453056,220463104,264503296,308281344,352321536,396361728,440401920,484442112,528482304,572522496,616300544,660340736,704380928,748421120,792461312,836501504,880279552,924319744,968359936,1012400128,1056440320,1100480512,1144520704,1188298752,1232338944,1276379136,1320419328,1364459520,1408499712,1452277760,1496317952,1540358144,1584398336,1628438528,1672478720,1716518912,1760296960,1804337152,1848377344,1892417536,1936457728,1980497920,2024275968,2068316160,2112356352,2156396544,2200436736,2420375552,2640314368,2860515328,3080454144,3300392960,3520331776,3740270592,3960471552,4180410368,4400349184,4620288000,4840488960,5060427776,5280366592,5500305408,5720506368,5940445184,6160384000,6380322816,6600523776,6820462592,7040401408,7260340224,7480279040,7700480000,7920418816,8140357632,8360296448,8580497408,8800436224,9020375040,9240313856,9460514816,9680453632,9900392448,10120331264,10340270080,10560471040,10780409856,11000348672,11220287488,11440488448,11660427264,11880366080,12100304896,12320505856,12540444672,12760383488,12980322304,13200523264,13420462080,13640400896,13860339712,14080278528,14300479488,14520418304,14740357120,14960295936,15180496896,15400435712,15620374528,15840313344,16060514304,16280453120,16500391936,16720330752,16940269568,17160470528,17380409344,17600348160,17820286976,18040487936,18260426752,18480365568,18700304384,18920505344,19140444160,19360382976,19580321792,19800522752,20020461568,20240400384,20460339200,20680278016,20900478976,21120417792,21340356608,21560295424,21780496384],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for attn-only-2l (Log scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('2ff1a906-cd94-45b4-b10a-a6e47115251f');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"bf6786a7-89fa-4dd5-9667-941d555564d7\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"bf6786a7-89fa-4dd5-9667-941d555564d7\")) {                    Plotly.newPlot(                        \"bf6786a7-89fa-4dd5-9667-941d555564d7\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162],\"xaxis\":\"x\",\"y\":[196608,3342336,6291456,9240576,12386304,15335424,18284544,21233664,24379392,27328512,30277632,45219840,60358656,75300864,90243072,105381888,120324096,135266304,150208512,165347328,180289536,195231744,210370560,225312768,240254976,255197184,270336000,285278208,300220416,360382464,420347904,480313344,540278784,600244224,660209664,720371712,780337152,840302592,900268032,960233472,1020198912,1080360960,1140326400,1200291840,1260257280,1320222720,1380384768,1440350208,1500315648,1560281088,1620246528,1680211968,1740374016,1800339456,1860304896,1920270336,1980235776,2040201216,2100363264,2160328704,2220294144,2280259584,2340225024,2400387072,2460352512,2520317952,2580283392,2640248832,2700214272,2760376320,2820341760,2880307200,2940272640,3000238080,3300261888,3600285696,3900309504,4200333312,4500357120,4800380928,5100208128,5400231936,5700255744,6000279552,6300303360,6600327168,6900350976,7200374784,7500201984,7800225792,8100249600,8400273408,8700297216,9000321024,9300344832,9600368640,9900392448,10200219648,10500243456,10800267264,11100291072,11400314880,11700338688,12000362496,12300386304,12600213504,12900237312,13200261120,13500284928,13800308736,14100332544,14400356352,14700380160,15000207360,15300231168,15600254976,15900278784,16200302592,16500326400,16800350208,17100374016,17400201216,17700225024,18000248832,18300272640,18600296448,18900320256,19200344064,19500367872,19800391680,20100218880,20400242688,20700266496,21000290304,21300314112,21600337920,21900361728,22200385536,22500212736,22800236544,23100260352,23400284160,23700307968,24000331776,24300355584,24600379392,24900206592,25200230400,25500254208,25800278016,26100301824,26400325632,26700349440,27000373248,27300200448,27600224256,27900248064,28200271872,28500295680,28800319488,29100343296,29400367104,29700390912],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for solu-12l (Log scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('bf6786a7-89fa-4dd5-9667-941d555564d7');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"e61f9b8a-6aef-47dc-8bb2-fd3a21006234\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"e61f9b8a-6aef-47dc-8bb2-fd3a21006234\")) {                    Plotly.newPlot(                        \"e61f9b8a-6aef-47dc-8bb2-fd3a21006234\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99,100,101,102,103,104,105,106,107,108,109,110,111,112,113,114,115,116,117,118,119,120,121,122,123,124,125,126,127,128,129,130,131,132,133,134,135,136,137,138,139,140,141,142,143,144,145,146,147,148,149,150,151,152,153,154,155,156,157,158,159,160,161,162,163,164,165,166,167,168,169,170,171,172,173,174,175,176,177,178,179,180,181,182,183,184,185,186,187,188,189,190,191,192,193,194,195,196,197,198,199,200,201,202,203,204,205,206,207,208,209,210,211,212,213,214,215,216,217,218,219,220,221,222,223,224,225,226,227,228,229,230,231,232,233,234,235,236,237,238,239,240,241,242,243,244,245,246,247,248,249,250,251,252,253,254,255,256,257,258,259,260,261,262,263,264,265,266,267,268,269,270,271,272,273,274,275,276,277,278,279,280,281,282,283,284,285,286,287,288,289,290,291,292,293,294,295,296,297,298,299,300,301,302,303,304,305,306,307,308,309,310,311,312,313,314,315,316,317,318,319,320,321,322,323,324,325,326,327,328,329,330,331,332,333,334,335,336,337,338,339,340,341,342,343,344,345,346,347,348,349,350,351,352,353,354,355,356,357,358,359,360,361,362,363,364,365,366,367,368,369,370,371,372,373,374,375,376,377,378,379,380,381,382,383,384,385,386,387,388,389,390,391,392,393,394,395,396,397,398,399,400,401,402,403,404,405,406,407,408,409,410,411,412,413,414,415,416,417,418,419,420,421,422,423,424,425,426,427,428,429,430,431,432,433,434,435,436,437,438,439,440,441,442,443,444,445,446,447,448,449,450,451,452,453,454,455,456,457,458,459,460,461,462,463,464,465,466,467,468,469,470,471,472,473,474,475,476,477,478,479,480,481,482,483,484,485,486,487,488,489,490,491,492,493,494,495,496,497,498,499,500,501,502,503,504,505,506,507,508,509,510,511,512,513,514,515,516,517,518,519,520,521,522,523,524,525,526,527,528,529,530,531,532,533,534,535,536,537,538,539,540,541,542,543,544,545,546,547,548,549,550,551,552,553,554,555,556,557,558,559,560,561,562,563,564,565,566,567,568,569,570,571,572,573,574,575,576,577,578,579,580,581,582,583,584,585,586,587,588,589,590,591,592,593,594,595,596,597,598,599,600,601,602,603,604,605,606,607,608],\"xaxis\":\"x\",\"y\":[0,10,20,30,40,50,60,70,80,90,100,150,200,250,300,350,400,450,500,550,600,650,700,750,800,850,900,950,1000,1050,1100,1150,1200,1250,1300,1350,1400,1450,1500,1550,1600,1650,1700,1750,1800,1850,1900,1950,2000,2100,2200,2300,2400,2500,2600,2700,2800,2900,3000,3100,3200,3300,3400,3500,3600,3700,3800,3900,4000,4100,4200,4300,4400,4500,4600,4700,4800,4900,5000,5100,5200,5300,5400,5500,5600,5700,5800,5900,6000,6100,6200,6300,6400,6500,6600,6700,6800,6900,7000,7100,7200,7300,7400,7500,7600,7700,7800,7900,8000,8100,8200,8300,8400,8500,8600,8700,8800,8900,9000,9100,9200,9300,9400,9500,9600,9700,9800,9900,10000,10100,10200,10300,10400,10500,10600,10700,10800,10900,11000,11100,11200,11300,11400,11500,11600,11700,11800,11900,12000,12100,12200,12300,12400,12500,12600,12700,12800,12900,13000,13100,13200,13300,13400,13500,13600,13700,13800,13900,14000,14100,14200,14300,14400,14500,14600,14700,14800,14900,15000,15100,15200,15300,15400,15500,15600,15700,15800,15900,16000,16100,16200,16300,16400,16500,16600,16700,16800,16900,17000,17100,17200,17300,17400,17500,17600,17700,17800,17900,18000,18100,18200,18300,18400,18500,18600,18700,18800,18900,19000,19100,19200,19300,19400,19500,19600,19700,19800,19900,20000,21000,22000,23000,24000,25000,26000,27000,28000,29000,30000,31000,32000,33000,34000,35000,36000,37000,38000,39000,40000,41000,42000,43000,44000,45000,46000,47000,48000,49000,50000,51000,52000,53000,54000,55000,56000,57000,58000,59000,60000,61000,62000,63000,64000,65000,66000,67000,68000,69000,70000,71000,72000,73000,74000,75000,76000,77000,78000,79000,80000,81000,82000,83000,84000,85000,86000,87000,88000,89000,90000,91000,92000,93000,94000,95000,96000,97000,98000,99000,100000,101000,102000,103000,104000,105000,106000,107000,108000,109000,110000,111000,112000,113000,114000,115000,116000,117000,118000,119000,120000,121000,122000,123000,124000,125000,126000,127000,128000,129000,130000,131000,132000,133000,134000,135000,136000,137000,138000,139000,140000,141000,142000,143000,144000,145000,146000,147000,148000,149000,150000,151000,152000,153000,154000,155000,156000,157000,158000,159000,160000,161000,162000,163000,164000,165000,166000,167000,168000,169000,170000,171000,172000,173000,174000,175000,176000,177000,178000,179000,180000,181000,182000,183000,184000,185000,186000,187000,188000,189000,190000,191000,192000,193000,194000,195000,196000,197000,198000,199000,200000,201000,202000,203000,204000,205000,206000,207000,208000,209000,210000,211000,212000,213000,214000,215000,216000,217000,218000,219000,220000,221000,222000,223000,224000,225000,226000,227000,228000,229000,230000,231000,232000,233000,234000,235000,236000,237000,238000,239000,240000,241000,242000,243000,244000,245000,246000,247000,248000,249000,250000,251000,252000,253000,254000,255000,256000,257000,258000,259000,260000,261000,262000,263000,264000,265000,266000,267000,268000,269000,270000,271000,272000,273000,274000,275000,276000,277000,278000,279000,280000,281000,282000,283000,284000,285000,286000,287000,288000,289000,290000,291000,292000,293000,294000,295000,296000,297000,298000,299000,300000,301000,302000,303000,304000,305000,306000,307000,308000,309000,310000,311000,312000,313000,314000,315000,316000,317000,318000,319000,320000,321000,322000,323000,324000,325000,326000,327000,328000,329000,330000,331000,332000,333000,334000,335000,336000,337000,338000,339000,340000,341000,342000,343000,344000,345000,346000,347000,348000,349000,350000,351000,352000,353000,354000,355000,356000,357000,358000,359000,360000,361000,362000,363000,364000,365000,366000,367000,368000,369000,370000,371000,372000,373000,374000,375000,376000,377000,378000,379000,380000,381000,382000,383000,384000,385000,386000,387000,388000,389000,390000,391000,392000,393000,394000,395000,396000,397000,398000,399000,400000],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"},\"type\":\"log\"},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for stanford-gpt2-small-a (Log scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('e61f9b8a-6aef-47dc-8bb2-fd3a21006234');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"d769b355-33f5-41ed-aa67-51f1a6f7e478\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"d769b355-33f5-41ed-aa67-51f1a6f7e478\")) {                    Plotly.newPlot(                        \"d769b355-33f5-41ed-aa67-51f1a6f7e478\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49],\"xaxis\":\"x\",\"y\":[832,1664,2496,3328,4160,4992,5824,6656,7488,8320,9152,9984,10816,11648,12480,13312,14144,14976,15808,16640,17472,18304,19136,19968,20800,21632,22464,23296,24128,24960,25792,26624,27456,28288,29120,29952,30784,31616,32448,33280,34112,34944,35776,36608,37440,38272,39104,39936,40768,41600],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for solu-1l-pile (Linear scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('d769b355-33f5-41ed-aa67-51f1a6f7e478');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.12.1.min.js\"></script>                <div id=\"0db05157-4bd7-4571-97a8-55d084522586\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0db05157-4bd7-4571-97a8-55d084522586\")) {                    Plotly.newPlot(                        \"0db05157-4bd7-4571-97a8-55d084522586\",                        [{\"hovertemplate\":\"variable=0<br>index=%{x}<br>value=%{y}<extra></extra>\",\"legendgroup\":\"0\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"lines+markers\",\"name\":\"0\",\"orientation\":\"v\",\"showlegend\":true,\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,58,59,60,61,62,63,64,65,66,67,68,69,70,71,72,73,74,75,76,77,78,79,80,81,82,83,84,85,86,87,88,89,90,91,92,93,94,95,96,97,98,99],\"xaxis\":\"x\",\"y\":[326,652,978,1304,1630,1956,2282,2608,2934,3260,3586,3912,4238,4564,4890,5216,5542,5868,6194,6520,6846,7172,7498,7824,8150,8476,8802,9128,9454,9780,10106,10432,10758,11084,11410,11736,12062,12388,12714,13040,13366,13692,14018,14344,14670,14996,15322,15648,15974,16300,16626,16952,17278,17604,17930,18256,18582,18908,19234,19560,19886,20212,20538,20864,21190,21516,21842,22168,22494,22820,23146,23472,23798,24124,24450,24776,25102,25428,25754,26080,26406,26732,27058,27384,27710,28036,28362,28688,29014,29340,29666,29992,30318,30644,30970,31296,31622,31948,32274,32600],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"value\"}},\"legend\":{\"title\":{\"text\":\"variable\"},\"tracegroupgap\":0},\"title\":{\"text\":\"Checkpoint Values for solu-6l-pile (Linear scale)\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('0db05157-4bd7-4571-97a8-55d084522586');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "from transformer_lens.loading_from_pretrained import get_checkpoint_labels\n",
        "for model_name in [\"attn-only-2l\", \"solu-12l\", \"stanford-gpt2-small-a\"]:\n",
        "    checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)\n",
        "    line(checkpoint_labels, xaxis=\"Checkpoint Index\", yaxis=f\"Checkpoint Value ({checkpoint_label_type})\", title=f\"Checkpoint Values for {model_name} (Log scale)\", log_y=True, markers=True)\n",
        "for model_name in [\"solu-1l-pile\", \"solu-6l-pile\"]:\n",
        "    checkpoint_labels, checkpoint_label_type = get_checkpoint_labels(model_name)\n",
        "    line(checkpoint_labels, xaxis=\"Checkpoint Index\", yaxis=f\"Checkpoint Value ({checkpoint_label_type})\", title=f\"Checkpoint Values for {model_name} (Linear scale)\", log_y=False, markers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6o-WjTi_xXc"
      },
      "source": [
        "### Example: Induction Head Phase Transition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdoGyAZR_xXc"
      },
      "source": [
        "One of the more interesting results analysing circuit formation during training is the [induction head phase transition](https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html). They find a pretty dramatic shift in models during training - there's a brief period where models go from not having induction heads to having them, which leads to the models suddenly becoming much better at in-context learning (using far back tokens to predict the next token, eg over 500 words back). This is enough of a big deal that it leads to a visible *bump* in the loss curve, where the model's rate of improvement briefly increases. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7tMhDHBV_xXc"
      },
      "source": [
        "As a brief demonstration of the existence of the phase transition, let's load some checkpoints of a two layer model, and see whether they have induction heads. An easy test, as we used above, is to give the model a repeated sequence of random tokens, and to check how good its loss is on the second half. `evals.induction_loss` is a rough util that runs this test on a model.\n",
        "(Note - this is deliberately a rough, non-rigorous test for the purposes of demonstration, eg `evals.induction_loss` by default just runs it on 4 sequences of 384 tokens repeated twice. These results totally don't do the paper justice - go check it out if you want to see the full results!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aAF-qjJy_xXc"
      },
      "source": [
        "In the interests of time and memory, let's look at a handful of checkpoints (chosen to be around the phase change), indices `[10, 25, 35, 60, -1]`. These are roughly 22M, 200M, 500M, 1.6B and 21.8B tokens through training, respectively. (I generally recommend looking things up based on indices, rather than checkpoint value!). "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "PVrfsAKF_xXc"
      },
      "outputs": [],
      "source": [
        "from transformer_lens import evals\n",
        "# We use the two layer model with SoLU activations, chosen fairly arbitrarily as being both small (so fast to download and keep in memory) and pretty good at the induction task.\n",
        "model_name = \"solu-2l\"\n",
        "# We can load a model from a checkpoint by specifying the checkpoint_index, -1 means the final checkpoint\n",
        "checkpoint_indices = [10, 25, 35, 60, -1]\n",
        "checkpointed_models = []\n",
        "tokens_trained_on = []\n",
        "induction_losses = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebQFFZTl_xXc"
      },
      "source": [
        "We load the models, cache them in a list, and "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "oOGYUoyu_xXc",
        "outputId": "adc57b3d-530b-44b3-beb1-61356df4b343"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn [120], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfor\u001b[39;00m index \u001b[39min\u001b[39;00m checkpoint_indices:\n\u001b[1;32m      2\u001b[0m     \u001b[39m# Load the model from the relevant checkpoint by index\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     model_for_this_checkpoint \u001b[39m=\u001b[39m HookedTransformer\u001b[39m.\u001b[39;49mfrom_pretrained(model_name, checkpoint_index\u001b[39m=\u001b[39;49mindex)\n\u001b[1;32m      4\u001b[0m     checkpointed_models\u001b[39m.\u001b[39mappend(model_for_this_checkpoint)\n\u001b[1;32m      6\u001b[0m     tokens_seen_for_this_checkpoint \u001b[39m=\u001b[39m model_for_this_checkpoint\u001b[39m.\u001b[39mcfg\u001b[39m.\u001b[39mcheckpoint_value\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/transformer_lens/HookedTransformer.py:578\u001b[0m, in \u001b[0;36mHookedTransformer.from_pretrained\u001b[0;34m(cls, model_name, fold_ln, center_writing_weights, center_unembed, refactor_factored_attn_matrices, checkpoint_index, checkpoint_value, hf_model, device, move_state_dict_to_device, **model_kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m cfg \u001b[39m=\u001b[39m loading\u001b[39m.\u001b[39mget_pretrained_model_config(\n\u001b[1;32m    570\u001b[0m     official_model_name,\n\u001b[1;32m    571\u001b[0m     checkpoint_index\u001b[39m=\u001b[39mcheckpoint_index,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    574\u001b[0m     device\u001b[39m=\u001b[39mdevice,\n\u001b[1;32m    575\u001b[0m )\n\u001b[1;32m    577\u001b[0m \u001b[39m# Get the state dict of the model (ie a mapping of parameter names to tensors), processed to match the HookedTransformer parameter names.\u001b[39;00m\n\u001b[0;32m--> 578\u001b[0m state_dict \u001b[39m=\u001b[39m loading\u001b[39m.\u001b[39;49mget_pretrained_state_dict(\n\u001b[1;32m    579\u001b[0m     official_model_name, cfg, hf_model\n\u001b[1;32m    580\u001b[0m )\n\u001b[1;32m    582\u001b[0m \u001b[39m# Create the HookedTransformer object\u001b[39;00m\n\u001b[1;32m    583\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(cfg, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/transformer_lens/loading_from_pretrained.py:553\u001b[0m, in \u001b[0;36mget_pretrained_state_dict\u001b[0;34m(official_model_name, cfg, hf_model)\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    552\u001b[0m     file_name \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mfilter\u001b[39m(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39mfinal.pth\u001b[39m\u001b[39m\"\u001b[39m), repo_files))[\u001b[39m0\u001b[39m]\n\u001b[0;32m--> 553\u001b[0m state_dict \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39;49mdownload_file_from_hf(official_model_name, file_name)\n\u001b[1;32m    554\u001b[0m \u001b[39mif\u001b[39;00m cfg\u001b[39m.\u001b[39moriginal_architecture \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mneel-solu-old\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m    555\u001b[0m     state_dict \u001b[39m=\u001b[39m convert_neel_solu_old_weights(state_dict, cfg)\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/transformer_lens/utils.py:34\u001b[0m, in \u001b[0;36mdownload_file_from_hf\u001b[0;34m(repo_name, file_name, subfolder, cache_dir, force_is_torch)\u001b[0m\n\u001b[1;32m     29\u001b[0m file_path \u001b[39m=\u001b[39m hf_hub_download(\n\u001b[1;32m     30\u001b[0m     repo_id\u001b[39m=\u001b[39mrepo_name, filename\u001b[39m=\u001b[39mfile_name, subfolder\u001b[39m=\u001b[39msubfolder, cache_dir\u001b[39m=\u001b[39mcache_dir\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m \u001b[39mif\u001b[39;00m file_path\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.pth\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mor\u001b[39;00m force_is_torch:\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(file_path)\n\u001b[1;32m     35\u001b[0m \u001b[39melif\u001b[39;00m file_path\u001b[39m.\u001b[39mendswith(\u001b[39m\"\u001b[39m\u001b[39m.json\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m     36\u001b[0m     \u001b[39mreturn\u001b[39;00m json\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(file_path, \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m))\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/serialization.py:712\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    710\u001b[0m             opened_file\u001b[39m.\u001b[39mseek(orig_position)\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[0;32m--> 712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n\u001b[1;32m    713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/serialization.py:1049\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1047\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(data_file, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1048\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1049\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1051\u001b[0m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1053\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/serialization.py:1019\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1017\u001b[0m \u001b[39mif\u001b[39;00m key \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m loaded_storages:\n\u001b[1;32m   1018\u001b[0m     nbytes \u001b[39m=\u001b[39m numel \u001b[39m*\u001b[39m torch\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1019\u001b[0m     load_tensor(dtype, nbytes, key, _maybe_decode_ascii(location))\n\u001b[1;32m   1021\u001b[0m \u001b[39mreturn\u001b[39;00m loaded_storages[key]\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/serialization.py:1001\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m    997\u001b[0m storage \u001b[39m=\u001b[39m zip_file\u001b[39m.\u001b[39mget_storage_from_record(name, numel, torch\u001b[39m.\u001b[39m_UntypedStorage)\u001b[39m.\u001b[39mstorage()\u001b[39m.\u001b[39m_untyped()\n\u001b[1;32m    998\u001b[0m \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    999\u001b[0m \u001b[39m# stop wrapping with _TypedStorage\u001b[39;00m\n\u001b[1;32m   1000\u001b[0m loaded_storages[key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39m_TypedStorage(\n\u001b[0;32m-> 1001\u001b[0m     wrap_storage\u001b[39m=\u001b[39mrestore_location(storage, location),\n\u001b[1;32m   1002\u001b[0m     dtype\u001b[39m=\u001b[39mdtype)\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/serialization.py:175\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    174\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 175\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    176\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    177\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/serialization.py:152\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    151\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 152\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    153\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    154\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
            "File \u001b[0;32m~/ml/arena/venv/lib/python3.9/site-packages/torch/serialization.py:136\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    133\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    135\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 136\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    137\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    138\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    139\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    140\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    141\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
          ]
        }
      ],
      "source": [
        "for index in checkpoint_indices:\n",
        "    # Load the model from the relevant checkpoint by index\n",
        "    model_for_this_checkpoint = HookedTransformer.from_pretrained(model_name, checkpoint_index=index)\n",
        "    checkpointed_models.append(model_for_this_checkpoint)\n",
        "\n",
        "    tokens_seen_for_this_checkpoint = model_for_this_checkpoint.cfg.checkpoint_value\n",
        "    tokens_trained_on.append(tokens_seen_for_this_checkpoint)\n",
        "\n",
        "    induction_loss_for_this_checkpoint = evals.induction_loss(model_for_this_checkpoint).item()\n",
        "    induction_losses.append(induction_loss_for_this_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXkR4ywY_xXc"
      },
      "source": [
        "We can plot this, and see there's a sharp shift from ~200-500M tokens trained on (note the log scale on the x axis). Interestingly, this is notably earlier than the phase transition in the paper, I'm not sure what's up with that.\n",
        "\n",
        "(To contextualise the numbers, the tokens in the random sequence are uniformly chosen from the first 20,000 tokens (out of ~48,000 total), so random performance is at least $\\ln(20000)\\approx 10$. A naive strategy like \"randomly choose a token that's already appeared in the first half of the sequence (384 elements)\" would get $\\ln(384)\\approx 5.95$, so the model is doing pretty well here.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WByaVRU3_xXc",
        "outputId": "6aceb57d-058c-4c65-c9ca-f248896ef207"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.16.1.min.js\"></script>                <div id=\"6e70a192-5d22-459b-9b0e-f6d00be0bd1e\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"6e70a192-5d22-459b-9b0e-f6d00be0bd1e\")) {                    Plotly.newPlot(                        \"6e70a192-5d22-459b-9b0e-f6d00be0bd1e\",                        [{\"hovertemplate\":\"Tokens Trained On=%{x}<br>index=%{y}<extra></extra>\",\"legendgroup\":\"\",\"line\":{\"color\":\"#636efa\",\"dash\":\"solid\"},\"marker\":{\"symbol\":\"circle\"},\"mode\":\"markers+lines\",\"name\":\"\",\"orientation\":\"h\",\"showlegend\":false,\"x\":[22282240,187432960,528482304,1628438528,21780496384],\"xaxis\":\"x\",\"y\":[0,1,2,3,4],\"yaxis\":\"y\",\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"Tokens Trained On\"},\"type\":\"log\"},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,1.0],\"title\":{\"text\":\"index\"}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Induction Loss over training: solu-2l\"}},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('6e70a192-5d22-459b-9b0e-f6d00be0bd1e');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "line(induction_losses, x=tokens_trained_on, xaxis=\"Tokens Trained On\", yaxis=\"Induction Loss\", title=\"Induction Loss over training: solu-2l\", markers=True, log_x=True)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6 (default, Aug  5 2022, 15:21:02) \n[Clang 14.0.0 (clang-1400.0.29.102)]"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
