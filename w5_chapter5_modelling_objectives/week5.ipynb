{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "import w5d1_tests\n",
    "from fancy_einsum import einsum\n",
    "from typing import Union\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "IntOrPair = Union[int, tuple[int, int]]\n",
    "Pair = tuple[int, int]\n",
    "def conv1d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Like torch's conv1d using bias=False and all other keyword arguments left at their default values.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_width)\n",
    "    \"\"\"\n",
    "    \n",
    "    batch, in_channels, width = x.shape\n",
    "    out_channels, in_channels_2, kernel_width = weights.shape\n",
    "    assert in_channels == in_channels_2, \"in_channels for x and weights don't match up\"\n",
    "    output_width = width - kernel_width + 1\n",
    "    \n",
    "    xsB, xsI, xsWi = x.stride()\n",
    "    wsO, wsI, wsW = weights.stride()\n",
    "    \n",
    "    x_new_shape = (batch, in_channels, output_width, kernel_width)\n",
    "    x_new_stride = (xsB, xsI, xsWi, xsWi)\n",
    "    # Common error: xsWi is always 1, so if you put 1 here you won't spot your mistake until you try this with conv2d!\n",
    "    x_strided = x.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "    \n",
    "    return einsum(\n",
    "        \"batch in_channels output_width kernel_width, out_channels in_channels kernel_width -> batch out_channels output_width\", \n",
    "        x_strided, weights\n",
    "    )\n",
    "\n",
    "def conv2d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n",
    "    '''Like torch's conv2d using bias=False and all other keyword arguments left at their default values.\n",
    "\n",
    "    x: shape (batch, in_channels, height, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_height, output_width)\n",
    "    '''\n",
    "    xsB, xsI, xsHi, xsWi = x.stride()\n",
    "    x_new_stride = (xsB, xsI, xsHi, xsWi, xsHi, xsWi)\n",
    "\n",
    "    batch, in_channels, height, width = x.shape\n",
    "    out_channels, in_channels_2, kernel_height, kernel_width = weights.shape\n",
    "    output_width = width - kernel_width + 1\n",
    "    output_height = height - kernel_height + 1\n",
    "    \n",
    "    x_new_shape = (batch, in_channels, output_height, output_width, kernel_height, kernel_width)\n",
    "\n",
    "\n",
    "    return einsum(\n",
    "        \"\"\"batch in_channels output_height output_width kernel_height kernel_width, \n",
    "        out_channels in_channels kernel_height kernel_width \n",
    "        -> \n",
    "        batch out_channels output_height output_width\"\"\", \n",
    "        x.as_strided(x_new_shape, x_new_stride), weights\n",
    "    )\n",
    "\n",
    "def pad1d(x: t.Tensor, left: int, right: int, pad_value: float) -> t.Tensor:\n",
    "    \"\"\"Return a new tensor with padding applied to the edges.\n",
    "\n",
    "    x: shape (batch, in_channels, width), dtype float32\n",
    "\n",
    "    Return: shape (batch, in_channels, left + right + width)\n",
    "    \"\"\"\n",
    "    B, C, W = x.shape\n",
    "    output = x.new_full(size=(B, C, left + W + right), fill_value=pad_value)\n",
    "    output[..., left : left + W] = x\n",
    "    # Note - you can't use `left:-right`, because `right` could be zero.\n",
    "    return output\n",
    "    \n",
    "def force_pair(v: IntOrPair) -> Pair:\n",
    "    \"\"\"Convert v to a pair of int, if it isn't already.\"\"\"\n",
    "    if isinstance(v, tuple):\n",
    "        if len(v) != 2:\n",
    "            raise ValueError(v)\n",
    "        return (int(v[0]), int(v[1]))\n",
    "    elif isinstance(v, int):\n",
    "        return (v, v)\n",
    "    raise ValueError(v)\n",
    "\n",
    "def pad2d(x: t.Tensor, left: int, right: int, top: int, bottom: int, pad_value: float) -> t.Tensor:\n",
    "    \"\"\"Return a new tensor with padding applied to the edges.\n",
    "\n",
    "    x: shape (batch, in_channels, height, width), dtype float32\n",
    "\n",
    "    Return: shape (batch, in_channels, top + height + bottom, left + width + right)\n",
    "    \"\"\"\n",
    "    B, C, H, W = x.shape\n",
    "    output = x.new_full(size=(B, C, top + H + bottom, left + W + right), fill_value=pad_value)\n",
    "    output[..., top : top + H, left : left + W] = x\n",
    "    return output\n",
    "\n",
    "def conv2d(x, weights, stride: IntOrPair = 1, padding: IntOrPair = 0) -> t.Tensor:\n",
    "    \"\"\"Like torch's conv2d using bias=False\n",
    "\n",
    "    x: shape (batch, in_channels, height, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_height, kernel_width)\n",
    "\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_height, output_width)\n",
    "    \"\"\"\n",
    "\n",
    "    stride_h, stride_w = force_pair(stride)\n",
    "    padding_h, padding_w = force_pair(padding)\n",
    "    \n",
    "    x_padded = pad2d(x, left=padding_w, right=padding_w, top=padding_h, bottom=padding_h, pad_value=0)\n",
    "    \n",
    "    batch, in_channels, height, width = x_padded.shape\n",
    "    out_channels, in_channels_2, kernel_height, kernel_width = weights.shape\n",
    "    assert in_channels == in_channels_2, \"in_channels for x and weights don't match up\"\n",
    "    output_width = 1 + (width - kernel_width) // stride_w\n",
    "    output_height = 1 + (height - kernel_height) // stride_h\n",
    "    \n",
    "    xsB, xsIC, xsH, xsW = x_padded.stride() # B for batch, IC for input channels, H for height, W for width\n",
    "    wsOC, wsIC, wsH, wsW = weights.stride()\n",
    "    \n",
    "    x_new_shape = (batch, in_channels, output_height, output_width, kernel_height, kernel_width)\n",
    "    x_new_stride = (xsB, xsIC, xsH * stride_h, xsW * stride_w, xsH, xsW)\n",
    "    \n",
    "    x_strided = x_padded.as_strided(size=x_new_shape, stride=x_new_stride)\n",
    "    \n",
    "    return einsum(\"B IC OH OW wH wW, OC IC wH wW -> B OC OH OW\", x_strided, weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_conv1d_minimal` passed!\n"
     ]
    }
   ],
   "source": [
    "def test_conv1d_minimal(conv1d_minimal, n_tests=20):\n",
    "    import numpy as np\n",
    "    for _ in range(n_tests):\n",
    "        b = np.random.randint(1, 10)\n",
    "        h = np.random.randint(10, 30)\n",
    "        ci = np.random.randint(1, 5)\n",
    "        co = np.random.randint(1, 5)\n",
    "        kernel_size = np.random.randint(1, 10)\n",
    "        x = t.randn((b, ci, h))\n",
    "        weights = t.randn((co, ci, kernel_size))\n",
    "        my_output = conv1d_minimal(x, weights)\n",
    "        torch_output = t.conv1d(x, weights, stride=1, padding=0)\n",
    "        t.testing.assert_close(my_output, torch_output)\n",
    "    print(\"All tests in `test_conv1d_minimal` passed!\")\n",
    "\n",
    "test_conv1d_minimal(conv1d_minimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 4, 13, 22, 15]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv_transpose1d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n",
    "    '''Like torch's conv_transpose1d using bias=False and all other keyword arguments left at their default values.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "    weights: shape (in_channels, out_channels, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_width)\n",
    "    '''\n",
    "\n",
    "    \"\"\"\n",
    "    x: shape (batch, in_channels, width)\n",
    "    weights: shape (out_channels, in_channels, kernel_width)\n",
    "    \"\"\"\n",
    "    _, _, width = x.shape\n",
    "    _, _, kernel_width = weights.shape\n",
    "    # add padding\n",
    "    padlen = kernel_width - 1\n",
    "    new_width = width + 2 * padlen\n",
    "    # reverse weights\n",
    "    reversed_weights = weights.flip(-1)\n",
    "\n",
    "    import torch.nn.functional as F\n",
    "    # now we expand to size (7, 11) by appending a row of 0s at pos 0 and pos 6, \n",
    "    # and a column of 0s at pos 10\n",
    "    padded_x = F.pad(input=x, pad=(padlen, padlen, 0, 0, 0, 0), mode='constant', value=0)\n",
    "    reversed_weights = rearrange(reversed_weights, 'in out w -> out in w')\n",
    "    return conv1d_minimal(padded_x, reversed_weights)\n",
    "\n",
    "\n",
    "# w5d1_tests.test_conv_transpose1d_minimal(conv_transpose1d_minimal)\n",
    "testtens = t.tensor([[[1,2,3]]])\n",
    "testweights = t.tensor([[[4,5]]])\n",
    "conv_transpose1d_minimal(testtens, testweights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1, 2, 3],\n",
      "         [4, 5, 6]]])\n",
      "tensor([0, 2, 4])\n",
      "2\n",
      "[1, 2, 3]\n",
      "[1, 2, 5]\n",
      "tensor([0, 2, 4])\n",
      "All tests in `test_fractional_stride_1d` passed!\n"
     ]
    }
   ],
   "source": [
    "def fractional_stride_1d(x, stride: int = 1):\n",
    "    '''Returns a version of x suitable for transposed convolutions, i.e. \"spaced out\" with zeros between its values.\n",
    "    This spacing only happens along the last dimension.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "\n",
    "    Example: \n",
    "        x = [[[1, 2, 3], [4, 5, 6]]]\n",
    "        stride = 2\n",
    "        output = [[[1, 0, 2, 0, 3], [4, 0, 5, 0, 6]]]\n",
    "    '''\n",
    "    if stride == 1:\n",
    "        return x \n",
    "    shape = list(x.shape)\n",
    "    print(stride)\n",
    "    print(shape)\n",
    "    stride = stride - 1\n",
    "    width = x.shape[2]\n",
    "    new_width = width + width * (stride) - stride\n",
    "    new_shape = [shape[0], shape[1], new_width]\n",
    "    print(new_shape)\n",
    "    new_x = t.zeros(new_shape, dtype=x.dtype)\n",
    "    print(t.arange(0, new_shape[2], stride+1))\n",
    "    new_x[:,:,t.arange(0, new_shape[2], stride+1)] = x    \n",
    "    return new_x\n",
    "\n",
    "x = t.tensor([[[1, 2, 3], [4, 5, 6]]])\n",
    "# fractional_stride_1d(x, 2)\n",
    "print(x[:,:,::1])\n",
    "print(t.arange(0, 5, 2))\n",
    "\n",
    "w5d1_tests.test_fractional_stride_1d(fractional_stride_1d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "[4, 5, 41]\n",
      "[4, 5, 121]\n",
      "tensor([  0,   3,   6,   9,  12,  15,  18,  21,  24,  27,  30,  33,  36,  39,\n",
      "         42,  45,  48,  51,  54,  57,  60,  63,  66,  69,  72,  75,  78,  81,\n",
      "         84,  87,  90,  93,  96,  99, 102, 105, 108, 111, 114, 117, 120])\n",
      "2\n",
      "[6, 6, 41]\n",
      "[6, 6, 81]\n",
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n",
      "        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70,\n",
      "        72, 74, 76, 78, 80])\n",
      "3\n",
      "[2, 11, 36]\n",
      "[2, 11, 106]\n",
      "tensor([  0,   3,   6,   9,  12,  15,  18,  21,  24,  27,  30,  33,  36,  39,\n",
      "         42,  45,  48,  51,  54,  57,  60,  63,  66,  69,  72,  75,  78,  81,\n",
      "         84,  87,  90,  93,  96,  99, 102, 105])\n",
      "2\n",
      "[2, 8, 37]\n",
      "[2, 8, 73]\n",
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n",
      "        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70,\n",
      "        72])\n",
      "2\n",
      "[3, 4, 43]\n",
      "[3, 4, 85]\n",
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n",
      "        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62, 64, 66, 68, 70,\n",
      "        72, 74, 76, 78, 80, 82, 84])\n",
      "2\n",
      "[2, 2, 32]\n",
      "[2, 2, 63]\n",
      "tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18, 20, 22, 24, 26, 28, 30, 32, 34,\n",
      "        36, 38, 40, 42, 44, 46, 48, 50, 52, 54, 56, 58, 60, 62])\n",
      "4\n",
      "[6, 1, 13]\n",
      "[6, 1, 49]\n",
      "tensor([ 0,  4,  8, 12, 16, 20, 24, 28, 32, 36, 40, 44, 48])\n",
      "4\n",
      "[6, 6, 37]\n",
      "[6, 6, 145]\n",
      "tensor([  0,   4,   8,  12,  16,  20,  24,  28,  32,  36,  40,  44,  48,  52,\n",
      "         56,  60,  64,  68,  72,  76,  80,  84,  88,  92,  96, 100, 104, 108,\n",
      "        112, 116, 120, 124, 128, 132, 136, 140, 144])\n",
      "All tests in `test_conv_transpose1d` passed!\n"
     ]
    }
   ],
   "source": [
    "# def conv_transpose1d_minimal(x: t.Tensor, weights: t.Tensor) -> t.Tensor:\n",
    "#     '''Like torch's conv_transpose1d using bias=False and all other keyword arguments left at their default values.\n",
    "\n",
    "#     x: shape (batch, in_channels, width)\n",
    "#     weights: shape (in_channels, out_channels, kernel_width)\n",
    "\n",
    "#     Returns: shape (batch, out_channels, output_width)\n",
    "#     '''\n",
    "\n",
    "#     \"\"\"\n",
    "#     x: shape (batch, in_channels, width)\n",
    "#     weights: shape (out_channels, in_channels, kernel_width)\n",
    "#     \"\"\"\n",
    "#     _, _, width = x.shape\n",
    "#     _, _, kernel_width = weights.shape\n",
    "#     # add padding\n",
    "#     padlen = kernel_width - 1\n",
    "#     new_width = width + 2 * padlen\n",
    "#     # reverse weights\n",
    "#     reversed_weights = weights.flip(-1)\n",
    "\n",
    "#     import torch.nn.functional as F\n",
    "#     # now we expand to size (7, 11) by appending a row of 0s at pos 0 and pos 6, \n",
    "#     # and a column of 0s at pos 10\n",
    "#     padded_x = F.pad(input=x, pad=(padlen, padlen, 0, 0, 0, 0), mode='constant', value=0)\n",
    "#     reversed_weights = rearrange(reversed_weights, 'in out w -> out in w')\n",
    "#     return conv1d_minimal(padded_x, reversed_weights)\n",
    "\n",
    "\n",
    "def conv_transpose1d(x, weights, stride: int = 1, padding: int = 0) -> t.Tensor:\n",
    "    '''Like torch's conv_transpose1d using bias=False and all other keyword arguments left at their default values.\n",
    "\n",
    "    x: shape (batch, in_channels, width)\n",
    "    weights: shape (in_channels, out_channels, kernel_width)\n",
    "\n",
    "    Returns: shape (batch, out_channels, output_width)\n",
    "    '''\n",
    "    _, _, width = x.shape\n",
    "    _, _, kernel_width = weights.shape\n",
    "    # add padding\n",
    "    padlen = kernel_width - 1 - padding\n",
    "    x_frac = fractional_stride_1d(x, stride)\n",
    "    padded_x = F.pad(input=x_frac, pad=(padlen, padlen, 0, 0, 0, 0), mode='constant', value=0)\n",
    "    reversed_weights = weights.flip(-1)\n",
    "    reversed_weights = rearrange(reversed_weights, 'in out w -> out in w')\n",
    "    \n",
    "    return conv1d_minimal(padded_x, reversed_weights)\n",
    "    \n",
    "    \n",
    "\n",
    "w5d1_tests.test_conv_transpose1d(conv_transpose1d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
