{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from einops import rearrange\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from einops import rearrange\n",
    "import wandb\n",
    "from fancy_einsum import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = print\n",
    "BDICT = {\n",
    "    '(': ')',\n",
    "    '[': ']',\n",
    "    '{': '}',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bracket_string(maxsize):\n",
    "    bracket_string = ''\n",
    "    stack = []\n",
    "    for _ in range(int(maxsize * 0.9)):\n",
    "        if len(stack) == 0 or t.rand(1) < 0.5: # put new bracket\n",
    "            bracket = random.choice(list(BDICT.keys()))\n",
    "            stack.append(BDICT[bracket])\n",
    "        else:\n",
    "            bracket = stack.pop()\n",
    "        bracket_string += bracket\n",
    "    while len(stack) > 0:\n",
    "        bracket_string += stack.pop()\n",
    "    if len(bracket_string) > maxsize:\n",
    "        return generate_bracket_string(maxsize)\n",
    "    else:\n",
    "        return bracket_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average len: 465.812\n"
     ]
    }
   ],
   "source": [
    "# mean with these params is 517.3\n",
    "BLEN = 500\n",
    "ITERS = 1000\n",
    "p(f'Average len: {sum([len(generate_bracket_string(BLEN)) for _ in range(ITERS)])/ITERS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValid(s: str) -> bool:\n",
    "        stack = []\n",
    "        for char in s:\n",
    "            try:\n",
    "                if char == '(':\n",
    "                    stack.append(char)\n",
    "                elif char == ')':\n",
    "                    if stack.pop() != '(':\n",
    "                        return False\n",
    "                elif char == '[':\n",
    "                    stack.append(char)\n",
    "                elif char == ']':\n",
    "                    if stack.pop() != '[':\n",
    "                        return False\n",
    "                elif char == '{':\n",
    "                    stack.append(char)\n",
    "                elif char == '}':\n",
    "                    if stack.pop() != '{':\n",
    "                        return False\n",
    "                else:\n",
    "                    raise Exception('Invalid character')\n",
    "            except:\n",
    "                return False\n",
    "        return len(stack) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_invalid_bracket_string(size):\n",
    "    bracket_string = generate_bracket_string(size)\n",
    "    # change random brackets to invalid\n",
    "    corrupt_size = random.randint(1, max(1, int(size * 0.1)))\n",
    "    bracketlist = list('()[]{}')\n",
    "    for _ in range(corrupt_size):\n",
    "        bracket_string = bracket_string.replace(random.choice(bracketlist), random.choice(bracketlist))\n",
    "\n",
    "    if isValid(bracket_string):\n",
    "        return make_invalid_bracket_string(size)\n",
    "    else:\n",
    "        return bracket_string\n",
    "\n",
    "assert [isValid(make_invalid_bracket_string(10)) for i in range(1000)].count(False) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with bracket strings\n",
    "# and labels for each bracket\n",
    "\n",
    "# tokenizer for brackets\n",
    "class BracketTokenizer:\n",
    "    def __init__(self, vocab, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.vocab_inv = {v: k for k, v in vocab.items()}\n",
    "        self.pad_token_id = self.vocab['[PAD]']\n",
    "        self.cls_token_id = self.vocab['[CLS]']\n",
    "        self.sep_token_id = self.vocab['[SEP]']\n",
    "        self.mask_token_id = self.vocab['[MASK]']\n",
    "        self.unk_token_id = self.vocab['[UNK]']\n",
    "        self.vocab_inv = {v: k for k, v in vocab.items()}\n",
    "        self.vocab_inv[self.pad_token_id] = '[PAD]'\n",
    "        self.vocab_inv[self.cls_token_id] = '[CLS]'\n",
    "        self.vocab_inv[self.sep_token_id] = '[SEP]'\n",
    "        self.vocab_inv[self.mask_token_id] = '[MASK]'\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.vocab_inv[id] for id in ids]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.encode(text)\n",
    "\n",
    "    def encode(self, text):\n",
    "        assert len(text) + 2 <= self.maxlen\n",
    "        ids = self.convert_tokens_to_ids(text)\n",
    "        ids = [self.cls_token_id] + ids + [self.sep_token_id] # add cls and sep tokens\n",
    "        ids = ids + [self.pad_token_id] * (self.maxlen - len(ids)) # pad to max len\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if type(ids) == t.Tensor:\n",
    "            ids = ids.tolist()\n",
    "        ids = [id for id in ids if id != self.pad_token_id]\n",
    "        # remove cls and sep tokens if present\n",
    "        if ids[0] == self.cls_token_id:\n",
    "            ids = ids[1:]\n",
    "        if ids[-1] == self.sep_token_id:\n",
    "            ids = ids[:-1]\n",
    "        tokens = ''.join(self.convert_ids_to_tokens(ids))\n",
    "        return tokens\n",
    "\n",
    "vocab = {'(': 0, ')': 1, '[': 2, ']': 3, '{': 4, '}': 5, '[PAD]': 6, '[CLS]': 7, '[SEP]': 8, '[MASK]': 9, '[UNK]': 10}\n",
    "simplevocab = {'(': 0, ')': 1, '[PAD]': 2, '[CLS]': 3, '[SEP]': 4, '[MASK]': 5, '[UNK]': 6}\n",
    "tokenizer = BracketTokenizer(vocab, maxlen=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()[()]{{}}{}\n",
      "()[]{}[][][]\n",
      "(([]))[][[[]]]\n",
      "{}[]{}{(){}}\n",
      "{{{{}}{[]}}}\n",
      "(()(()))()()\n",
      "(({[]}))()[()]\n",
      "[](([[()]]()))\n",
      "(([{}{}()])())\n",
      "()[]()[]{[]()}\n"
     ]
    }
   ],
   "source": [
    "bracketss = [generate_bracket_string(tokenizer.maxlen-2) for _ in range(10)]\n",
    "for brackets in bracketss:\n",
    "    print(brackets)\n",
    "    assert len(tokenizer(brackets)) == tokenizer.maxlen\n",
    "    assert tokenizer.decode(tokenizer(brackets)) == brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BracketDataset(TensorDataset):\n",
    "    def __init__(self, size, tokenizer: BracketTokenizer, validfrac=0.7):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.validfrac = validfrac\n",
    "        self.rng = random.Random(42)\n",
    "        self.train = self._make_dataset()\n",
    "        super().__init__(*self.train)\n",
    "    \n",
    "    def _make_dataset(self):\n",
    "        validsize = int(self.size * self.validfrac)\n",
    "        invalidsize = self.size - validsize\n",
    "        randsize = lambda: 2 * self.rng.randint(2, self.tokenizer.maxlen // 2) - 2\n",
    "        valid_bracket_strings = [self.tokenizer(generate_bracket_string(randsize())) for _ in zip(range(validsize))]\n",
    "        valid_bracket_labels = [1] * validsize\n",
    "        invalid_bracket_strings = [self.tokenizer(make_invalid_bracket_string(randsize())) for _ in range(invalidsize)]\n",
    "        invalid_bracket_labels = [0] * invalidsize\n",
    "        \n",
    "        bracket_strings = valid_bracket_strings + invalid_bracket_strings\n",
    "        bracket_labels = valid_bracket_labels + invalid_bracket_labels\n",
    "        # shuffle\n",
    "        zipped = list(zip(bracket_strings, bracket_labels))\n",
    "        self.rng.shuffle(zipped)\n",
    "        bracket_strings, bracket_labels = zip(*zipped)\n",
    "        # to tensor\n",
    "        bracket_strings = t.tensor(bracket_strings, dtype=t.long)\n",
    "        bracket_labels = t.tensor(bracket_labels, dtype=t.long)\n",
    "        return bracket_strings, bracket_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = BracketDataset(size=4096, tokenizer=tokenizer)\n",
    "testset = BracketDataset(size=512, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in trainset:\n",
    "    assert isValid(tokenizer.decode(x)) == y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([][]))})} 0\n",
      "{()}{()()} 1\n",
      "{} 1\n",
      "[][(())] 1\n",
      "[{}]({}()) 1\n",
      "))]()]()]{}) 0\n",
      "]}[][]]} 0\n",
      "()[]{}[{}] 1\n",
      "(){[][ 0\n",
      "({}) 1\n",
      "[](){}{}{{}{}} 1\n",
      "{[)}()[)[) 0\n",
      "(({[]((}(((( 0\n",
      "{}()[] 1\n",
      "[[]](){)() 0\n",
      "[{}]()({{}}) 1\n",
      "()()() 1\n",
      "[{}]{} 1\n",
      "({[{}]})[[]] 1\n",
      "[]()[]{{}[]} 1\n",
      "()()[]([]) 1\n",
      "())})[]}() 0\n",
      "({{[]}})() 1\n",
      "()[] 1\n",
      "[()[ 0\n",
      "[][()] 1\n",
      "[][{}]}) 0\n",
      "[](()) 1\n",
      "{}{}({}){} 1\n",
      "{}{{[]}}() 1\n",
      "[]({}){}({}) 1\n",
      "(}[] 0\n",
      "([({})()][]) 1\n",
      "()[()] 1\n",
      "[]{}{[]()} 1\n",
      "[] 1\n",
      "{()( 0\n",
      "])])]){}[]{])} 0\n",
      "({})(()[[]()]) 1\n",
      "(){] 0\n",
      "{([[]])}[] 1\n",
      "[[][(())]] 1\n",
      "({}) 1\n",
      "[]{}{}{}{} 1\n",
      "{}[[}} 0\n",
      "(()) 1\n",
      "[] 1\n",
      "() 1\n",
      "({[] 0\n",
      "()[] 1\n",
      "{} 1\n"
     ]
    }
   ],
   "source": [
    "for i, (x, y) in enumerate(trainset):\n",
    "    print(tokenizer.decode(x), y.item())\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 16])\n"
     ]
    }
   ],
   "source": [
    "for (x, y) in trainloader:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Transformer Modules\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hidden_size, num_heads = config.hidden_size, config.num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        Q, K, V = self.W_Q(x), self.W_K(x), self.W_V(x)\n",
    "        att = self.multihead_masked_attention(Q, K, V, self.num_heads, additive_attention_mask)\n",
    "        return self.W_O(att)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int, additive_attention_mask: Optional[t.Tensor]):\n",
    "        '''\n",
    "        Q: shape (b, s1, e)\n",
    "        K: shape (b, s2, e)\n",
    "        V: shape (b, s2, e)\n",
    "\n",
    "        e = nheads * h\n",
    "        b = batch\n",
    "        s = seq_len\n",
    "        h = hidden\n",
    "\n",
    "        Return: shape (b s e)\n",
    "        '''\n",
    "\n",
    "        assert Q.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] % n_heads == 0\n",
    "        assert V.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "        Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "        batch, nheads, seq_len, headsize = Q.shape\n",
    "\n",
    "        scaled_dot_prod = einsum('b nheads sk h, b nheads sq h -> b nheads sq sk', K, Q) / (headsize ** 0.5)\n",
    "        if additive_attention_mask is not None:\n",
    "            scaled_dot_prod += additive_attention_mask # (batch, 1, 1, sk)\n",
    "        attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "        attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "        attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "        return attention\n",
    "\n",
    "class BERTMLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs, p_dropout = config.hidden_size, config.dropout\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(hs, hs * 4)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(hs * 4, hs)),   \n",
    "            ('dropout', nn.Dropout(p_dropout))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class BERTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = BERTMLP(config)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        additive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        '''\n",
    "        h1 = self.ln1(self.attention(x, additive_attention_mask) + x) # TODO chain this\n",
    "        h2 = self.ln2(self.mlp(h1) + h1)\n",
    "        return h2\n",
    "\n",
    "def make_additive_attention_mask(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "    '''\n",
    "    one_zero_attention_mask: \n",
    "        shape (batch, seq)\n",
    "        Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "    big_negative_number:\n",
    "        Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "    Out: \n",
    "        shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        Contains 0 if attention is allowed, big_negative_number if not.\n",
    "    '''\n",
    "    return rearrange((1 - one_zero_attention_mask) * big_negative_number, 'batch seq -> batch 1 1 seq')\n",
    "\n",
    "class BertCommon(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        self.tokentype_emb = nn.Embedding(2, config.hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(config.dropout)\n",
    "        self.bertblocks = nn.ModuleList([BERTBlock(config) for i in range(config.num_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        token_embedding = self.token_emb(input_ids) # (b, seq_len, emb)\n",
    "        batch, seq_len = input_ids.shape\n",
    "        positional_embedding = self.pos_emb(t.arange(seq_len, device=input_ids.device)) # (seq_len, emb)\n",
    "        token_type_ids = token_type_ids if token_type_ids else t.zeros_like(input_ids)\n",
    "        token_type_embedding = self.tokentype_emb(token_type_ids) # (b, seq_len, emb)\n",
    "        x = self.dropout1(self.ln1(token_embedding + positional_embedding + token_type_embedding))\n",
    "        mask = make_additive_attention_mask(one_zero_attention_mask) if one_zero_attention_mask is not None else None\n",
    "        for block in self.bertblocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "class BertLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs = config.hidden_size\n",
    "        self.bertcommon = BertCommon(config)\n",
    "        self.linear = nn.Linear(hs * config.max_seq_len, 2)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        xavier = 1 / (config.vocab_size ** 0.5)\n",
    "        self.unembed_bias = nn.parameter.Parameter(t.randn(config.vocab_size) * 2 * xavier - xavier) # N(-xavier, xavier)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        x = self.bertcommon(input_ids, one_zero_attention_mask, token_type_ids)\n",
    "        # just finish with a binary classification\n",
    "        # torch.Size([64, 512, 768]) torch.Size([768, 11])\n",
    "        # flatten the sequence dimension\n",
    "        x = rearrange(x, 'batch seq emb -> batch (seq emb)')\n",
    "        x = self.linear(x)\n",
    "        # x = self.gelu(x)\n",
    "        # x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "bertconfig = TransformerConfig(\n",
    "    num_layers = 2,\n",
    "    num_heads = 4,\n",
    "    vocab_size = len(vocab),\n",
    "    hidden_size = 100,\n",
    "    max_seq_len = tokenizer.maxlen,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "\n",
    "my_bert = BertLanguageModel(bertconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model: BertLanguageModel, config_dict: dict) -> t.optim.AdamW:\n",
    "    '''\n",
    "    Loop over model parameters and form two parameter groups:\n",
    "\n",
    "    - The first group includes the weights of each Linear layer and uses the weight decay in config_dict\n",
    "    - The second has all other parameters and uses weight decay of 0\n",
    "    '''\n",
    "    params1 = []\n",
    "    params2 = []\n",
    "    matches = ['W_O.weight', 'W_V.weight', 'W_Q.weight', 'W_K.weight', 'linear1.weight', 'linear2.weight', 'linear.weight']\n",
    "    for name, param in model.named_parameters():\n",
    "        if any([match in name for match in matches]):\n",
    "            params1.append(param)\n",
    "        else:\n",
    "            params2.append(param)\n",
    "    \n",
    "    # \n",
    "    # \n",
    "    params = [\n",
    "        {'params': params1, 'weight_decay': config_dict['weight_decay']},\n",
    "        {'params': params2, 'weight_decay': 0, **config_dict}\n",
    "    ]\n",
    "    return t.optim.AdamW(params, lr=config_dict['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_for_step(step: int, max_step: int, max_lr: float, warmup_step_frac: float):\n",
    "    '''\n",
    "    The authors used learning rate warmup from an unspecified value and an unspecified shape to a maximum of 1e-4 for the first 10,000 steps out of 1 million, and then linearly decayed to an unspecified value.\n",
    "\n",
    "    From the repo, we can see in optimization.py that AdamW is used for the optimizer, that the warmup is linear and that the epsilon used for AdamW is 1e-6.\n",
    "\n",
    "    Assume that the initial learning rate and the final learning rate are both 1/10th of the maximum, and that we want to warm-up for 1% of the total number of steps.\n",
    "    Return the learning rate for use at this step of training.'''\n",
    "    warmup_steps = int(max_step * warmup_step_frac)\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * step / warmup_steps\n",
    "    else:\n",
    "        return max_lr * (max_step - step) / (max_step - warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds: t.Tensor, targets: t.Tensor) -> float:\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    return (preds == targets).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 1.6069:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((( tensor([[-1.9332,  2.3217]])\n",
      "))(( tensor([[-2.2118,  2.5835]])\n",
      "()() tensor([[-2.1859,  2.3808]])\n",
      ")) tensor([[-1.9444,  2.4374]])\n",
      "() tensor([[-2.0990,  2.7319]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.5983: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:01<00:00, 44.95it/s]\n",
      "Loss 0.5518:   6%|â–‹         | 4/64 [00:00<00:01, 33.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((( tensor([[ 0.1983, -0.0083]])\n",
      "))(( tensor([[ 0.7751, -0.6380]])\n",
      "()() tensor([[-0.4450,  0.3301]])\n",
      ")) tensor([[ 0.5242, -0.5141]])\n",
      "() tensor([[-0.1597, -0.0525]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.1941: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:01<00:00, 44.58it/s]\n",
      "Loss 0.1922:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:01, 45.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((( tensor([[ 3.9170, -2.9281]])\n",
      "))(( tensor([[ 2.9779, -1.4640]])\n",
      "()() tensor([[-1.2837,  1.5322]])\n",
      ")) tensor([[ 4.1218, -2.5658]])\n",
      "() tensor([[-1.5344,  1.5978]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0101: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:01<00:00, 45.15it/s]\n",
      "Loss 0.0125:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:01, 44.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((( tensor([[ 5.0696, -4.8935]])\n",
      "))(( tensor([[-2.5230,  3.0130]])\n",
      "()() tensor([[-5.5734,  5.5346]])\n",
      ")) tensor([[ 3.5120, -2.6335]])\n",
      "() tensor([[-6.1492,  6.3551]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0005: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:01<00:00, 44.98it/s]\n",
      "Loss 0.0079:  16%|â–ˆâ–Œ        | 10/64 [00:00<00:01, 44.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(((( tensor([[ 5.2483, -5.4290]])\n",
      "))(( tensor([[-1.8967,  2.8358]])\n",
      "()() tensor([[-5.7074,  5.7196]])\n",
      ")) tensor([[ 5.8669, -5.6101]])\n",
      "() tensor([[-6.7756,  6.5710]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0010: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:01<00:00, 43.34it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import requests\n",
    "def flat(x: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Combines batch and sequence dimensions.\"\"\"\n",
    "    return rearrange(x, \"b s ... -> (b s) ...\")\n",
    "\n",
    "def bert_mlm_pretrain(model: BertLanguageModel, config_dict: dict, train_loader: DataLoader) -> None:\n",
    "    '''Train using masked language modelling.'''\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    opt = make_optimizer(model, config_dict)\n",
    "    # settings=wandb.Settings(start_method=\"fork\"))\n",
    "    # wandb.init(project=\"bert-brackets\", config=config_dict)\n",
    "    # wandb.watch(model)\n",
    "    run_name = wandb.run.name if wandb.run else 'bert-brackets'\n",
    "    # tqdm progress bar of train loader annotated with epoch number\n",
    "    # os.makedirs(f\"models/{run_name}\")\n",
    "    # t.save(model.state_dict(), f\"./models/{run_name}/{run_name}-e-1.pt\")\n",
    "    \n",
    "    for epoch in range(config_dict['epochs']):\n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        for n_batch, (batch, target) in enumerate(progress_bar):\n",
    "            # print(batch[0], target[0])\n",
    "\n",
    "            # with t.inference_mode():\n",
    "            #     print(f'{target.sum() / target.numel()}')\n",
    "            # wandb.log({\"epoch\": epoch, \"batch\": n_batch})\n",
    "            batch = batch.to(device)\n",
    "            opt.zero_grad()\n",
    "            # lr = lr_for_step(\n",
    "            #     n_batch + epoch * len(train_loader),\n",
    "            #     max_step=int(len(train_loader) * config_dict[\"epochs\"]),\n",
    "            #     max_lr=config_dict[\"lr\"],\n",
    "            #     warmup_step_frac=config_dict[\"warmup_step_frac\"],\n",
    "            # )\n",
    "            # for param_group in opt.param_groups:\n",
    "            #     param_group[\"lr\"] = lr\n",
    "            # masked_input_ids, mask = random_mask(batch, tokenizer.mask_token_id, tokenizer.vocab_size)\n",
    "            # masked_input_ids = masked_input_ids.to(device)\n",
    "            # mask = mask.to(device)\n",
    "            mask = (batch != tokenizer.pad_token_id).float()\n",
    "            mask.requires_grad = False\n",
    "            logits = model(batch.to(device), mask, token_type_ids=None)\n",
    "            # print(batch.shape)\n",
    "            # print(logits.shape)\n",
    "            # print(target.shape)\n",
    "            # use inference mode below to get accuracy\n",
    "            # with t.no_grad():\n",
    "            #     for x, y in testloader:\n",
    "            #         x = x.to(device)\n",
    "            #         y = y.to(device)\n",
    "            #         m = (x != tokenizer.pad_token_id).float()\n",
    "            #         logits = model(x, m, token_type_ids=None)\n",
    "            #         acc = accuracy(logits, y)\n",
    "            #         wandb.log({\"test_acc\": acc})\n",
    "            #         break\n",
    "            loss = F.cross_entropy(logits, target.to(device).long())\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # wandb.log({\"loss\": loss, \"lr\": lr})\n",
    "            opt.step()\n",
    "            progress_bar.set_description(f\"Loss {loss.item():.4f}\")\n",
    "            if n_batch % 100 == 0:\n",
    "                with t.inference_mode():\n",
    "                    tests = [\n",
    "                        \"((((\",\n",
    "                        \"))((\",\n",
    "                        \"()()\",\n",
    "                        \"))\",\n",
    "                        \"()\"\n",
    "                    ]\n",
    "                    tensors = [t.tensor(tokenizer(s)).unsqueeze(0).to(device) for s in tests]\n",
    "                    losses = [model(ts, (ts != tokenizer.pad_token_id).float(), token_type_ids=None) for ts in tensors]\n",
    "                    [print(f\"{s} {l}\") for s, l in zip(tests, losses)]\n",
    "                    progress_bar.set_description(f\"Loss {loss.item():.4f}\")\n",
    "\n",
    "                    \n",
    "        # t.save(model.state_dict(), f\"./models/{run_name}/{run_name}-e{epoch}.pt\")\n",
    "    # wandb.finish()\n",
    "    requests.post(\"https://ntfy.sh/arena-brackets\", data=f\"Done training {run_name} ðŸŽ‰\".encode(encoding='utf-8'))\n",
    "\n",
    "config_dict = dict(\n",
    "    lr=0.001,\n",
    "    epochs=5,\n",
    "    batch_size=64*4,\n",
    "    weight_decay=0.01,\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    warmup_step_frac=0.01,\n",
    "    eps=1e-06,\n",
    "    max_grad_norm=None,\n",
    ")\n",
    "\n",
    "bert_mlm_pretrain(my_bert, config_dict, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 1, 0, 8, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6]])\n",
      "tensor([[-3.4914,  3.9164]])\n"
     ]
    }
   ],
   "source": [
    "with t.inference_mode():\n",
    "    s = \")(\"\n",
    "    tens = t.tensor(tokenizer(s)).unsqueeze(0).to(device)\n",
    "    print(tens)\n",
    "    print(my_bert(tens, (tens != tokenizer.pad_token_id).float(), token_type_ids=None))\n",
    "    # for batch, target in testloader:\n",
    "    #     batch = batch.to(device)\n",
    "    #     target = target.to(device)\n",
    "    #     mask = (batch != tokenizer.pad_token_id).float()\n",
    "    #     mask.requires_grad = False\n",
    "    #     logits = my_bert(batch, mask, token_type_ids=None)\n",
    "    #     print(accuracy(logits, target))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
