{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from einops import rearrange\n",
    "from torch.nn import functional as F\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from einops import rearrange\n",
    "import wandb\n",
    "from fancy_einsum import einsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_bracket_string(maxsize):\n",
    "    BDICT = {\n",
    "        '(': ')',\n",
    "        '[': ']',\n",
    "        '{': '}',\n",
    "    }\n",
    "    bracket_string = ''\n",
    "    stack = []\n",
    "    for _ in range(int(maxsize * 0.9)):\n",
    "        if len(stack) == 0 or t.rand(1) < 0.5: # put new bracket\n",
    "            bracket = random.choice(list(BDICT.keys()))\n",
    "            stack.append(BDICT[bracket])\n",
    "        else:\n",
    "            bracket = stack.pop()\n",
    "        bracket_string += bracket\n",
    "    while len(stack) > 0:\n",
    "        bracket_string += stack.pop()\n",
    "    if len(bracket_string) > maxsize:\n",
    "        return generate_bracket_string(maxsize)\n",
    "    else:\n",
    "        return bracket_string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average len: 466.228\n"
     ]
    }
   ],
   "source": [
    "# mean with these params is 517.3\n",
    "BLEN = 500\n",
    "ITERS = 1000\n",
    "p(f'Average len: {sum([len(generate_bracket_string(BLEN)) for _ in range(ITERS)])/ITERS}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{}[{()}][]'"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_bracket_string(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isValid(s: str) -> bool:\n",
    "        stack = []\n",
    "        for char in s:\n",
    "            try:\n",
    "                if char == '(':\n",
    "                    stack.append(char)\n",
    "                elif char == ')':\n",
    "                    if stack.pop() != '(':\n",
    "                        return False\n",
    "                elif char == '[':\n",
    "                    stack.append(char)\n",
    "                elif char == ']':\n",
    "                    if stack.pop() != '[':\n",
    "                        return False\n",
    "                elif char == '{':\n",
    "                    stack.append(char)\n",
    "                elif char == '}':\n",
    "                    if stack.pop() != '{':\n",
    "                        return False\n",
    "                else:\n",
    "                    raise Exception('Invalid character')\n",
    "            except:\n",
    "                return False\n",
    "        return len(stack) == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_invalid_bracket_string(size):\n",
    "    bracket_string = generate_bracket_string(size)\n",
    "    # change random brackets to invalid\n",
    "    corrupt_size = random.randint(1, max(1, int(size * 0.1)))\n",
    "    for _ in range(corrupt_size):\n",
    "        bracket_string = bracket_string.replace(random.choice(list('()[]{}')), random.choice(list('()[]{}')), 1)\n",
    "\n",
    "    if isValid(bracket_string):\n",
    "        return make_invalid_bracket_string(size)\n",
    "    else:\n",
    "        return bracket_string\n",
    "\n",
    "assert [isValid(make_invalid_bracket_string(10)) for i in range(1000)].count(False) == 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with bracket strings\n",
    "# and labels for each bracket\n",
    "\n",
    "# tokenizer for brackets\n",
    "class BracketTokenizer:\n",
    "    def __init__(self, vocab, maxlen):\n",
    "        self.maxlen = maxlen\n",
    "        self.vocab = vocab\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.vocab_inv = {v: k for k, v in vocab.items()}\n",
    "        self.pad_token_id = self.vocab['[PAD]']\n",
    "        self.cls_token_id = self.vocab['[CLS]']\n",
    "        self.sep_token_id = self.vocab['[SEP]']\n",
    "        self.mask_token_id = self.vocab['[MASK]']\n",
    "        self.unk_token_id = self.vocab['[UNK]']\n",
    "        self.vocab_inv = {v: k for k, v in vocab.items()}\n",
    "        self.vocab_inv[self.pad_token_id] = '[PAD]'\n",
    "        self.vocab_inv[self.cls_token_id] = '[CLS]'\n",
    "        self.vocab_inv[self.sep_token_id] = '[SEP]'\n",
    "        self.vocab_inv[self.mask_token_id] = '[MASK]'\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self.vocab[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, ids):\n",
    "        return [self.vocab_inv[id] for id in ids]\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.encode(text)\n",
    "\n",
    "    def encode(self, text):\n",
    "        assert len(text) + 2 <= self.maxlen\n",
    "        ids = self.convert_tokens_to_ids(text)\n",
    "        ids = [self.cls_token_id] + ids + [self.sep_token_id] # add cls and sep tokens\n",
    "        ids = ids + [self.pad_token_id] * (self.maxlen - len(ids)) # pad to max len\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if type(ids) == t.Tensor:\n",
    "            ids = ids.tolist()\n",
    "        ids = [id for id in ids if id != self.pad_token_id]\n",
    "        # remove cls and sep tokens if present\n",
    "        if ids[0] == self.cls_token_id:\n",
    "            ids = ids[1:]\n",
    "        if ids[-1] == self.sep_token_id:\n",
    "            ids = ids[:-1]\n",
    "        tokens = ''.join(self.convert_ids_to_tokens(ids))\n",
    "        return tokens\n",
    "\n",
    "vocab = {'(': 0, ')': 1, '[': 2, ']': 3, '{': 4, '}': 5, '[PAD]': 6, '[CLS]': 7, '[SEP]': 8, '[MASK]': 9, '[UNK]': 10}\n",
    "simplevocab = {'(': 0, ')': 1, '[PAD]': 2, '[CLS]': 3, '[SEP]': 4, '[MASK]': 5, '[UNK]': 6}\n",
    "tokenizer = BracketTokenizer(vocab, maxlen=6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]{}\n",
      "[[]]\n",
      "[]{}\n",
      "(){}\n",
      "(){}\n",
      "[()]\n",
      "[]()\n",
      "{}[]\n",
      "[()]\n",
      "()()\n"
     ]
    }
   ],
   "source": [
    "bracketss = [generate_bracket_string(4) for _ in range(10)]\n",
    "for brackets in bracketss:\n",
    "    print(brackets)\n",
    "    assert len(tokenizer(brackets)) == 6\n",
    "    assert tokenizer.decode(tokenizer(brackets)) == brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BracketDataset(TensorDataset):\n",
    "    def __init__(self, size, tokenizer: BracketTokenizer, validfrac=0.7):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.validfrac = validfrac\n",
    "        self.rng = random.Random(42)\n",
    "        self.train = self._make_dataset()\n",
    "        super().__init__(*self.train)\n",
    "    \n",
    "    def _make_dataset(self):\n",
    "        validsize = int(self.size * self.validfrac)\n",
    "        invalidsize = self.size - validsize\n",
    "        valid_bracket_strings = [self.tokenizer(generate_bracket_string(tokenizer.maxlen-2)) for _ in range(validsize)]\n",
    "        valid_bracket_labels = [1] * validsize\n",
    "        invalid_bracket_strings = [self.tokenizer(make_invalid_bracket_string(tokenizer.maxlen-2)) for _ in range(invalidsize)]\n",
    "        invalid_bracket_labels = [0] * invalidsize\n",
    "        \n",
    "        bracket_strings = valid_bracket_strings + invalid_bracket_strings\n",
    "        bracket_labels = valid_bracket_labels + invalid_bracket_labels\n",
    "        # shuffle\n",
    "        zipped = list(zip(bracket_strings, bracket_labels))\n",
    "        self.rng.shuffle(zipped)\n",
    "        bracket_strings, bracket_labels = zip(*zipped)\n",
    "        # to tensor\n",
    "        bracket_strings = t.tensor(bracket_strings, dtype=t.long)\n",
    "        bracket_labels = t.tensor(bracket_labels, dtype=t.long)\n",
    "        return bracket_strings, bracket_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = BracketDataset(size=4096, tokenizer=tokenizer)\n",
    "testset = BracketDataset(size=512, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x,y in trainset:\n",
    "    assert isValid(tokenizer.decode(x)) == y.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (x, y) in enumerate(trainset):\n",
    "    print(tokenizer.decode(x), y.item())\n",
    "    if i == 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader = DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "testloader = DataLoader(testset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 6])\n"
     ]
    }
   ],
   "source": [
    "for (x, y) in trainloader:\n",
    "    print(x.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Transformer Modules\n",
    "@dataclass(frozen=True)\n",
    "class TransformerConfig:\n",
    "    '''Constants used throughout your decoder-only transformer model.'''\n",
    "\n",
    "    num_layers: int\n",
    "    num_heads: int\n",
    "    vocab_size: int\n",
    "    hidden_size: int # also embedding dim or d_model\n",
    "    max_seq_len: int = 5000 \n",
    "    dropout: float = 0.1\n",
    "    layer_norm_epsilon: float = 1e-05\n",
    "    device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hidden_size, num_heads = config.hidden_size, config.num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.W_Q = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_K = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_V = nn.Linear(hidden_size, hidden_size)\n",
    "        self.W_O = nn.Linear(hidden_size, hidden_size)\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        Q, K, V = self.W_Q(x), self.W_K(x), self.W_V(x)\n",
    "        att = self.multihead_masked_attention(Q, K, V, self.num_heads, additive_attention_mask)\n",
    "        return self.W_O(att)\n",
    "\n",
    "    def multihead_masked_attention(self, Q: t.Tensor, K: t.Tensor, V: t.Tensor, n_heads: int, additive_attention_mask: Optional[t.Tensor]):\n",
    "        '''\n",
    "        Q: shape (b, s1, e)\n",
    "        K: shape (b, s2, e)\n",
    "        V: shape (b, s2, e)\n",
    "\n",
    "        e = nheads * h\n",
    "        b = batch\n",
    "        s = seq_len\n",
    "        h = hidden\n",
    "\n",
    "        Return: shape (b s e)\n",
    "        '''\n",
    "\n",
    "        assert Q.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] % n_heads == 0\n",
    "        assert V.shape[-1] % n_heads == 0\n",
    "        assert K.shape[-1] == V.shape[-1]\n",
    "\n",
    "        Q = rearrange(Q, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        K = rearrange(K, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "        V = rearrange(V, 'b s (nheads h) -> b nheads s h', nheads=n_heads)\n",
    "\n",
    "        batch, nheads, seq_len, headsize = Q.shape\n",
    "\n",
    "        scaled_dot_prod = einsum('b nheads sk h, b nheads sq h -> b nheads sq sk', K, Q) / (headsize ** 0.5)\n",
    "        if additive_attention_mask is not None:\n",
    "            scaled_dot_prod += additive_attention_mask # (batch, 1, 1, sk)\n",
    "        attention_probs = scaled_dot_prod.softmax(dim=-1)\n",
    "        attention_vals = einsum('b nheads s1 s2, b nheads s2 c -> b nheads s1 c', attention_probs, V)\n",
    "        attention = rearrange(attention_vals, 'b nheads s c -> b s (nheads c)')\n",
    "        return attention\n",
    "\n",
    "class BERTMLP(nn.Module):\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs, p_dropout = config.hidden_size, config.dropout\n",
    "        self.mlp = nn.Sequential(OrderedDict([\n",
    "            ('linear1', nn.Linear(hs, hs * 4)),\n",
    "            ('GELU', nn.GELU()),\n",
    "            ('linear2', nn.Linear(hs * 4, hs)),   \n",
    "            ('dropout', nn.Dropout(p_dropout))\n",
    "        ]))\n",
    "\n",
    "    def forward(self, x: t.Tensor):\n",
    "        return self.mlp(x)\n",
    "\n",
    "class BERTBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.attention = MultiheadAttention(config)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.mlp = BERTMLP(config)\n",
    "        self.ln2 = nn.LayerNorm(config.hidden_size)\n",
    "        \n",
    "    def forward(self, x: t.Tensor, additive_attention_mask: Optional[t.Tensor] = None) -> t.Tensor:\n",
    "        '''\n",
    "        x: shape (batch, seq, hidden_size)\n",
    "        additive_attention_mask: shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        '''\n",
    "        h1 = self.ln1(self.attention(x, additive_attention_mask) + x) # TODO chain this\n",
    "        h2 = self.ln2(self.mlp(h1) + h1)\n",
    "        return h2\n",
    "\n",
    "def make_additive_attention_mask(one_zero_attention_mask: t.Tensor, big_negative_number: float = -10000) -> t.Tensor:\n",
    "    '''\n",
    "    one_zero_attention_mask: \n",
    "        shape (batch, seq)\n",
    "        Contains 1 if this is a valid token and 0 if it is a padding token.\n",
    "\n",
    "    big_negative_number:\n",
    "        Any negative number large enough in magnitude that exp(big_negative_number) is 0.0 for the floating point precision used.\n",
    "\n",
    "    Out: \n",
    "        shape (batch, nheads=1, seqQ=1, seqK)\n",
    "        Contains 0 if attention is allowed, big_negative_number if not.\n",
    "    '''\n",
    "    return rearrange((1 - one_zero_attention_mask) * big_negative_number, 'batch seq -> batch 1 1 seq')\n",
    "\n",
    "class BertCommon(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        self.token_emb = nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.pos_emb = nn.Embedding(config.max_seq_len, config.hidden_size)\n",
    "        self.tokentype_emb = nn.Embedding(2, config.hidden_size)\n",
    "        self.ln1 = nn.LayerNorm(config.hidden_size)\n",
    "        self.dropout1 = nn.Dropout(config.dropout)\n",
    "        self.bertblocks = nn.ModuleList([BERTBlock(config) for i in range(config.num_layers)])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        token_embedding = self.token_emb(input_ids) # (b, seq_len, emb)\n",
    "        batch, seq_len = input_ids.shape\n",
    "        positional_embedding = self.pos_emb(t.arange(seq_len, device=input_ids.device)) # (seq_len, emb)\n",
    "        token_type_ids = token_type_ids if token_type_ids else t.zeros_like(input_ids)\n",
    "        token_type_embedding = self.tokentype_emb(token_type_ids) # (b, seq_len, emb)\n",
    "        x = self.dropout1(self.ln1(token_embedding + positional_embedding + token_type_embedding))\n",
    "        mask = make_additive_attention_mask(one_zero_attention_mask) if one_zero_attention_mask is not None else None\n",
    "        for block in self.bertblocks:\n",
    "            x = block(x, mask)\n",
    "        return x\n",
    "\n",
    "class BertLanguageModel(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig):\n",
    "        super().__init__()\n",
    "        hs = config.hidden_size\n",
    "        self.bertcommon = BertCommon(config)\n",
    "        self.linear = nn.Linear(hs * config.max_seq_len, 2)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.ln = nn.LayerNorm(config.hidden_size)\n",
    "        xavier = 1 / (config.vocab_size ** 0.5)\n",
    "        self.unembed_bias = nn.parameter.Parameter(t.randn(config.vocab_size) * 2 * xavier - xavier) # N(-xavier, xavier)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: t.Tensor,\n",
    "        one_zero_attention_mask: Optional[t.Tensor] = None,\n",
    "        token_type_ids: Optional[t.Tensor] = None,\n",
    "    ) -> t.Tensor:\n",
    "        '''\n",
    "        input_ids: (batch, seq) - the token ids\n",
    "        one_zero_attention_mask: (batch, seq) - only used in training, passed to `make_additive_attention_mask` and used in the attention blocks.\n",
    "        token_type_ids: (batch, seq) - only used for NSP, passed to token type embedding.\n",
    "        '''\n",
    "        x = self.bertcommon(input_ids, one_zero_attention_mask, token_type_ids)\n",
    "        # just finish with a binary classification\n",
    "        # torch.Size([64, 512, 768]) torch.Size([768, 11])\n",
    "        # flatten the sequence dimension\n",
    "        x = rearrange(x, 'batch seq emb -> batch (seq emb)')\n",
    "        x = self.linear(x)\n",
    "        # x = self.gelu(x)\n",
    "        # x = self.ln(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "bertconfig = TransformerConfig(\n",
    "    num_layers = 2,\n",
    "    num_heads = 4,\n",
    "    vocab_size = len(vocab),\n",
    "    hidden_size = 100,\n",
    "    max_seq_len = tokenizer.maxlen,\n",
    "    dropout = 0.1,\n",
    "    layer_norm_epsilon = 1e-12\n",
    ")\n",
    "\n",
    "my_bert = BertLanguageModel(bertconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_optimizer(model: BertLanguageModel, config_dict: dict) -> t.optim.AdamW:\n",
    "    '''\n",
    "    Loop over model parameters and form two parameter groups:\n",
    "\n",
    "    - The first group includes the weights of each Linear layer and uses the weight decay in config_dict\n",
    "    - The second has all other parameters and uses weight decay of 0\n",
    "    '''\n",
    "    params1 = []\n",
    "    params2 = []\n",
    "    matches = ['W_O.weight', 'W_V.weight', 'W_Q.weight', 'W_K.weight', 'linear1.weight', 'linear2.weight', 'linear.weight']\n",
    "    for name, param in model.named_parameters():\n",
    "        if any([match in name for match in matches]):\n",
    "            params1.append(param)\n",
    "        else:\n",
    "            params2.append(param)\n",
    "    \n",
    "    # \n",
    "    # \n",
    "    params = [\n",
    "        {'params': params1, 'weight_decay': config_dict['weight_decay']},\n",
    "        {'params': params2, 'weight_decay': 0, **config_dict}\n",
    "    ]\n",
    "    return t.optim.AdamW(params, lr=config_dict['lr'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_for_step(step: int, max_step: int, max_lr: float, warmup_step_frac: float):\n",
    "    '''\n",
    "    The authors used learning rate warmup from an unspecified value and an unspecified shape to a maximum of 1e-4 for the first 10,000 steps out of 1 million, and then linearly decayed to an unspecified value.\n",
    "\n",
    "    From the repo, we can see in optimization.py that AdamW is used for the optimizer, that the warmup is linear and that the epsilon used for AdamW is 1e-6.\n",
    "\n",
    "    Assume that the initial learning rate and the final learning rate are both 1/10th of the maximum, and that we want to warm-up for 1% of the total number of steps.\n",
    "    Return the learning rate for use at this step of training.'''\n",
    "    warmup_steps = int(max_step * warmup_step_frac)\n",
    "    if step < warmup_steps:\n",
    "        return max_lr * step / warmup_steps\n",
    "    else:\n",
    "        return max_lr * (max_step - step) / (max_step - warmup_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(preds: t.Tensor, targets: t.Tensor) -> float:\n",
    "    preds = preds.argmax(dim=-1)\n",
    "    return (preds == targets).float().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0001:  11%|â–ˆ         | 7/64 [00:00<00:00, 63.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 3, 2, 3, 8]) tensor(0)\n",
      ")( tensor([[ 13.9402, -13.6258]])\n",
      "(( tensor([[ 12.2055, -12.4263]])\n",
      "() tensor([[ 1.8318, -1.8004]])\n",
      "tensor([7, 0, 1, 3, 1, 8]) tensor(0)\n",
      "tensor([7, 2, 3, 5, 3, 8]) tensor(0)\n",
      "tensor([7, 2, 3, 5, 1, 8]) tensor(0)\n",
      "tensor([7, 2, 3, 5, 5, 8]) tensor(0)\n",
      "tensor([7, 0, 0, 1, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 4, 5, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 3, 5, 8]) tensor(0)\n",
      "tensor([7, 1, 1, 0, 1, 8]) tensor(0)\n",
      "tensor([7, 5, 0, 1, 3, 8]) tensor(0)\n",
      "tensor([7, 0, 0, 1, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 4, 5, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0025:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:00, 66.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 1, 5, 3, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 4, 0, 1, 5, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 2, 3, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 2, 1, 5, 8]) tensor(0)\n",
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 2, 3, 5, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0003:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:00<00:00, 67.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 3, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 3, 4, 5, 8]) tensor(0)\n",
      "tensor([7, 4, 5, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 0, 1, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 4, 5, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 4, 1, 3, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0011:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [00:00<00:00, 66.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 0, 0, 1, 3, 8]) tensor(0)\n",
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 0, 3, 3, 8]) tensor(0)\n",
      "tensor([7, 0, 4, 5, 3, 8]) tensor(0)\n",
      "tensor([7, 4, 2, 0, 5, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 3, 1, 8]) tensor(0)\n",
      "tensor([7, 4, 0, 1, 5, 8]) tensor(1)\n",
      "tensor([7, 4, 1, 2, 3, 8]) tensor(0)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 1, 5, 3, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0016: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 66.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 2, 0, 8]) tensor(0)\n",
      "tensor([7, 5, 3, 0, 1, 8]) tensor(0)\n",
      "tensor([7, 2, 0, 1, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 0, 2, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0004:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 3, 4, 1, 8]) tensor(0)\n",
      ")( tensor([[ 13.7649, -13.5611]])\n",
      "(( tensor([[ 12.1170, -13.2243]])\n",
      "() tensor([[ 3.2688, -4.2737]])\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 0, 1, 0, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 4, 1, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0016:  11%|â–ˆ         | 7/64 [00:00<00:00, 66.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 0, 4, 5, 8]) tensor(0)\n",
      "tensor([7, 0, 0, 1, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 0, 1, 5, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0026:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:00, 67.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 3, 0, 1, 8]) tensor(0)\n",
      "tensor([7, 4, 5, 0, 2, 8]) tensor(0)\n",
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 4, 5, 3, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0136:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:00, 67.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 2, 3, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 2, 3, 3, 8]) tensor(1)\n",
      "tensor([7, 3, 0, 1, 1, 8]) tensor(0)\n",
      "tensor([7, 0, 4, 1, 1, 8]) tensor(0)\n",
      "tensor([7, 2, 4, 5, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 4, 4, 5, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0004:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:00, 68.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 5, 1, 3, 8]) tensor(0)\n",
      "tensor([7, 2, 4, 2, 3, 8]) tensor(0)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 4, 5, 5, 8]) tensor(1)\n",
      "tensor([7, 4, 0, 4, 5, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0025:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:00<00:00, 67.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 5, 4, 1, 8]) tensor(0)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 2, 5, 3, 8]) tensor(0)\n",
      "tensor([7, 2, 0, 0, 1, 8]) tensor(0)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 0, 1, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0004:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 42/64 [00:00<00:00, 67.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 4, 4, 5, 8]) tensor(0)\n",
      "tensor([7, 4, 1, 2, 3, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0060:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [00:00<00:00, 67.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 5, 3, 1, 8]) tensor(0)\n",
      "tensor([7, 4, 4, 5, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 4, 5, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 3, 1, 2, 3, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0004:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 56/64 [00:00<00:00, 67.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 2, 3, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 5, 0, 1, 8]) tensor(0)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 1, 1, 0, 1, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0015: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 67.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 0, 8]) tensor(0)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0003:   0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 0, 1, 5, 8]) tensor(0)\n",
      ")( tensor([[ 13.7339, -13.8372]])\n",
      "(( tensor([[ 7.8539, -8.6239]])\n",
      "() tensor([[ 3.4062, -4.2411]])\n",
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 0, 1, 5, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 2, 3, 5, 8]) tensor(1)\n",
      "tensor([7, 4, 2, 3, 5, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0003:  11%|â–ˆ         | 7/64 [00:00<00:00, 64.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 4, 5, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 2, 3, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 0, 3, 5, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0001:  11%|â–ˆ         | 7/64 [00:00<00:00, 64.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 0, 3, 5, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 3, 0, 1, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0002:  22%|â–ˆâ–ˆâ–       | 14/64 [00:00<00:00, 66.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 2, 3, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 0, 1, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 0, 1, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0004:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:00, 66.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 1, 4, 3, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 0, 1, 2, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 3, 5, 2, 3, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0001:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:00<00:00, 66.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0002:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:00<00:00, 67.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 0, 1, 3, 8]) tensor(1)\n",
      "tensor([7, 4, 2, 3, 5, 8]) tensor(1)\n",
      "tensor([7, 0, 4, 5, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 2, 4, 3, 8]) tensor(0)\n",
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0001:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:00<00:00, 67.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 2, 1, 8]) tensor(0)\n",
      "tensor([7, 0, 1, 0, 1, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0003:  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [00:00<00:00, 67.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 2, 3, 5, 8]) tensor(0)\n",
      "tensor([7, 0, 0, 1, 3, 8]) tensor(0)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0002:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 42/64 [00:00<00:00, 67.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 2, 4, 5, 8]) tensor(0)\n",
      "tensor([7, 3, 2, 3, 1, 8]) tensor(0)\n",
      "tensor([7, 4, 5, 0, 1, 8]) tensor(1)\n",
      "tensor([7, 4, 5, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 0, 2, 3, 2, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0002:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [00:00<00:00, 67.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 4, 5, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 1, 3, 0, 1, 8]) tensor(0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0003:  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [00:00<00:00, 67.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 4, 5, 3, 8]) tensor(1)\n",
      "tensor([7, 1, 5, 4, 5, 8]) tensor(0)\n",
      "tensor([7, 4, 5, 2, 3, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0001:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 56/64 [00:00<00:00, 66.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 2, 4, 5, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 2, 3, 1, 8]) tensor(1)\n",
      "tensor([7, 0, 1, 2, 3, 8]) tensor(1)\n",
      "tensor([7, 0, 0, 5, 1, 8]) tensor(0)\n",
      "tensor([7, 2, 4, 5, 3, 8]) tensor(1)\n",
      "tensor([7, 2, 3, 0, 1, 8]) tensor(1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss 0.0287: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [00:00<00:00, 66.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 0, 2, 3, 1, 8]) tensor(1)\n",
      "tensor([7, 2, 0, 1, 2, 8]) tensor(0)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tqdm\n",
    "import requests\n",
    "def flat(x: t.Tensor) -> t.Tensor:\n",
    "    \"\"\"Combines batch and sequence dimensions.\"\"\"\n",
    "    return rearrange(x, \"b s ... -> (b s) ...\")\n",
    "\n",
    "def bert_mlm_pretrain(model: BertLanguageModel, config_dict: dict, train_loader: DataLoader) -> None:\n",
    "    '''Train using masked language modelling.'''\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    opt = make_optimizer(model, config_dict)\n",
    "    # settings=wandb.Settings(start_method=\"fork\"))\n",
    "    # wandb.init(project=\"bert-brackets\", config=config_dict)\n",
    "    # wandb.watch(model)\n",
    "    run_name = wandb.run.name if wandb.run else 'bert-brackets'\n",
    "    # tqdm progress bar of train loader annotated with epoch number\n",
    "    # os.makedirs(f\"models/{run_name}\")\n",
    "    # t.save(model.state_dict(), f\"./models/{run_name}/{run_name}-e-1.pt\")\n",
    "    \n",
    "    for epoch in range(config_dict['epochs']):\n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        for n_batch, (batch, target) in enumerate(progress_bar):\n",
    "            print(batch[0], target[0])\n",
    "\n",
    "            # with t.inference_mode():\n",
    "            #     print(f'{target.sum() / target.numel()}')\n",
    "            # wandb.log({\"epoch\": epoch, \"batch\": n_batch})\n",
    "            batch = batch.to(device)\n",
    "            opt.zero_grad()\n",
    "            # lr = lr_for_step(\n",
    "            #     n_batch + epoch * len(train_loader),\n",
    "            #     max_step=int(len(train_loader) * config_dict[\"epochs\"]),\n",
    "            #     max_lr=config_dict[\"lr\"],\n",
    "            #     warmup_step_frac=config_dict[\"warmup_step_frac\"],\n",
    "            # )\n",
    "            # for param_group in opt.param_groups:\n",
    "            #     param_group[\"lr\"] = lr\n",
    "            # masked_input_ids, mask = random_mask(batch, tokenizer.mask_token_id, tokenizer.vocab_size)\n",
    "            # masked_input_ids = masked_input_ids.to(device)\n",
    "            # mask = mask.to(device)\n",
    "            mask = (batch != tokenizer.pad_token_id).float()\n",
    "            mask.requires_grad = False\n",
    "            logits = model(batch.to(device), mask, token_type_ids=None)\n",
    "            # print(batch.shape)\n",
    "            # print(logits.shape)\n",
    "            # print(target.shape)\n",
    "            # use inference mode below to get accuracy\n",
    "            # with t.no_grad():\n",
    "            #     for x, y in testloader:\n",
    "            #         x = x.to(device)\n",
    "            #         y = y.to(device)\n",
    "            #         m = (x != tokenizer.pad_token_id).float()\n",
    "            #         logits = model(x, m, token_type_ids=None)\n",
    "            #         acc = accuracy(logits, y)\n",
    "            #         wandb.log({\"test_acc\": acc})\n",
    "            #         break\n",
    "            loss = F.cross_entropy(logits, target.to(device).long())\n",
    "            loss.backward()\n",
    "            # nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "            # wandb.log({\"loss\": loss, \"lr\": lr})\n",
    "            opt.step()\n",
    "            progress_bar.set_description(f\"Loss {loss.item():.4f}\")\n",
    "            if n_batch % 100 == 0:\n",
    "                with t.inference_mode():\n",
    "                    s1 = \")(\"\n",
    "                    s2 = \"((\"\n",
    "                    s3 = \"()\"\n",
    "                    t1 = t.tensor(tokenizer(s1)).unsqueeze(0).to(device)\n",
    "                    t2 = t.tensor(tokenizer(s2)).unsqueeze(0).to(device)\n",
    "                    t3 = t.tensor(tokenizer(s3)).unsqueeze(0).to(device)\n",
    "                    progress_bar.set_description(f\"Loss {loss.item():.4f}\")\n",
    "                    l1 = model(t1, (t1 != tokenizer.pad_token_id).float(), token_type_ids=None)\n",
    "                    l2 = model(t2, (t2 != tokenizer.pad_token_id).float(), token_type_ids=None)\n",
    "                    l3 = model(t3, (t3 != tokenizer.pad_token_id).float(), token_type_ids=None)\n",
    "                    print(f\"{s1} {l1}\")\n",
    "                    print(f\"{s2} {l2}\")\n",
    "                    print(f\"{s3} {l3}\")\n",
    "\n",
    "                    \n",
    "        t.save(model.state_dict(), f\"./models/{run_name}/{run_name}-e{epoch}.pt\")\n",
    "    # wandb.finish()\n",
    "    requests.post(\"https://ntfy.sh/arena-brackets\", data=f\"Done training {run_name} ðŸŽ‰\".encode(encoding='utf-8'))\n",
    "\n",
    "config_dict = dict(\n",
    "    lr=0.001,\n",
    "    epochs=3,\n",
    "    batch_size=64*4,\n",
    "    weight_decay=0.01,\n",
    "    mask_token_id=tokenizer.mask_token_id,\n",
    "    warmup_step_frac=0.01,\n",
    "    eps=1e-06,\n",
    "    max_grad_norm=None,\n",
    ")\n",
    "\n",
    "bert_mlm_pretrain(my_bert, config_dict, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 1, 0, 8, 6, 6]])\n",
      "tensor([[ 7.2528, -7.3399]])\n"
     ]
    }
   ],
   "source": [
    "with t.inference_mode():\n",
    "    s = \")(\"\n",
    "    tens = t.tensor(tokenizer(s)).unsqueeze(0).to(device)\n",
    "    print(tens)\n",
    "    print(my_bert(tens, None, token_type_ids=None))\n",
    "    # for batch, target in testloader:\n",
    "    #     batch = batch.to(device)\n",
    "    #     target = target.to(device)\n",
    "    #     mask = (batch != tokenizer.pad_token_id).float()\n",
    "    #     mask.requires_grad = False\n",
    "    #     logits = my_bert(batch, mask, token_type_ids=None)\n",
    "    #     print(accuracy(logits, target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[7, 0, 1, 8, 6, 6]])\n",
      "tensor([[ 1.7358, -1.3136]])\n"
     ]
    }
   ],
   "source": [
    "with t.inference_mode():\n",
    "    s = \"()\"\n",
    "    tens = t.tensor(tokenizer(s)).unsqueeze(0).to(device)\n",
    "    print(tens)\n",
    "    print(my_bert(tens, None, token_type_ids=None))\n",
    "    # for batch, target in testloader:\n",
    "    #     batch = batch.to(device)\n",
    "    #     target = target.to(device)\n",
    "    #     mask = (batch != tokenizer.pad_token_id).float()\n",
    "    #     mask.requires_grad = False\n",
    "    #     logits = my_bert(batch, mask, token_type_ids=None)\n",
    "    #     print(accuracy(logits, target))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
