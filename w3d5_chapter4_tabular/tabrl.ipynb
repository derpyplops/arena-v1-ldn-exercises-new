{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gettext import find\n",
    "from typing import Optional, Union\n",
    "import numpy as np\n",
    "import gym\n",
    "import gym.spaces\n",
    "import gym.envs.registration\n",
    "from gym.utils import seeding\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import utils\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "Arr = np.ndarray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    def __init__(self, num_states: int, num_actions: int, start=0, terminal=None):\n",
    "        self.num_states = num_states\n",
    "        self.num_actions = num_actions\n",
    "        self.start = start\n",
    "        if terminal is None:\n",
    "            self.terminal = np.array([], dtype=int)\n",
    "        else:\n",
    "            self.terminal = terminal\n",
    "        (self.T, self.R) = self.build()\n",
    "\n",
    "    def build(self):\n",
    "        '''\n",
    "        Constructs the T and R tensors from the dynamics of the environment.\n",
    "        Outputs:\n",
    "            T : (num_states, num_actions, num_states) State transition probabilities\n",
    "            R : (num_states, num_actions, num_states) Reward function\n",
    "        '''\n",
    "        num_states = self.num_states\n",
    "        num_actions = self.num_actions\n",
    "        T = np.zeros((num_states, num_actions, num_states))\n",
    "        R = np.zeros((num_states, num_actions, num_states))\n",
    "        for s in range(num_states):\n",
    "            for a in range(num_actions):\n",
    "                (states, rewards, probs) = self.dynamics(s, a)\n",
    "                (all_s, all_r, all_p) = self.out_pad(states, rewards, probs)\n",
    "                T[s, a, all_s] = all_p\n",
    "                R[s, a, all_s] = all_r\n",
    "        return (T, R)\n",
    "\n",
    "    def dynamics(self, state: int, action: int) -> tuple[Arr, Arr, Arr]:\n",
    "        '''\n",
    "        Computes the distribution over possible outcomes for a given state\n",
    "        and action.\n",
    "        Inputs:\n",
    "            state : int (index of state)\n",
    "            action : int (index of action)\n",
    "        Outputs:\n",
    "            states  : (m,) all the possible next states\n",
    "            rewards : (m,) rewards for each next state transition\n",
    "            probs   : (m,) likelihood of each state-reward pair\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def render(pi: Arr):\n",
    "        '''\n",
    "        Takes a policy pi, and draws an image of the behavior of that policy,\n",
    "        if applicable.\n",
    "        Inputs:\n",
    "            pi : (num_actions,) a policy\n",
    "        Outputs:\n",
    "            None\n",
    "        '''\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def out_pad(self, states: Arr, rewards: Arr, probs: Arr):\n",
    "        '''\n",
    "        Inputs:\n",
    "            states  : (m,) all the possible next states\n",
    "            rewards : (m,) rewards for each next state transition\n",
    "            probs   : (m,) likelihood of each state-reward pair\n",
    "        Outputs:\n",
    "            states  : (num_states,) all the next states\n",
    "            rewards : (num_states,) rewards for each next state transition\n",
    "            probs   : (num_states,) likelihood of each state-reward pair (including\n",
    "                           probability zero outcomes.)\n",
    "        '''\n",
    "        out_s = np.arange(self.num_states)\n",
    "        out_r = np.zeros(self.num_states)\n",
    "        out_p = np.zeros(self.num_states)\n",
    "        for i in range(len(states)):\n",
    "            idx = states[i]\n",
    "            out_r[idx] += rewards[i]\n",
    "            out_p[idx] += probs[i]\n",
    "        return (out_s, out_r, out_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Toy(Environment):\n",
    "    def dynamics(self, state: int, action: int):\n",
    "        (S0, SL, SR) = (0, 1, 2)\n",
    "        LEFT = 0\n",
    "        num_states = 3\n",
    "        num_actions = 2\n",
    "        assert 0 <= state < self.num_states and 0 <= action < self.num_actions\n",
    "        if state == S0:\n",
    "            if action == LEFT:\n",
    "                (next_state, reward) = (SL, 1)\n",
    "            else:\n",
    "                (next_state, reward) = (SR, 0)\n",
    "        elif state == SL:\n",
    "            (next_state, reward) = (0, 0)\n",
    "        elif state == SR:\n",
    "            (next_state, reward) = (0, 2)\n",
    "        return (np.array([next_state]), np.array([reward]), np.array([1]))\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(3, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0.]\n",
      "  [0. 0. 1.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]]\n",
      "\n",
      " [[1. 0. 0.]\n",
      "  [1. 0. 0.]]]\n",
      "[[[0. 1. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[2. 0. 0.]\n",
      "  [2. 0. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "if MAIN:\n",
    "    toy = Toy()\n",
    "    print(toy.T)\n",
    "    print(toy.R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Norvig(Environment):\n",
    "    def dynamics(self, state: int, action: int) -> tuple[Arr, Arr, Arr]:\n",
    "        def state_index(state):\n",
    "            assert 0 <= state[0] < self.width and 0 <= state[1] < self.height, print(state)\n",
    "            pos = state[0] + state[1] * self.width\n",
    "            assert 0 <= pos < self.num_states, print(state, pos)\n",
    "            return pos\n",
    "\n",
    "        pos = self.states[state]\n",
    "        move = self.actions[action]\n",
    "        if state in self.terminal or state in self.walls:\n",
    "            return (np.array([state]), np.array([0]), np.array([1]))\n",
    "        out_probs = np.zeros(self.num_actions) + 0.1\n",
    "        out_probs[action] = 0.7\n",
    "        out_states = np.zeros(self.num_actions, dtype=int) + self.num_actions\n",
    "        out_rewards = np.zeros(self.num_actions) + self.penalty\n",
    "        new_states = [pos + x for x in self.actions]\n",
    "        for (i, s_new) in enumerate(new_states):\n",
    "            if not (0 <= s_new[0] < self.width and 0 <= s_new[1] < self.height):\n",
    "                out_states[i] = state\n",
    "                continue\n",
    "            new_state = state_index(s_new)\n",
    "            if new_state in self.walls:\n",
    "                out_states[i] = state\n",
    "            else:\n",
    "                out_states[i] = new_state\n",
    "            for idx in range(len(self.terminal)):\n",
    "                if new_state == self.terminal[idx]:\n",
    "                    out_rewards[i] = self.goal_rewards[idx]\n",
    "        return (out_states, out_rewards, out_probs)\n",
    "\n",
    "    def render(self, pi: Arr):\n",
    "        assert len(pi) == self.num_states\n",
    "        emoji = [\"⬆️\", \"➡️\", \"⬇️\", \"⬅️\"]\n",
    "        grid = [emoji[act] for act in pi]\n",
    "        grid[3] = \"🟩\"\n",
    "        grid[7] = \"🟥\"\n",
    "        grid[5] = \"⬛\"\n",
    "        print(str(grid[0:4]) + \"\\n\" + str(grid[4:8]) + \"\\n\" + str(grid[8:]))\n",
    "\n",
    "    def __init__(self, penalty=-0.04):\n",
    "        self.height = 3\n",
    "        self.width = 4\n",
    "        self.penalty = penalty\n",
    "        num_states = self.height * self.width\n",
    "        num_actions = 4\n",
    "        self.states = np.array([[x, y] for y in range(self.height) for x in range(self.width)])\n",
    "        self.actions = np.array([[0, -1], [1, 0], [0, 1], [-1, 0]])\n",
    "        self.dim = (self.height, self.width)\n",
    "        terminal = np.array([3, 7], dtype=int)\n",
    "        self.walls = np.array([5], dtype=int)\n",
    "        self.goal_rewards = np.array([1.0, -1])\n",
    "        super().__init__(num_states, num_actions, start=8, terminal=terminal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 144 steps.\n",
      "Converged in 117 steps.\n",
      "Converged in 144 steps.\n",
      "Converged in 122 steps.\n",
      "Converged in 142 steps.\n"
     ]
    }
   ],
   "source": [
    "def policy_eval_numerical(env: Environment, pi: Arr, gamma=0.99, eps=1e-08) -> Arr:\n",
    "    '''\n",
    "    Numerically evaluates the value of a given policy by iterating the Bellman equation\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        pi : shape (num_states,) - The policy to evaluate\n",
    "        gamma: float - Discount factor\n",
    "        eps  : float - Tolerance\n",
    "    Outputs:\n",
    "        value : float (num_states,) - The value function for policy pi\n",
    "    '''\n",
    "\n",
    "    vs = np.zeros(env.num_states)\n",
    "    delta = np.array([100000])\n",
    "    while delta > eps:\n",
    "        new_vs = np.copy(vs)\n",
    "        for s, v in enumerate(vs):\n",
    "            p_nextstates = env.T[s, pi[s]]\n",
    "            r_nextstates = env.R[s, pi[s]]\n",
    "            new_vs[s] = np.dot(p_nextstates, r_nextstates + gamma * vs)\n",
    "        delta = np.abs(new_vs - vs).max(keepdims=True)\n",
    "        vs = new_vs\n",
    "    return vs\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_eval(policy_eval_numerical, exact=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_eval_exact(env: Environment, pi: Arr, gamma=0.99) -> Arr:\n",
    "    '''\n",
    "    Exactly evaluates the value of a given policy by solving the Bellman equation\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        pi : shape (num_states,) - The policy to evaluate\n",
    "        gamma: float - Discount factor\n",
    "    Outputs:\n",
    "        value : float (num_states,) - The value function for policy pi\n",
    "\n",
    "    formula: v = (I - gamma * P_pi)^-1 * r_pi\n",
    "    '''\n",
    "    I = np.eye(env.num_states)\n",
    "    # P_pi = np.zeros((env.num_states, env.num_states))\n",
    "    # R_pi = np.zeros((env.num_states, env.num_states))\n",
    "    # for i in range(env.num_states):\n",
    "    #     for j in range(env.num_states):\n",
    "    #         P_pi[i, j] = env.T[i, pi[i], j]\n",
    "    #         R_pi[i, j] = env.R[i, pi[i], j]\n",
    "\n",
    "    P_pi = np.array([env.T[i, pi[i]] for i in range(env.num_states)])\n",
    "    R_pi = np.array([env.R[i, pi[i]] for i in range(env.num_states)])\n",
    "    \n",
    "    # nonvectorized\n",
    "    # r_pi = np.zeros(env.num_states)\n",
    "    # for i in range(env.num_states):\n",
    "    #     r_pi[i] = (P_pi * R_pi).sum(axis=1)[i]\n",
    "\n",
    "    r_pi = (P_pi * R_pi).sum(axis=1)\n",
    "    v = np.linalg.inv(I - gamma * P_pi) @ r_pi\n",
    "    return v\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_eval(policy_eval_exact, exact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_improvement(env: Environment, V: Arr, gamma=0.99) -> Arr:\n",
    "    '''\n",
    "    Inputs:\n",
    "        env: Environment\n",
    "        V  : (num_states,) value of each state following some policy pi\n",
    "    Outputs:\n",
    "        pi_better : vector (num_states,) of actions representing a new policy obtained via policy iteration\n",
    "\n",
    "    pi_better = argmax_a sum_s p(s'|s,a) * (r(s,a,s') + gamma * V(s'))\n",
    "    '''\n",
    "    pi_better = np.zeros(env.num_states, dtype=int)\n",
    "    for s in range(env.num_states):\n",
    "        Q = np.zeros(env.num_actions)\n",
    "        for a in range(env.num_actions):\n",
    "            p_nextstates = env.T[s, a]\n",
    "            r_nextstates = env.R[s, a]\n",
    "            Q[a] = np.dot(p_nextstates, r_nextstates + gamma * V)\n",
    "        pi_better[s] = np.argmax(Q)\n",
    "    return pi_better\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_policy_improvement(policy_improvement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['➡️', '➡️', '➡️', '🟩']\n",
      "['⬆️', '⬛', '⬆️', '🟥']\n",
      "['⬆️', '⬅️', '⬆️', '⬅️']\n"
     ]
    }
   ],
   "source": [
    "def find_optimal_policy(env: Environment, gamma=0.99):\n",
    "    '''\n",
    "    Inputs:\n",
    "        env: environment\n",
    "    Outputs:\n",
    "        pi : (num_states,) int, of actions represeting an optimal policy\n",
    "    '''\n",
    "    pi = np.zeros(env.num_states, dtype=int)\n",
    "    while True:\n",
    "        V = policy_eval_exact(env, pi, gamma)\n",
    "        pi_better = policy_improvement(env, V, gamma)\n",
    "        if np.all(pi == pi_better):\n",
    "            return pi\n",
    "        pi = pi_better\n",
    "\n",
    "if MAIN:\n",
    "    utils.test_find_optimal_policy(find_optimal_policy)\n",
    "    penalty = -0.04\n",
    "    norvig = Norvig(penalty)\n",
    "    pi_opt = find_optimal_policy(norvig, gamma=0.99)\n",
    "    norvig.render(pi_opt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
