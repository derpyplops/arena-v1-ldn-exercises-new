{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Discrete\n",
    "from einops import rearrange\n",
    "\n",
    "from utils import make_env, ppo_parse_args\n",
    "import tests\n",
    "\n",
    "from collections import OrderedDict\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from distutils.util import strtobool\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Discrete\n",
    "from typing import Any, List, Optional, Union, Tuple, Iterable\n",
    "from einops import rearrange\n",
    "from utils import ppo_parse_args, make_env\n",
    "import solutions\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "RUNNING_FROM_FILE = \"ipykernel_launcher\" in os.path.basename(sys.argv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    t.nn.init.orthogonal_(layer.weight, std)\n",
    "    t.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    critic: nn.Sequential\n",
    "    actor: nn.Sequential\n",
    "\n",
    "    num_actions = 34789017348903127489\n",
    "\n",
    "    def __init__(self, envs: gym.vector.SyncVectorEnv):\n",
    "        super().__init__()\n",
    "        obs_shape = envs.single_observation_space.shape[0] # should this be multi\n",
    "        num_actions = envs.single_action_space.n\n",
    "        self.actor = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", layer_init(nn.Linear(obs_shape, 64))),\n",
    "            (\"tanh\", nn.Tanh()),\n",
    "            (\"linear2\", layer_init(nn.Linear(64, 64))),\n",
    "            (\"tanh\", nn.Tanh()),\n",
    "            (\"linear2\", layer_init(nn.Linear(64, num_actions), std=np.sqrt(0.01))),\n",
    "        ]))\n",
    "\n",
    "        self.critic = nn.Sequential(OrderedDict([\n",
    "            (\"linear1\", layer_init(nn.Linear(obs_shape, 64))),\n",
    "            (\"tanh\", nn.Tanh()),\n",
    "            (\"linear2\", layer_init(nn.Linear(64, 64))),\n",
    "            (\"tanh\", nn.Tanh()),\n",
    "            (\"linear2\", layer_init(nn.Linear(64, 1), std=np.sqrt(1))),\n",
    "        ]))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def compute_advantages(\n",
    "    next_value: t.Tensor,\n",
    "    next_done: t.Tensor,\n",
    "    rewards: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    dones: t.Tensor,\n",
    "    device: t.device,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> t.Tensor:\n",
    "    '''Compute advantages using Generalized Advantage Estimation.\n",
    "\n",
    "    next_value: shape (1, env) - represents V(s_{t+1}) which is needed for the last advantage term\n",
    "    next_done: shape (env,)\n",
    "    rewards: shape (t, env)\n",
    "    values: shape (t, env)\n",
    "    dones: shape (t, env)\n",
    "\n",
    "    Return: shape (t, env)\n",
    "    '''\n",
    "    dim_t, n_envs = dones.shape\n",
    "    valuesplus = values.clone()\n",
    "    valuesplus[0:-1] = values[1:]\n",
    "    valuesplus[-1] = next_value\n",
    "    donesplus = dones.clone()\n",
    "    donesplus[0:-1] = dones[1:]\n",
    "    donesplus[-1] = next_done\n",
    "\n",
    "    T = values.shape[0]\n",
    "\n",
    "    deltas = rewards + gamma * valuesplus * (1.0 - donesplus) - values\n",
    "    advantages = deltas.clone().to(device)\n",
    "    # non-vectorized implementation\n",
    "    for i in range(1, T):\n",
    "        i = T-i\n",
    "        advantages[i-1] = deltas[i-1] + gamma * gae_lambda * (1.0 - dones[i]) * advantages[i]\n",
    "\n",
    "    return advantages\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_compute_advantages(compute_advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_minibatch_indexes` passed.\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Minibatch:\n",
    "    obs: t.Tensor\n",
    "    logprobs: t.Tensor\n",
    "    actions: t.Tensor\n",
    "    advantages: t.Tensor\n",
    "    returns: t.Tensor\n",
    "    values: t.Tensor\n",
    "\n",
    "def minibatch_indexes(batch_size: int, minibatch_size: int) -> list[np.ndarray]:\n",
    "    '''Return a list of length (batch_size // minibatch_size) where each element is an array of indexes into the batch.\n",
    "\n",
    "    Each index should appear exactly once.\n",
    "    '''\n",
    "    assert batch_size % minibatch_size == 0\n",
    "    # size = batch_size // minibatch_size\n",
    "    # indexes = np.arange(batch_size)\n",
    "    # np.random.shuffle(indexes)\n",
    "    # return [indexes[i * minibatch_size : (i + 1) * minibatch_size] for i in range(size)]\n",
    "    indices = np.random.permutation(batch_size)\n",
    "    indices = rearrange(indices, \"(mb_num mb_size) -> mb_num mb_size\", mb_size=minibatch_size)\n",
    "    return list(indices)\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_minibatch_indexes(minibatch_indexes)\n",
    "\n",
    "def make_minibatches(\n",
    "    obs: t.Tensor,\n",
    "    logprobs: t.Tensor,\n",
    "    actions: t.Tensor,\n",
    "    advantages: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    obs_shape: tuple,\n",
    "    action_shape: tuple,\n",
    "    batch_size: int,\n",
    "    minibatch_size: int,\n",
    ") -> list[Minibatch]:\n",
    "    '''Flatten the environment and steps dimension into one batch dimension, then shuffle and split into minibatches.'''\n",
    "    idxss = minibatch_indexes(batch_size, minibatch_size)\n",
    "    returns = advantages + values\n",
    "    # return [Minibatch(\n",
    "    #         obs=obs[:, idxs].reshape(-1, *obs_shape),\n",
    "    #         logprobs=logprobs[:, idxs].reshape(-1),\n",
    "    #         actions=actions[:, idxs].reshape(-1, *action_shape),\n",
    "    #         advantages=advantages[:, idxs].reshape(-1),\n",
    "    #         returns=advantages[:, idxs].reshape(-1) + values[:, idxs].reshape(-1),\n",
    "    #         values=values[:, idxs].reshape(-1),\n",
    "    #     )\n",
    "    # for idxs in idxss]\n",
    "    data = (obs, logprobs, actions, advantages, returns, values)\n",
    "    shapes = (obs_shape, (), action_shape, (), (), ())\n",
    "    return [\n",
    "        Minibatch(*[d.reshape((-1,) + s)[ind] for d, s in zip(data, shapes)])\n",
    "        for ind in minibatch_indexes(batch_size, minibatch_size)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_calc_policy_loss` passed.\n"
     ]
    }
   ],
   "source": [
    "def calc_policy_loss(\n",
    "    probs: Categorical, mb_action: t.Tensor, mb_advantages: t.Tensor, mb_logprobs: t.Tensor, clip_coef: float\n",
    ") -> t.Tensor:\n",
    "    '''Return the policy loss, suitable for maximisation with gradient ascent.\n",
    "\n",
    "    probs: a distribution containing the actor's unnormalized logits of shape (minibatch, num_actions)\n",
    "\n",
    "    clip_coef: amount of clipping, denoted by epsilon in Eq 7.\n",
    "\n",
    "    normalize: if true, normalize mb_advantages to have mean 0, variance 1\n",
    "    '''\n",
    "    rttheta = t.exp(probs.log_prob(mb_action) - mb_logprobs)\n",
    "    mb_advantages = (mb_advantages - mb_advantages.mean()) / mb_advantages.std()\n",
    "\n",
    "    return t.min(\n",
    "        rttheta * mb_advantages,\n",
    "        rttheta.clip(1 - clip_coef, 1 + clip_coef) * mb_advantages\n",
    "    ).mean()\n",
    "    \n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_policy_loss(calc_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests in `test_calc_value_function_loss` passed!\n"
     ]
    }
   ],
   "source": [
    "def calc_value_function_loss(critic: nn.Sequential, mb_obs: t.Tensor, mb_returns: t.Tensor, v_coef: float) -> t.Tensor:\n",
    "    '''Compute the value function portion of the loss function.\n",
    "\n",
    "    v_coef: the coefficient for the value loss, which weights its contribution to the overall loss. Denoted by c_1 in the paper.\n",
    "    '''\n",
    "    return 0.5 * v_coef * nn.MSELoss()(critic(mb_obs), mb_returns)\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_value_function_loss(calc_value_function_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy_loss(probs: Categorical, ent_coef: float):\n",
    "    '''Return the entropy loss term.\n",
    "\n",
    "    ent_coef: the coefficient for the entropy loss, which weights its contribution to the overall loss. Denoted by c_2 in the paper.\n",
    "    '''\n",
    "    return ent_coef * probs.entropy().mean()\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_entropy_loss(calc_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOScheduler:\n",
    "    def __init__(self, optimizer, initial_lr: float, end_lr: float, num_updates: int):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.num_updates = num_updates\n",
    "        self.n_step_calls = 0\n",
    "\n",
    "    def step(self):\n",
    "        '''Implement linear learning rate decay so that after num_updates calls to step, the learning rate is end_lr.'''\n",
    "        if self.n_step_calls > self.num_updates:\n",
    "            return self.end_lr\n",
    "        else:\n",
    "            return self.initial_lr + (self.end_lr - self.initial_lr) * self.n_step_calls / self.num_updates\n",
    "\n",
    "def make_optimizer(agent: Agent, num_updates: int, initial_lr: float, end_lr: float) -> tuple[optim.Adam, PPOScheduler]:\n",
    "    '''Return an appropriately configured Adam with its attached scheduler.'''\n",
    "    optimizer = optim.Adam(agent.parameters(), lr=initial_lr, maximize=True)\n",
    "    return optimizer, PPOScheduler(optimizer, initial_lr, end_lr, num_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "__file__ = 'ppo.py'\n",
    "\n",
    "@dataclass\n",
    "class PPOArgs:\n",
    "    exp_name: str = os.path.basename(__file__).rstrip(\".py\")\n",
    "    seed: int = 1\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    track: bool = True\n",
    "    wandb_project_name: str = \"PPOCart\"\n",
    "    wandb_entity: str = None\n",
    "    capture_video: bool = True\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    total_timesteps: int = 500000\n",
    "    learning_rate: float = 0.00025\n",
    "    num_envs: int = 4\n",
    "    num_steps: int = 128\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    num_minibatches: int = 4\n",
    "    update_epochs: int = 4\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    batch_size: int = 512\n",
    "    minibatch_size: int = 128\n",
    "import tqdm\n",
    "def train_ppo(args: PPOArgs):\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    # writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    # writer.add_text(\n",
    "    #     \"hyperparameters\",\n",
    "    #     \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join([f\"|{key}|{value}|\" for (key, value) in vars(args).items()]),\n",
    "    # )\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "    action_shape = envs.single_action_space.shape\n",
    "    assert action_shape is not None\n",
    "    assert isinstance(envs.single_action_space, Discrete), \"only discrete action space is supported\"\n",
    "    agent = Agent(envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "    (optimizer, scheduler) = make_optimizer(agent, num_updates, args.learning_rate, 0.0)\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + action_shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    global_step = 0\n",
    "    old_approx_kl = 0.0\n",
    "    approx_kl = 0.0\n",
    "    value_loss = t.tensor(0.0)\n",
    "    policy_loss = t.tensor(0.0)\n",
    "    entropy_loss = t.tensor(0.0)\n",
    "    clipfracs = []\n",
    "    info = []\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "\n",
    "    for _ in tqdm.trange(num_updates):\n",
    "        for i in range(0, args.num_steps):\n",
    "            \"YOUR CODE: Rollout phase (see detail #1)\"\n",
    "            obs[i] = next_obs\n",
    "            dones[i] = next_done\n",
    "\n",
    "            with t.inference_mode():\n",
    "                next_values = agent.critic(next_obs).flatten()\n",
    "                logits = agent.actor(next_obs)\n",
    "            \n",
    "            probs = Categorical(logits=logits)\n",
    "            action = probs.sample()\n",
    "            logprob = probs.log_prob(action)\n",
    "\n",
    "            next_obs, reward, done, info = envs.step(action.cpu().numpy())\n",
    "            rewards[i] = t.from_numpy(reward).to(device)\n",
    "            actions[i] = action\n",
    "            logprobs[i] = logprob\n",
    "            values[i] = next_values\n",
    "\n",
    "            next_obs = t.from_numpy(next_obs).to(device)\n",
    "            next_done = t.from_numpy(done).to(device)\n",
    "\n",
    "            for item in info:\n",
    "                if \"episode\" in item.keys():\n",
    "                    # print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")                   \n",
    "                    # writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                    # writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                    wandb.log({\"charts/episodic_return\": item[\"episode\"][\"r\"], \"charts/episodic_length\": item[\"episode\"][\"l\"]})\n",
    "                    break\n",
    "        next_value = rearrange(agent.critic(next_obs), \"env 1 -> 1 env\")\n",
    "        advantages = compute_advantages(\n",
    "            next_value, next_done, rewards, values, dones, device, args.gamma, args.gae_lambda\n",
    "        )\n",
    "        clipfracs.clear()\n",
    "        for _ in range(args.update_epochs):\n",
    "            minibatches = make_minibatches(\n",
    "                obs,\n",
    "                logprobs,\n",
    "                actions,\n",
    "                advantages,\n",
    "                values,\n",
    "                envs.single_observation_space.shape,\n",
    "                action_shape,\n",
    "                args.batch_size,\n",
    "                args.minibatch_size,\n",
    "            )\n",
    "            for mb in minibatches:\n",
    "                \"YOUR CODE: compute loss on the minibatch and step the optimizer (not the scheduler). Do detail #11 (global gradient clipping) here using nn.utils.clip_grad_norm_.\"\n",
    "                logits = agent.actor(mb.obs)\n",
    "                probs = Categorical(logits=logits)\n",
    "                policy_loss = calc_policy_loss(probs, mb.actions, mb.advantages, mb.logprobs, args.clip_coef)\n",
    "                value_loss = calc_value_function_loss(agent.critic, mb.obs, mb.returns, args.vf_coef)\n",
    "                entropy_loss = calc_entropy_loss(probs, args.ent_coef)\n",
    "                loss = policy_loss - value_loss + entropy_loss\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "        scheduler.step()\n",
    "        (y_pred, y_true) = (mb.values.cpu().numpy(), mb.returns.cpu().numpy())\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        with torch.no_grad():\n",
    "            newlogprob: t.Tensor = probs.log_prob(mb.actions)\n",
    "            logratio = newlogprob - mb.logprobs\n",
    "            ratio = logratio.exp()\n",
    "            old_approx_kl = (-logratio).mean().item()\n",
    "            approx_kl = (ratio - 1 - logratio).mean().item()\n",
    "            clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "        # writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        # writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "        # writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "        # writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        # writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl, global_step)\n",
    "        # writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        # writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        # writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        # writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        wandb.log({\n",
    "            \"totalloss\": value_loss.item() + policy_loss.item() + entropy_loss.item(),\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"], \n",
    "            \"value_loss\": value_loss.item(), \n",
    "            \"policy_loss\": policy_loss.item(), \n",
    "            \"entropy\": entropy_loss.item(), \n",
    "            \"old_approx_kl\": old_approx_kl, \n",
    "            \"approx_kl\": approx_kl, \n",
    "            \"clipfrac\": np.mean(clipfracs), \n",
    "            \"explained_variance\": explained_var, \n",
    "            \"SPS\": int(global_step / (time.time() - start_time))\n",
    "            })\n",
    "        # if global_step % 10 == 0:\n",
    "        #     print(\"steps per second (SPS):\", int(global_step / (time.time() - start_time)))\n",
    "    envs.close()\n",
    "    # writer.close()\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Try running this file from the command line instead: python ppo.py --help\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jon/ml/arena/venv/lib/python3.9/site-packages/gym/envs/registration.py:595: UserWarning: \u001b[33mWARN: Overriding environment EasyCart-v0\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {id}\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e90c37fc71fc4cb09bf7a33ee52cd566",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016675175000030625, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jon/ml/arena/arena-v1-ldn-exercises-new/w4d3_chapter4_ppo/wandb/run-20221202_143215-ttx0q6pg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jnhl/PPOCart/runs/ttx0q6pg\" target=\"_blank\">EasyCart-v0__ppo__1__1669991535</a></strong> to <a href=\"https://wandb.ai/jnhl/PPOCart\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/976 [00:00<?, ?it/s]/Users/jon/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  0%|          | 1/976 [00:00<02:44,  5.94it/s]/Users/jon/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  1%|          | 5/976 [00:00<01:08, 14.22it/s]/Users/jon/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  1%|▏         | 14/976 [00:00<00:43, 22.05it/s]/Users/jon/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "  5%|▍         | 44/976 [00:02<00:36, 25.27it/s]/Users/jon/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      " 20%|█▉        | 192/976 [00:07<00:26, 29.26it/s]/Users/jon/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      " 48%|████▊     | 465/976 [00:17<00:16, 30.75it/s]/Users/jon/ml/arena/venv/lib/python3.9/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([128])) that is different to the input size (torch.Size([128, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "100%|██████████| 976/976 [00:33<00:00, 28.89it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0884b6c0473b46a0a4827791e2aaf12c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.410 MB of 0.410 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>approx_kl</td><td>▁▁▁▁▁▁▁█▁▁▁▁▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>charts/episodic_length</td><td>▁▁▁▁▁▁▁▂▁▁▃▁▄█▂█▃▄▃▃▅▃▆▄▄▆███▇▄▄▄▅██▅▇██</td></tr><tr><td>charts/episodic_return</td><td>▁▁▁▁▁▁▁▂▁▁▃▁▃█▂█▃▄▃▂▄▃▅▄▃▅███▆▃▃▃▅▇█▅▆▇█</td></tr><tr><td>clipfrac</td><td>▁▁▁▁▁▁▁█▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>entropy</td><td>█▆▄▄▄▃▃▁▄▃▅▄▄▄▄▄▄▃▄▃▄▂▃▂▁▂▃▄▃▃▃▃▃▄▄▃▄▄▄▂</td></tr><tr><td>explained_variance</td><td>▅▄▅▄▇▂▃▁▄▃▄▄▄▄▄▄▅▄▅▅▁▅▅▅█▇▆▅▅▆▅▅▅▅▄▆▆▆▅▅</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>old_approx_kl</td><td>▁▃▃▂▃▁▃█▃▄▂▃▃▂▂▃▂▃▃▃▃▂▂▃▃▃▃▃▃▂▃▃▃▂▂▃▂▃▂▃</td></tr><tr><td>policy_loss</td><td>▅▅▅█▄▃▅▁▄▇▆▅▅▇▁▅▅▅▅▆▅▅▅▆▅▅▅▇▇▅▅▅▅▅▅▆▄▆▅▇</td></tr><tr><td>totalloss</td><td>▃▃▄▄▃▃▃▅▃▅▄▄▆▅▃▅▁▆▂▄▁▃▅▇▁▆▆▆▆▁▃▁▂▅▃█▂▃▆▅</td></tr><tr><td>value_loss</td><td>▃▃▄▄▃▃▃▅▃▅▄▄▆▅▃▅▁▆▂▄▁▃▅▇▁▆▆▆▆▁▃▁▂▅▃█▂▃▆▅</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>SPS</td><td>0</td></tr><tr><td>approx_kl</td><td>7e-05</td></tr><tr><td>charts/episodic_length</td><td>296</td></tr><tr><td>charts/episodic_return</td><td>282.42896</td></tr><tr><td>clipfrac</td><td>0.0</td></tr><tr><td>entropy</td><td>0.00551</td></tr><tr><td>explained_variance</td><td>0.00221</td></tr><tr><td>lr</td><td>0.00025</td></tr><tr><td>old_approx_kl</td><td>0.0002</td></tr><tr><td>policy_loss</td><td>0.00443</td></tr><tr><td>totalloss</td><td>42.40238</td></tr><tr><td>value_loss</td><td>42.39244</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">EasyCart-v0__ppo__1__1669991535</strong>: <a href=\"https://wandb.ai/jnhl/PPOCart/runs/ttx0q6pg\" target=\"_blank\">https://wandb.ai/jnhl/PPOCart/runs/ttx0q6pg</a><br/>Synced 6 W&B file(s), 8 media file(s), 0 artifact file(s) and 1 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221202_143215-ttx0q6pg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "import gym\n",
    "from gym import logger, spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "import math\n",
    "\n",
    "class EasyCart(CartPoleEnv):\n",
    "    def step(self, action):\n",
    "        (obs, rew, done, info) = super().step(action)\n",
    "        pos, vel, angle, ang_vel = obs\n",
    "        c = 1\n",
    "        new_reward = 1 - (pos / 2.4) ** 2\n",
    "        \n",
    "        return (obs, new_reward, done, info)\n",
    "\n",
    "gym.envs.registration.register(id=\"EasyCart-v0\", entry_point=EasyCart, max_episode_steps=500)\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "if MAIN:\n",
    "    if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n",
    "        filename = globals().get(\"__file__\", \"<filename of this script>\")\n",
    "        print(f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\")\n",
    "        args = PPOArgs(env_id=\"EasyCart-v0\")\n",
    "    else:\n",
    "        args = ppo_parse_args()\n",
    "    train_ppo(args)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
