{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "import re\n",
    "from dataclasses import dataclass\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Discrete\n",
    "from einops import rearrange\n",
    "\n",
    "from w4d3_chapter4_ppo.utils import make_env, ppo_parse_args\n",
    "from w4d3_chapter4_ppo import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import sys\n",
    "from distutils.util import strtobool\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch as t\n",
    "import gym\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions.categorical import Categorical\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from gym.spaces import Discrete\n",
    "from typing import Any, List, Optional, Union, Tuple, Iterable\n",
    "from einops import rearrange\n",
    "from rl_utils import ppo_parse_args, make_env\n",
    "import part4_dqn_solution\n",
    "\n",
    "MAIN = __name__ == \"__main__\"\n",
    "RUNNING_FROM_FILE = \"ipykernel_launcher\" in os.path.basename(sys.argv[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n",
    "    t.nn.init.orthogonal_(layer.weight, std)\n",
    "    t.nn.init.constant_(layer.bias, bias_const)\n",
    "    return layer\n",
    "\n",
    "class Agent(nn.Module):\n",
    "    critic: nn.Sequential\n",
    "    actor: nn.Sequential\n",
    "\n",
    "    def __init__(self, envs: gym.vector.SyncVectorEnv):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@t.inference_mode()\n",
    "def compute_advantages(\n",
    "    next_value: t.Tensor,\n",
    "    next_done: t.Tensor,\n",
    "    rewards: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    dones: t.Tensor,\n",
    "    device: t.device,\n",
    "    gamma: float,\n",
    "    gae_lambda: float,\n",
    ") -> t.Tensor:\n",
    "    '''Compute advantages using Generalized Advantage Estimation.\n",
    "\n",
    "    next_value: shape (1, env) - represents V(s_{t+1}) which is needed for the last advantage term\n",
    "    next_done: shape (env,)\n",
    "    rewards: shape (t, env)\n",
    "    values: shape (t, env)\n",
    "    dones: shape (t, env)\n",
    "\n",
    "    Return: shape (t, env)\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_compute_advantages(compute_advantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Minibatch:\n",
    "    obs: t.Tensor\n",
    "    logprobs: t.Tensor\n",
    "    actions: t.Tensor\n",
    "    advantages: t.Tensor\n",
    "    returns: t.Tensor\n",
    "    values: t.Tensor\n",
    "\n",
    "def minibatch_indexes(batch_size: int, minibatch_size: int) -> list[np.ndarray]:\n",
    "    '''Return a list of length (batch_size // minibatch_size) where each element is an array of indexes into the batch.\n",
    "\n",
    "    Each index should appear exactly once.\n",
    "    '''\n",
    "    assert batch_size % minibatch_size == 0\n",
    "    pass\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    test_minibatch_indexes(minibatch_indexes)\n",
    "\n",
    "def make_minibatches(\n",
    "    obs: t.Tensor,\n",
    "    logprobs: t.Tensor,\n",
    "    actions: t.Tensor,\n",
    "    advantages: t.Tensor,\n",
    "    values: t.Tensor,\n",
    "    obs_shape: tuple,\n",
    "    action_shape: tuple,\n",
    "    batch_size: int,\n",
    "    minibatch_size: int,\n",
    ") -> list[Minibatch]:\n",
    "    '''Flatten the environment and steps dimension into one batch dimension, then shuffle and split into minibatches.'''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_policy_loss(\n",
    "    probs: Categorical, mb_action: t.Tensor, mb_advantages: t.Tensor, mb_logprobs: t.Tensor, clip_coef: float\n",
    ") -> t.Tensor:\n",
    "    '''Return the policy loss, suitable for maximisation with gradient ascent.\n",
    "\n",
    "    probs: a distribution containing the actor's unnormalized logits of shape (minibatch, num_actions)\n",
    "\n",
    "    clip_coef: amount of clipping, denoted by epsilon in Eq 7.\n",
    "\n",
    "    normalize: if true, normalize mb_advantages to have mean 0, variance 1\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    test_calc_policy_loss(calc_policy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_value_function_loss(critic: nn.Sequential, mb_obs: t.Tensor, mb_returns: t.Tensor, v_coef: float) -> t.Tensor:\n",
    "    '''Compute the value function portion of the loss function.\n",
    "\n",
    "    v_coef: the coefficient for the value loss, which weights its contribution to the overall loss. Denoted by c_1 in the paper.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_value_function_loss(calc_value_function_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_entropy_loss(probs: Categorical, ent_coef: float):\n",
    "    '''Return the entropy loss term.\n",
    "\n",
    "    ent_coef: the coefficient for the entropy loss, which weights its contribution to the overall loss. Denoted by c_2 in the paper.\n",
    "    '''\n",
    "    pass\n",
    "\n",
    "if MAIN and RUNNING_FROM_FILE:\n",
    "    tests.test_calc_entropy_loss(calc_entropy_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOScheduler:\n",
    "    def __init__(self, optimizer, initial_lr: float, end_lr: float, num_updates: int):\n",
    "        self.optimizer = optimizer\n",
    "        self.initial_lr = initial_lr\n",
    "        self.end_lr = end_lr\n",
    "        self.num_updates = num_updates\n",
    "        self.n_step_calls = 0\n",
    "\n",
    "    def step(self):\n",
    "        '''Implement linear learning rate decay so that after num_updates calls to step, the learning rate is end_lr.'''\n",
    "        pass\n",
    "\n",
    "def make_optimizer(agent: Agent, num_updates: int, initial_lr: float, end_lr: float) -> tuple[optim.Adam, PPOScheduler]:\n",
    "    '''Return an appropriately configured Adam with its attached scheduler.'''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PPOArgs:\n",
    "    exp_name: str = os.path.basename(__file__).rstrip(\".py\")\n",
    "    seed: int = 1\n",
    "    torch_deterministic: bool = True\n",
    "    cuda: bool = True\n",
    "    track: bool = True\n",
    "    wandb_project_name: str = \"PPOCart\"\n",
    "    wandb_entity: str = None\n",
    "    capture_video: bool = False\n",
    "    env_id: str = \"CartPole-v1\"\n",
    "    total_timesteps: int = 500000\n",
    "    learning_rate: float = 0.00025\n",
    "    num_envs: int = 4\n",
    "    num_steps: int = 128\n",
    "    gamma: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    num_minibatches: int = 4\n",
    "    update_epochs: int = 4\n",
    "    clip_coef: float = 0.2\n",
    "    ent_coef: float = 0.01\n",
    "    vf_coef: float = 0.5\n",
    "    max_grad_norm: float = 0.5\n",
    "    batch_size: int = 512\n",
    "    minibatch_size: int = 128\n",
    "\n",
    "def train_ppo(args):\n",
    "    run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n",
    "    if args.track:\n",
    "        import wandb\n",
    "\n",
    "        wandb.init(\n",
    "            project=args.wandb_project_name,\n",
    "            entity=args.wandb_entity,\n",
    "            sync_tensorboard=True,\n",
    "            config=vars(args),\n",
    "            name=run_name,\n",
    "            monitor_gym=True,\n",
    "            save_code=True,\n",
    "        )\n",
    "    writer = SummaryWriter(f\"runs/{run_name}\")\n",
    "    writer.add_text(\n",
    "        \"hyperparameters\",\n",
    "        \"|param|value|\\n|-|-|\\n%s\" % \"\\n\".join([f\"|{key}|{value}|\" for (key, value) in vars(args).items()]),\n",
    "    )\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.backends.cudnn.deterministic = args.torch_deterministic\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "    envs = gym.vector.SyncVectorEnv(\n",
    "        [make_env(args.env_id, args.seed + i, i, args.capture_video, run_name) for i in range(args.num_envs)]\n",
    "    )\n",
    "    action_shape = envs.single_action_space.shape\n",
    "    assert action_shape is not None\n",
    "    assert isinstance(envs.single_action_space, Discrete), \"only discrete action space is supported\"\n",
    "    agent = Agent(envs).to(device)\n",
    "    num_updates = args.total_timesteps // args.batch_size\n",
    "    (optimizer, scheduler) = make_optimizer(agent, num_updates, args.learning_rate, 0.0)\n",
    "    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n",
    "    actions = torch.zeros((args.num_steps, args.num_envs) + action_shape).to(device)\n",
    "    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n",
    "    global_step = 0\n",
    "    old_approx_kl = 0.0\n",
    "    approx_kl = 0.0\n",
    "    value_loss = t.tensor(0.0)\n",
    "    policy_loss = t.tensor(0.0)\n",
    "    entropy_loss = t.tensor(0.0)\n",
    "    clipfracs = []\n",
    "    info = []\n",
    "    start_time = time.time()\n",
    "    next_obs = torch.Tensor(envs.reset()).to(device)\n",
    "    next_done = torch.zeros(args.num_envs).to(device)\n",
    "    for _ in range(num_updates):\n",
    "        for i in range(0, args.num_steps):\n",
    "            \"YOUR CODE: Rollout phase (see detail #1)\"\n",
    "            for item in info:\n",
    "                if \"episode\" in item.keys():\n",
    "                    print(f\"global_step={global_step}, episodic_return={item['episode']['r']}\")\n",
    "                    writer.add_scalar(\"charts/episodic_return\", item[\"episode\"][\"r\"], global_step)\n",
    "                    writer.add_scalar(\"charts/episodic_length\", item[\"episode\"][\"l\"], global_step)\n",
    "                    break\n",
    "        next_value = rearrange(agent.critic(next_obs), \"env 1 -> 1 env\")\n",
    "        advantages = compute_advantages(\n",
    "            next_value, next_done, rewards, values, dones, device, args.gamma, args.gae_lambda\n",
    "        )\n",
    "        clipfracs.clear()\n",
    "        for _ in range(args.update_epochs):\n",
    "            minibatches = make_minibatches(\n",
    "                obs,\n",
    "                logprobs,\n",
    "                actions,\n",
    "                advantages,\n",
    "                values,\n",
    "                envs.single_observation_space.shape,\n",
    "                action_shape,\n",
    "                args.batch_size,\n",
    "                args.minibatch_size,\n",
    "            )\n",
    "            for mb in minibatches:\n",
    "                \"YOUR CODE: compute loss on the minibatch and step the optimizer (not the scheduler). Do detail #11 (global gradient clipping) here using nn.utils.clip_grad_norm_.\"\n",
    "        scheduler.step()\n",
    "        (y_pred, y_true) = (mb.values.cpu().numpy(), mb.returns.cpu().numpy())\n",
    "        var_y = np.var(y_true)\n",
    "        explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n",
    "        with torch.no_grad():\n",
    "            newlogprob: t.Tensor = probs.log_prob(mb.actions)\n",
    "            logratio = newlogprob - mb.logprobs\n",
    "            ratio = logratio.exp()\n",
    "            old_approx_kl = (-logratio).mean().item()\n",
    "            approx_kl = (ratio - 1 - logratio).mean().item()\n",
    "            clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n",
    "        writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n",
    "        writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n",
    "        writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl, global_step)\n",
    "        writer.add_scalar(\"losses/approx_kl\", approx_kl, global_step)\n",
    "        writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n",
    "        writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n",
    "        writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n",
    "        if global_step % 10 == 0:\n",
    "            print(\"steps per second (SPS):\", int(global_step / (time.time() - start_time)))\n",
    "    envs.close()\n",
    "    writer.close()\n",
    "\n",
    "if MAIN:\n",
    "    if \"ipykernel_launcher\" in os.path.basename(sys.argv[0]):\n",
    "        filename = globals().get(\"__file__\", \"<filename of this script>\")\n",
    "        print(f\"Try running this file from the command line instead: python {os.path.basename(filename)} --help\")\n",
    "        args = PPOArgs()\n",
    "    else:\n",
    "        args = ppo_parse_args()\n",
    "    train_ppo(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.envs.classic_control.cartpole import CartPoleEnv\n",
    "import gym\n",
    "from gym import logger, spaces\n",
    "from gym.error import DependencyNotInstalled\n",
    "import math\n",
    "\n",
    "class EasyCart(CartPoleEnv):\n",
    "    def step(self, action):\n",
    "        (obs, rew, done, info) = super().step(action)\n",
    "        \"TODO: YOUR CODE HERE\"\n",
    "\n",
    "gym.envs.registration.register(id=\"EasyCart-v0\", entry_point=EasyCart, max_episode_steps=500)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
