{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as t\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import TensorDataset\n",
    "from einops import rearrange\n",
    "from torch.nn import functional as F\n",
    "import tqdm\n",
    "import random\n",
    "from collections import OrderedDict\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from einops import rearrange\n",
    "import wandb\n",
    "from fancy_einsum import einsum\n",
    "\n",
    "import transformer_lens\n",
    "import transformer_lens.utils as utils\n",
    "from transformer_lens.hook_points import HookedRootModule, HookPoint  # Hooking utilities\n",
    "from transformer_lens import HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset with bracket strings\n",
    "# and labels for each bracket\n",
    "\n",
    "# tokenizer for brackets\n",
    "class Tokenizer:\n",
    "    def __init__(self, vocab_size_without_special, seq_len):\n",
    "        self.vocab_size = vocab_size_without_special\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "        self.pad_token_id = vocab_size_without_special\n",
    "        self.cls_token_id = vocab_size_without_special + 1\n",
    "        self.sep_token_id = vocab_size_without_special + 2\n",
    "\n",
    "    def __call__(self, text):\n",
    "        return self.encode(text)\n",
    "\n",
    "    def encode(self, nums):\n",
    "        assert len(nums) <= self.seq_len\n",
    "        nums = [self.cls_token_id] + nums + [self.sep_token_id] # add cls and sep tokens\n",
    "        nums = nums + [self.pad_token_id] * (self.seq_len - len(nums) + 2) # pad to max len\n",
    "        assert len(nums) == self.seq_len + 2\n",
    "        return nums\n",
    "\n",
    "    def decode(self, ids):\n",
    "        if type(ids) == t.Tensor:\n",
    "            ids = ids.tolist()\n",
    "        ids = [id for id in ids if id != self.pad_token_id]\n",
    "        # remove cls and sep tokens if present\n",
    "        if ids[0] == self.cls_token_id:\n",
    "            ids = ids[1:]\n",
    "        if ids[-1] == self.sep_token_id:\n",
    "            ids = ids[:-1]\n",
    "        return ids\n",
    "\n",
    "tokenizer = Tokenizer(10, 16)\n",
    "\n",
    "eg = [1,2,3]\n",
    "assert eg == tokenizer.decode(tokenizer(eg))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10]) torch.Size([2, 10])\n",
      "tensor([[ 5,  6,  5, 10, 14,  9,  3, 14, 12, 13],\n",
      "        [ 0,  4,  7, 12, 12,  8, 12,  3, 10,  3]])\n",
      "tensor([[ 5,  6,  6, 10, 14, 14, 14, 14, 14, 14],\n",
      "        [ 0,  4,  7, 12, 12, 12, 12, 12, 12, 12]])\n"
     ]
    }
   ],
   "source": [
    "class Dataset(TensorDataset):\n",
    "    def __init__(self, size, vocab_size, seq_len):\n",
    "        # self.tokenizer = tokenizer\n",
    "        self.size = size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_len = seq_len\n",
    "        self.rng = random.Random(42)\n",
    "        self.train = self._make_dataset()\n",
    "        super().__init__(*self.train)\n",
    "    \n",
    "    def _make_dataset(self):\n",
    "        x = t.randint(0, self.vocab_size, (self.size, self.seq_len))\n",
    "        # add cls and sep tokens\n",
    "        # x = t.cat([t.ones(self.size, 1).long() * self.tokenizer.cls_token_id, x, t.ones(self.size, 1).long() * self.tokenizer.sep_token_id], dim=1)\n",
    "        # pad to max len\n",
    "        # x = t.cat([x, t.ones(self.size, self.tokenizer.seq_len + 2 - x.shape[1]).long() * self.tokenizer.pad_token_id], dim=1)\n",
    "        # assert x.shape == (self.size, self.tokenizer.seq_len + 2)\n",
    "        y = x.cummax(-1).values\n",
    "        return x, y\n",
    "\n",
    "# test dataset \n",
    "for x, y in DataLoader(Dataset(10, 15, 10), batch_size=2):\n",
    "    print(x.shape, y.shape)\n",
    "    print(x)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001, 'batch_size': 64, 'epochs': 10, 'seed': 42, 'use_wandb': False, 'vocab_size': 1000, 'seq_len': 16}\n"
     ]
    }
   ],
   "source": [
    "device = t.device('cuda' if t.cuda.is_available() else 'cpu')\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    lr: float = 1e-3\n",
    "    batch_size: int = 64\n",
    "    epochs: int = 10\n",
    "    seed: int = 42\n",
    "    use_wandb: bool = False\n",
    "    vocab_size: int = 1000\n",
    "    seq_len: int = 16\n",
    "\n",
    "config = Config()\n",
    "print(config.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht_config = HookedTransformerConfig(\n",
    "    n_layers=1,\n",
    "    d_model=64,\n",
    "    n_heads=2,\n",
    "    n_ctx=16,\n",
    "    d_head=32,\n",
    "    attn_only=True,\n",
    "    d_vocab=config.vocab_size\n",
    ")\n",
    "model = HookedTransformer(ht_config, tokenizer)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = Dataset(10000, config.vocab_size, config.seq_len)\n",
    "valset = Dataset(1000, config.vocab_size, config.seq_len)\n",
    "testset = Dataset(1000, config.vocab_size, config.seq_len)\n",
    "\n",
    "trainloader = DataLoader(trainset, batch_size=config.batch_size, shuffle=True)\n",
    "valloader = DataLoader(valset, batch_size=config.batch_size, shuffle=False)\n",
    "testloader = DataLoader(testset, batch_size=config.batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lr': 0.001, 'batch_size': 64, 'epochs': 10, 'seed': None, 'use_wandb': False, 'vocab_size': 1000, 'seq_len': 16, 'n_layers': 1, 'd_model': 64, 'n_ctx': 16, 'd_head': 32, 'model_name': 'custom', 'n_heads': 2, 'd_mlp': None, 'act_fn': None, 'd_vocab': 1000, 'eps': 1e-05, 'use_attn_result': False, 'use_attn_scale': True, 'use_local_attn': False, 'original_architecture': None, 'from_checkpoint': False, 'checkpoint_index': None, 'checkpoint_label_type': None, 'checkpoint_value': None, 'tokenizer_name': None, 'window_size': None, 'attn_types': None, 'init_mode': 'gpt2', 'normalization_type': 'LN', 'device': 'cpu', 'attention_dir': 'causal', 'attn_only': True, 'initializer_range': 0.1, 'init_weights': True, 'scale_attn_by_inverse_layer_idx': False, 'positional_embedding_type': 'standard', 'final_rms': False, 'd_vocab_out': 1000, 'parallel_attn_mlp': False, 'rotary_dim': None, 'n_params': 16384}\n"
     ]
    }
   ],
   "source": [
    "print(config.__dict__ | ht_config.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moving model to device:  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjnhl\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/jon/ml/arena/arena-v1-ldn-exercises-new/wandb/run-20221216_111426-dyxirqvn</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/jnhl/gpt-max/runs/dyxirqvn\" target=\"_blank\">jumping-microwave-7</a></strong> to <a href=\"https://wandb.ai/jnhl/gpt-max\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 157/157 [00:02<00:00, 60.94it/s, Train Loss=3.28, Test Loss=3.15]\n",
      "Epoch 1: 100%|██████████| 157/157 [00:02<00:00, 65.53it/s, Train Loss=0.81, Test Loss=0.854]\n",
      "Epoch 2: 100%|██████████| 157/157 [00:02<00:00, 64.91it/s, Train Loss=0.345, Test Loss=0.454]\n",
      "Epoch 3: 100%|██████████| 157/157 [00:02<00:00, 68.55it/s, Train Loss=0.216, Test Loss=0.28] \n",
      "Epoch 4: 100%|██████████| 157/157 [00:02<00:00, 69.04it/s, Train Loss=0.225, Test Loss=0.272]\n",
      "Epoch 5: 100%|██████████| 157/157 [00:02<00:00, 68.43it/s, Train Loss=0.178, Test Loss=0.21]  \n",
      "Epoch 6: 100%|██████████| 157/157 [00:02<00:00, 62.39it/s, Train Loss=0.123, Test Loss=0.164] \n",
      "Epoch 7: 100%|██████████| 157/157 [00:03<00:00, 47.15it/s, Train Loss=0.0575, Test Loss=0.144]\n",
      "Epoch 8: 100%|██████████| 157/157 [00:03<00:00, 51.74it/s, Train Loss=0.0213, Test Loss=0.182]\n",
      "Epoch 9: 100%|██████████| 157/157 [00:02<00:00, 70.53it/s, Train Loss=0.0562, Test Loss=0.146]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c13cd9bd6216428997a1b660ea0ebd3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='0.001 MB of 0.036 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=0.027022…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test Loss</td><td>█▇▆▅▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>Train Loss</td><td>█▇▇▆▄▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test Loss</td><td>0.14618</td></tr><tr><td>Train Loss</td><td>0.05622</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">jumping-microwave-7</strong>: <a href=\"https://wandb.ai/jnhl/gpt-max/runs/dyxirqvn\" target=\"_blank\">https://wandb.ai/jnhl/gpt-max/runs/dyxirqvn</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221216_111426-dyxirqvn/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def train(model, config: Config, ht_config: HookedTransformerConfig, train_loader: DataLoader) -> None:\n",
    "    '''Train using masked language modelling.'''\n",
    "    model.train()\n",
    "    model.to(device)\n",
    "    opt = t.optim.Adam(model.parameters(), lr=config.lr)\n",
    "    if config.use_wandb:\n",
    "        wandb.init(project=\"gpt-max\", config=config.__dict__ | ht_config.__dict__)\n",
    "        wandb.watch(model)\n",
    "    run_name = wandb.run.name if config.use_wandb else 'gpt-max'\n",
    "    # tqdm progress bar of train loader annotated with epoch number\n",
    "    # make dir if doesn't exist \n",
    "    if not os.path.exists(f\"models/{run_name}\"):\n",
    "        os.makedirs(f\"models/{run_name}\")\n",
    "    t.save(model.state_dict(), f\"./models/{run_name}/{run_name}-e-1.pt\")\n",
    "    \n",
    "    for epoch in range(config.epochs):\n",
    "        progress_bar = tqdm.tqdm(train_loader, desc=f\"Epoch {epoch}\")\n",
    "        for n_batch, (batch, target) in enumerate(progress_bar):\n",
    "            batch = batch.to(device)\n",
    "            opt.zero_grad()\n",
    "\n",
    "            mask = (batch != tokenizer.pad_token_id).float()\n",
    "            mask.requires_grad = False\n",
    "            logits = model(batch.to(device))\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.shape[-1]), target.view(-1))\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            # measure test loss \n",
    "            if n_batch % 10 == 0:\n",
    "                test_loss = 0\n",
    "                with t.no_grad():\n",
    "                    for batch, target in testloader:\n",
    "                        batch = batch.to(device)\n",
    "                        logits = model(batch)\n",
    "                        test_loss += F.cross_entropy(logits.view(-1, logits.shape[-1]), target.view(-1)).item()\n",
    "                test_loss /= len(testloader)\n",
    "                progress_bar.set_postfix({\"Train Loss\": loss.item(), \"Test Loss\": test_loss})\n",
    "                if config.use_wandb:\n",
    "                    wandb.log({\"Train Loss\": loss.item(), \"Test Loss\": test_loss})\n",
    "        t.save(model.state_dict(), f\"./models/{run_name}/{run_name}-e-{epoch+1}.pt\")\n",
    "    if config.use_wandb:\n",
    "        wandb.finish()\n",
    "    requests.post(\"https://ntfy.sh/arena-brackets\", data=f\"Done training {run_name} 🎉\".encode(encoding='utf-8'))\n",
    "\n",
    "config.use_wandb = True\n",
    "train(model, config, ht_config, trainloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([467, 655, 605,  67, 369,  94, 474, 218, 798, 169, 374, 816, 107])\n",
      "tensor([[467, 655, 655, 655, 655, 655, 655, 655, 798, 798, 798, 816, 816]])\n",
      "\n",
      "tensor([332, 784, 158, 359, 176, 349, 689, 996, 728, 988, 829,  31, 715])\n",
      "tensor([[332, 784, 784, 784, 784, 784, 784, 996, 996, 996, 996, 996, 996]])\n",
      "\n",
      "tensor([700, 961, 618, 702, 268, 569,  66, 712, 105, 176,   0,  32, 570])\n",
      "tensor([[700, 961, 961, 961, 961, 961, 961, 961, 961, 961, 961, 961, 961]])\n",
      "\n",
      "tensor([318, 473, 159, 728, 713, 184, 789, 310, 495, 219, 892, 532, 739])\n",
      "tensor([[318, 473, 473, 728, 728, 728, 789, 789, 789, 789, 892, 892, 892]])\n",
      "\n",
      "tensor([123,  32, 663, 810, 364, 330, 806, 955, 513, 791,  91, 803, 848])\n",
      "tensor([[123, 123, 663, 810, 810, 810, 806, 955, 955, 955, 955, 955, 955]])\n",
      "\n",
      "tensor([343, 655, 159,  13, 355, 465, 366, 959, 117, 156, 583, 172, 302])\n",
      "tensor([[343, 655, 655, 655, 655, 655, 655, 959, 959, 959, 959, 959, 959]])\n",
      "\n",
      "tensor([174, 968, 203,  48, 108, 679, 411, 586, 747, 780, 209,  60, 231])\n",
      "tensor([[174, 968, 968, 968, 968, 968, 968, 968, 968, 968, 968, 968, 968]])\n",
      "\n",
      "tensor([825, 932, 461, 449, 482, 656, 967, 481, 988, 769, 110,  86, 279])\n",
      "tensor([[825, 932, 932, 932, 932, 932, 967, 967, 988, 988, 988, 988, 988]])\n",
      "\n",
      "tensor([316, 131, 147,  18, 373, 272, 348, 840, 239, 414,  91, 430, 732])\n",
      "tensor([[316, 316, 316, 316, 316, 316, 316, 840, 840, 840, 840, 840, 840]])\n",
      "\n",
      "tensor([359, 816, 873, 765, 884, 255, 395, 599, 781, 707, 638, 814, 341])\n",
      "tensor([[359, 816, 873, 873, 884, 884, 884, 884, 884, 884, 884, 884, 884]])\n",
      "\n",
      "tensor([698, 420, 515, 393, 784, 618, 214, 686, 717, 763, 289, 536, 460])\n",
      "tensor([[698, 698, 698, 698, 784, 784, 784, 784, 784, 784, 784, 784, 784]])\n",
      "\n",
      "tensor([967,   9,  26, 190,  10, 858, 985, 797, 974, 593, 740,  45, 644])\n",
      "tensor([[967, 967, 967, 967, 967, 967, 985, 985, 985, 985, 985, 985, 985]])\n",
      "\n",
      "tensor([836, 113, 488, 149, 666, 367, 458, 565, 959, 182, 468, 339, 439])\n",
      "tensor([[836, 836, 836, 836, 836, 836, 836, 836, 959, 959, 959, 959, 959]])\n",
      "\n",
      "tensor([389, 134, 682, 985, 403, 374, 156, 307, 681, 351, 764, 900, 602])\n",
      "tensor([[389, 389, 682, 985, 985, 985, 985, 985, 985, 985, 985, 985, 985]])\n",
      "\n",
      "tensor([971, 690, 281,  24, 962, 160, 923, 247, 475, 611, 846,  51, 879])\n",
      "tensor([[971, 971, 971, 971, 971, 971, 971, 971, 971, 971, 971, 971, 971]])\n",
      "\n",
      "tensor([392, 318, 109, 730, 675,  53, 778, 875, 833, 684, 603, 196, 373])\n",
      "tensor([[392, 318, 318, 730, 730, 730, 778, 875, 875, 875, 875, 875, 875]])\n",
      "\n",
      "tensor([877, 121, 658, 823, 560, 714, 773, 149, 589, 772, 680, 672, 689])\n",
      "tensor([[877, 877, 877, 877, 877, 877, 877, 877, 877, 877, 877, 877, 877]])\n",
      "\n",
      "tensor([144, 768, 764, 986,  65, 581, 641,  74, 466, 843, 650, 621, 882])\n",
      "tensor([[144, 768, 764, 986, 986, 986, 986, 986, 986, 986, 986, 986, 986]])\n",
      "\n",
      "tensor([589, 212, 607, 608, 367, 561, 664, 273, 331, 312, 245, 750, 589])\n",
      "tensor([[589, 589, 589, 608, 589, 608, 664, 664, 664, 664, 664, 750, 750]])\n",
      "\n",
      "tensor([467,  80, 313, 759, 604, 208,  51, 154, 470, 320, 326, 997, 645])\n",
      "tensor([[467, 467, 467, 759, 759, 759, 759, 759, 759, 759, 759, 997, 997]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with t.inference_mode():\n",
    "    tests = t.randint(0, config.vocab_size, (20, 13))\n",
    "    for test in tests:\n",
    "        print(test)\n",
    "        print(model(test.unsqueeze(0).to(device)).argmax(-1))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 1000])\n"
     ]
    }
   ],
   "source": [
    "with t.inference_mode():\n",
    "    tests = t.randint(0, config.vocab_size, (20, 13))\n",
    "    for test in tests:\n",
    "        print(model(test.unsqueeze(0).to(device)).shape)\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (hook_pos_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0): TransformerBlock(\n",
       "      (ln1): LayerNorm(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "      )\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def view_post_final_ln_logit_hook(ts, hook: HookPoint):\n",
    "    '''View the logits after the final layer norm.'''\n",
    "    # clean = clean_cache[hook.name]\n",
    "    # need to use the hook that gets the thing before the final layer norm\n",
    "    return ts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10000, 16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset.tensors[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000])\n"
     ]
    }
   ],
   "source": [
    "logits, cache = model.run_with_cache(trainset.tensors[0])\n",
    "logits = logits[0,0,:]\n",
    "print(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocablen = logits.shape[-1]\n",
    "# diffdir = logits - ((logits.sum(-1, keepdim=True) - logits) / (vocablen - 1))\n",
    "\n",
    "# diffdir shape is (batch_size, seq_len)\n",
    "\n",
    "diffdir = logits - ((logits.sum(-1, keepdim=True) - logits) / (vocablen - 1))\n",
    "diffdir.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.7695, -0.2882,  0.6947,  ..., -1.3604, -2.7450,  0.4886],\n",
       "         [-0.6885, -0.5564,  0.1146,  ..., -1.3411, -2.5370,  0.8246],\n",
       "         [-0.7988, -0.1549,  0.5057,  ..., -1.4457, -2.5238,  1.1742],\n",
       "         ...,\n",
       "         [-2.6439, -5.3273, -6.2290,  ...,  3.4536,  3.6812,  0.7984],\n",
       "         [-2.9570, -5.3918, -6.3698,  ...,  3.5195,  3.4453,  1.1000],\n",
       "         [-2.5041, -5.4830, -6.3181,  ...,  4.2196,  3.7131,  0.9899]]],\n",
       "       grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def view_post_final_ln_logit_hook(ts, hook: HookPoint):\n",
    "    '''View the logits after the final layer norm.'''\n",
    "    # clean = clean_cache[hook.name]\n",
    "    # need to use the hook that gets the thing before the final layer norm\n",
    "    print(ts.shape)\n",
    "    return ts\n",
    "\n",
    "ln_final = model.ln_final\n",
    "inputs = t.randint(0, config.vocab_size, (1, 13)).to(device)\n",
    "\n",
    "model.run_with_hooks(inputs, fwd_hooks=[(\"ln_final.hook_normalized\", view_post_final_ln_logit_hook)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_post_final_ln_dir(model) -> t.Tensor: \n",
    "    \"\"\"\n",
    "    Returns the \"direction\" that the post final layer norm layer points to to get the logits.\n",
    "\n",
    "    We get this by doing a calculation on the logits\n",
    "    \"\"\"\n",
    "    return model.decoder.weight[0, :] - model.decoder.weight[1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = trainset.tensors[0].to(device)\n",
    "logits, cache = model.run_with_cache(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inp shape: torch.Size([10000, 64])\n",
      "out shape: torch.Size([10000, 64])\n",
      "r^2:  tensor(0.9762, dtype=torch.float64)\n",
      "torch.Size([])\n"
     ]
    }
   ],
   "source": [
    "from typing import Union, Tuple\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "def get_ln_fit(\n",
    "    model, data, seq_pos: Union[None, int]\n",
    ") -> Tuple[LinearRegression, t.Tensor]:\n",
    "    '''\n",
    "    if seq_pos is None, find best fit for all sequence positions. Otherwise, fit only for given seq_pos.\n",
    "\n",
    "    Returns: A tuple of a (fitted) sklearn LinearRegression object and a dimensionless tensor containing the r^2 of the fit (hint: wrap a value in torch.tensor() to make a dimensionless tensor)\n",
    "    '''\n",
    "    inp = cache['blocks.0.hook_resid_post']\n",
    "    out = cache['ln_final.hook_normalized']\n",
    "    if seq_pos is None:\n",
    "        inp = rearrange(inp, 'b s e -> (b s) e')\n",
    "        out = rearrange(out, 'b s e -> (b s) e')\n",
    "    else:\n",
    "        inp = inp[:, seq_pos, :]\n",
    "        out = out[:, seq_pos, :]\n",
    "    print(f'inp shape: {inp.shape}')\n",
    "    print(f'out shape: {out.shape}')\n",
    "\n",
    "    linear_regression = LinearRegression().fit(inp, out)\n",
    "    r2 = linear_regression.score(inp, out)\n",
    "    return linear_regression, t.tensor(r2)\n",
    "    \n",
    "\n",
    "\n",
    "if True:\n",
    "    (final_ln_fit, r2) = get_ln_fit(model, data, seq_pos=0)\n",
    "    print(\"r^2: \", r2)\n",
    "    print(r2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_L(model, data):\n",
    "    (final_ln_fit, r2) = get_ln_fit(model, data, model.norm, seq_pos=0)\n",
    "    L = t.from_numpy(final_ln_fit.coef_)\n",
    "    return L\n",
    "\n",
    "def get_pre_final_ln_dir(model, data) -> t.Tensor:\n",
    "    post_final_ln_dir = get_post_final_ln_dir(model)\n",
    "    L = get_L(model, data)\n",
    "    return t.einsum(\"i,ij->j\", post_final_ln_dir, L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActivationCache with keys ['hook_embed', 'hook_pos_embed', 'blocks.0.hook_resid_pre', 'blocks.0.ln1.hook_scale', 'blocks.0.ln1.hook_normalized', 'blocks.0.attn.hook_q', 'blocks.0.attn.hook_k', 'blocks.0.attn.hook_v', 'blocks.0.attn.hook_attn_scores', 'blocks.0.attn.hook_pattern', 'blocks.0.attn.hook_z', 'blocks.0.hook_attn_out', 'blocks.0.hook_resid_post', 'ln_final.hook_scale', 'ln_final.hook_normalized']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_out_by_head(model, data, layer: int) -> t.Tensor:\n",
    "    '''\n",
    "    Get the output of the heads in a particular layer when the model is run on the data.\n",
    "    Returns a tensor of shape (batch, num_heads, seq, embed_width)\n",
    "    '''\n",
    "    module = model.layers[layer].self_attn.W_O\n",
    "    r = cache[f'blocks.{layer}.hook_z']\n",
    "    # print(f'r shape: {r.shape}')\n",
    "    r = rearrange(r, 'batch seq (nheads headsize) -> batch nheads seq headsize', nheads=model.nhead)\n",
    "    # print(f'r shape: {r.shape}')\n",
    "    W_O = module.weight\n",
    "    # print(f'weight shape: {W_O.shape}')\n",
    "    W_O = rearrange(W_O, 'emb (nheads headsize) -> nheads emb headsize', nheads=model.nhead)\n",
    "    # print(f'weight shape: {W_O.shape}')\n",
    "    out = einsum('batch nheads seq headsize, nheads emb headsize -> batch nheads seq emb', r, W_O)\n",
    "    # print(f'out shape: {out.shape}')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_out_by_components(model, data) -> t.Tensor:\n",
    "    '''\n",
    "    Computes a tensor of shape [10, dataset_size, seq_pos, emb] representing the output of the model's components when run on the data.\n",
    "    The first dimension is  [embeddings, head 0.0, head 0.1, mlp 0, head 1.0, head 1.1, mlp 1, head 2.0, head 2.1, mlp 2]\n",
    "    '''\n",
    "    out = []\n",
    "    out.append(cache['hook_pos_embed'])\n",
    "    for i in range(model.nlayers):\n",
    "        out.append(get_out_by_head(model, data, i)[:, 0, :, :])\n",
    "        out.append(get_out_by_head(model, data, i)[:, 1, :, :])\n",
    "    return t.stack(out, dim=0)\n",
    "\n",
    "if MAIN:\n",
    "    w5d5_tests.test_get_out_by_component(get_out_by_components, model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'get_out_by_components' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [27], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m     fig\u001b[39m.\u001b[39mshow()\n\u001b[1;32m     29\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 30\u001b[0m     out_by_components \u001b[39m=\u001b[39m get_out_by_components(model, data)[:, :, \u001b[39m0\u001b[39m, :]\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     31\u001b[0m     unbalanced_dir \u001b[39m=\u001b[39m get_pre_final_ln_dir(model, data)\u001b[39m.\u001b[39mdetach()\n\u001b[1;32m     32\u001b[0m     magnitudes \u001b[39m=\u001b[39m einsum(\u001b[39m\"\u001b[39m\u001b[39mcomponent sample emb, emb -> component sample\u001b[39m\u001b[39m\"\u001b[39m, out_by_components, unbalanced_dir)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'get_out_by_components' is not defined"
     ]
    }
   ],
   "source": [
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def hists_per_comp(magnitudes, data, n_layers=3, xaxis_range=(-1, 1)):\n",
    "    num_comps = magnitudes.shape[0]\n",
    "    titles = {\n",
    "        (1, 1): \"embeddings\",\n",
    "        (2, 1): \"head 0.0\",\n",
    "        (2, 2): \"head 0.1\",\n",
    "        (2, 3): \"mlp 0\",\n",
    "        (3, 1): \"head 1.0\",\n",
    "        (3, 2): \"head 1.1\",\n",
    "        (3, 3): \"mlp 1\",\n",
    "        (4, 1): \"head 2.0\",\n",
    "        (4, 2): \"head 2.1\",\n",
    "        (4, 3): \"mlp 2\"\n",
    "    }\n",
    "    assert num_comps == len(titles)\n",
    "\n",
    "    fig = make_subplots(rows=n_layers+1, cols=3)\n",
    "    for ((row, col), title), mag in zip(titles.items(), magnitudes):\n",
    "        if row == n_layers+2: break\n",
    "        fig.add_trace(go.Histogram(x=mag[data.isbal].numpy(), name=\"Balanced\", marker_color=\"blue\", opacity=0.5, legendgroup = '1', showlegend=title==\"embeddings\"), row=row, col=col)\n",
    "        fig.add_trace(go.Histogram(x=mag[~data.isbal].numpy(), name=\"Unbalanced\", marker_color=\"red\", opacity=0.5, legendgroup = '2', showlegend=title==\"embeddings\"), row=row, col=col)\n",
    "        fig.update_xaxes(title_text=title, row=row, col=col, range=xaxis_range)\n",
    "    fig.update_layout(width=1200, height=250*(n_layers+1), barmode=\"overlay\", legend=dict(yanchor=\"top\", y=0.92, xanchor=\"left\", x=0.4), title=\"Histograms of component significance\")\n",
    "    fig.show()\n",
    "\n",
    "if True:\n",
    "    out_by_components = get_out_by_components(model, data)[:, :, 0, :].detach()\n",
    "    unbalanced_dir = get_pre_final_ln_dir(model, data).detach()\n",
    "    magnitudes = einsum(\"component sample emb, emb -> component sample\", out_by_components, unbalanced_dir)\n",
    "    magnitudes = magnitudes - magnitudes[:, data.isbal].mean(-1, keepdim=True)\n",
    "\n",
    "    assert \"magnitudes\" in locals(), \"You need to define `magnitudes`\"\n",
    "    hists_per_comp(magnitudes, data, xaxis_range=[-10, 20])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a41fdc720b403cff5d22ec3440153970555b5fcc336583b0458a17a41b31d53f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
